{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PDF has 361 pages.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def get_page_count(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Get the number of pages\n",
    "    num_pages = pdf_document.page_count\n",
    "    \n",
    "    # Close the document\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return num_pages\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'example.pdf'\n",
    "\n",
    "# Get and print the page count\n",
    "page_count = get_page_count(pdf_path)\n",
    "print(f'The PDF has {page_count} pages.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 2:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI with LangChain\n",
      "Build large language model (LLM) apps with Python, \n",
      "ChatGPT, and other \n",
      "----------------------------------------\n",
      "Page 3:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI with LangChain\n",
      "Copyright © 2023 Packt Publishing\n",
      "All rights reserved. No part of this \n",
      "----------------------------------------\n",
      "Page 4:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): To Diane and Nico\n",
      "– Ben Auffarth\n",
      "\n",
      "----------------------------------------\n",
      "Page 5:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Contributors\n",
      "About the author\n",
      "Ben Auffarth is a seasoned data science leader with a background and P\n",
      "----------------------------------------\n",
      "Page 6:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): About the reviewers\n",
      "Leonid Ganeline is a machine learning engineer with extensive experience in natu\n",
      "----------------------------------------\n",
      "Page 7:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Join our community on Discord\n",
      "Join our community's Discord space for discussions with the authors an\n",
      "----------------------------------------\n",
      "Page 8:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "Preface\n",
      " xv\n",
      "Chapter 1: What Is Generative AI?\n",
      " 1\n",
      "Introducing generative AI..\n",
      "----------------------------------------\n",
      "Page 9:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "viii\n",
      "Chapter 2: LangChain for LLM Apps\n",
      " 37\n",
      "Going beyond stochastic parrots....\n",
      "----------------------------------------\n",
      "Page 10:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "ix\n",
      "Azure • 84\n",
      "Anthropic • 85\n",
      "Exploring local models.............................\n",
      "----------------------------------------\n",
      "Page 11:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "x\n",
      "Vector indexing • 140\n",
      "Vector libraries • 141\n",
      "Vector databases • 143\n",
      "Loading and \n",
      "----------------------------------------\n",
      "Page 12:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "xi\n",
      "Automating software development..............................................\n",
      "----------------------------------------\n",
      "Page 13:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "xii\n",
      "Chain-of-thought prompting • 248\n",
      "Self-consistency • 249\n",
      "Tree-of-thought • 251\n",
      "\n",
      "----------------------------------------\n",
      "Page 14:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Table of Contents\n",
      "xiii\n",
      "Economic consequences......................................................\n",
      "----------------------------------------\n",
      "Page 15:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 16:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Preface\n",
      "In the dynamic and rapidly advancing field of AI, generative AI stands out as a disruptive f\n",
      "----------------------------------------\n",
      "Page 17:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Preface\n",
      "xvi\n",
      "Whether you are a beginner or an experienced developer, this book will be a valuable res\n",
      "----------------------------------------\n",
      "Page 18:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Preface\n",
      "xvii\n",
      "It looks at implementing a Streamlit application to create interactive LLM applications\n",
      "----------------------------------------\n",
      "Page 19:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Preface\n",
      "xviii\n",
      "Chapter 9, Generative AI in Production, addresses the complexities of deploying LLMs w\n",
      "----------------------------------------\n",
      "Page 20:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Preface\n",
      "xix\n",
      "CodeInText: Indicates code words in text, database table names, folder names, filenames,\n",
      "----------------------------------------\n",
      "Page 21:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Preface\n",
      "xx\n",
      "Get in touch\n",
      "Feedback from our readers is always welcome.\n",
      "General feedback: Email feedbac\n",
      "----------------------------------------\n",
      "Page 22:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Download a free PDF copy of this book\n",
      "Thanks for purchasing this book!\n",
      "Do you like to read on the go\n",
      "----------------------------------------\n",
      "Page 23:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 24:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 1\n",
      "What Is Generative AI?\n",
      "Over the last decade, deep learning has evolved massively to process and ge\n",
      "----------------------------------------\n",
      "Page 25:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "2\n",
      "Introducing generative AI\n",
      "In the media, there is substantial coverage of AI\n",
      "----------------------------------------\n",
      "Page 26:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "3\n",
      "You can see significant improvements in recent years in this benchmark. Particularly, it\n",
      "----------------------------------------\n",
      "Page 27:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "4\n",
      "For example, GPT-4 could be considered a polymath that works tirelessly wit\n",
      "----------------------------------------\n",
      "Page 28:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "5\n",
      "  \n",
      "Figure 1.2: ML in a suit. Generated by a model on replicate.com, Diffusers Stable Dif\n",
      "----------------------------------------\n",
      "Page 29:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "6\n",
      "This class diagram illustrates how LLMs combine deep learning techniques li\n",
      "----------------------------------------\n",
      "Page 30:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "7\n",
      "•\t\n",
      "Text-to-video: Models that generate video content from text descriptions. Example: \n",
      "P\n",
      "----------------------------------------\n",
      "Page 31:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "8\n",
      "The rapid progress shows the potential of generative AI across diverse doma\n",
      "----------------------------------------\n",
      "Page 32:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "9\n",
      "Figure 1.4: Cost of computer storage since the 1950s in dollars (unadjusted) per terabyt\n",
      "----------------------------------------\n",
      "Page 33:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "10\n",
      "This trend toward larger models started around 2009, when NVIDIA catalyzed\n",
      "----------------------------------------\n",
      "Page 34:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "11\n",
      "This has made it possible to train generative models on much larger datasets than would\n",
      "----------------------------------------\n",
      "Page 35:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "12\n",
      "Recently, LLMs have found applications for tasks like essay generation, co\n",
      "----------------------------------------\n",
      "Page 36:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "13\n",
      "What is a GPT?\n",
      "LLMs are deep neural networks adept at understanding and generating huma\n",
      "----------------------------------------\n",
      "Page 37:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "14\n",
      "The size of the training corpus for LLMs has been increasing drastically. \n",
      "----------------------------------------\n",
      "Page 38:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "15\n",
      "For some models, especially proprietary and closed-source models, this information is n\n",
      "----------------------------------------\n",
      "Page 39:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "16\n",
      "Apparently, GPT-4 was trained on about 13 trillion tokens. However, these \n",
      "----------------------------------------\n",
      "Page 40:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "17\n",
      "PaLM 2 was also tested on various professional language-proficiency exams. The exams us\n",
      "----------------------------------------\n",
      "Page 41:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "18\n",
      "Claude and Claude 2 are AI assistants created by Anthropic. Evaluations su\n",
      "----------------------------------------\n",
      "Page 42:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "19\n",
      "There are quite a few companies and organizations developing generative AI in gener-\n",
      "al\n",
      "----------------------------------------\n",
      "Page 43:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "20\n",
      "The complexity of estimating parameters in generative AI models suggests t\n",
      "----------------------------------------\n",
      "Page 44:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "21\n",
      "In addition, the decoder has a third sub-layer that performs Multi-Head Attention (MHA)\n",
      "----------------------------------------\n",
      "Page 45:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "22\n",
      "•\t\n",
      "Multi-head attention: Instead of applying attention once, the transform\n",
      "----------------------------------------\n",
      "Page 46:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "23\n",
      "The combination of these architectural features allows GPT models to successfully tackl\n",
      "----------------------------------------\n",
      "Page 47:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "24\n",
      "In comparing different language models, perplexity is often used as a benc\n",
      "----------------------------------------\n",
      "Page 48:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "25\n",
      "There are a lot of tokenizers that work according to different principles, but common t\n",
      "----------------------------------------\n",
      "Page 49:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "26\n",
      "They predicted that large models would perform better if substantially sma\n",
      "----------------------------------------\n",
      "Page 50:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "27\n",
      "How to try out these models\n",
      "You can access OpenAI’s model through their website or thei\n",
      "----------------------------------------\n",
      "Page 51:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "28\n",
      "Models like Midjourney, DALL-E 2, and Stable Diffusion provide creative an\n",
      "----------------------------------------\n",
      "Page 52:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "29\n",
      "The unique aspect of generative image models is the reverse diffusion process, where th\n",
      "----------------------------------------\n",
      "Page 53:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "30\n",
      "Stable Diffusion was developed by the CompVis group at LMU Munich (High-Re\n",
      "----------------------------------------\n",
      "Page 54:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "31\n",
      "For training the image generation model in the latent space itself (latent diffusion mo\n",
      "----------------------------------------\n",
      "Page 55:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "32\n",
      "The following images illustrate text-to-image generation from a text promp\n",
      "----------------------------------------\n",
      "Page 56:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "33\n",
      "What can AI do in other domains?\n",
      "Generative AI models have demonstrated impressive capa\n",
      "----------------------------------------\n",
      "Page 57:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): What Is Generative AI?\n",
      "34\n",
      "AudioLM\n",
      "Google\n",
      "2023\n",
      "Sound/\n",
      "speech\n",
      "Tokenizer + \n",
      "transformer LM + \n",
      "detokeniz\n",
      "----------------------------------------\n",
      "Page 58:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 1\n",
      "35\n",
      "Questions\n",
      "I think it’s a good habit to check that you’ve digested the material when rea\n",
      "----------------------------------------\n",
      "Page 59:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 60:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 2\n",
      "LangChain for LLM Apps\n",
      "Large Language Models (LLMs) like GPT-4 have demonstrated immense capabilit\n",
      "----------------------------------------\n",
      "Page 61:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "38\n",
      "Going beyond stochastic parrots\n",
      "LLMs have gained significant attention and\n",
      "----------------------------------------\n",
      "Page 62:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "39\n",
      "•\t\n",
      "Lack of context: LLMs struggle to incorporate relevant context like previous convers\n",
      "----------------------------------------\n",
      "Page 63:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "40\n",
      "Let’s look at a few examples of problems with LLMs. The cut-off day issue \n",
      "----------------------------------------\n",
      "Page 64:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "41\n",
      "In this case, we can see that the model talks about a different LangChain, which is a d\n",
      "----------------------------------------\n",
      "Page 65:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "42\n",
      "The LLM hasn’t stored the result of the calculation or hasn’t encountered \n",
      "----------------------------------------\n",
      "Page 66:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "43\n",
      "To re-emphasize what we previously mentioned, raw model scale alone cannot impart compo\n",
      "----------------------------------------\n",
      "Page 67:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "44\n",
      "In contrast, an LLM app is an application that utilizes an LLM to understa\n",
      "----------------------------------------\n",
      "Page 68:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "45\n",
      "As can be seen in the preceding figure, the client layer collects user text queries and\n",
      "----------------------------------------\n",
      "Page 69:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "46\n",
      "•\t\n",
      "Question answering: Users can ask an LLM app questions in plain languag\n",
      "----------------------------------------\n",
      "Page 70:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "47\n",
      "Beyond basic LLM API usage, LangChain facilitates advanced interactions like conversati\n",
      "----------------------------------------\n",
      "Page 71:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "48\n",
      "LangChain comes with many extensions and a larger ecosystem that is develo\n",
      "----------------------------------------\n",
      "Page 72:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "49\n",
      "LangChainHub is a central repository for sharing artifacts like prompts, chains, and ag\n",
      "----------------------------------------\n",
      "Page 73:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "50\n",
      "While still relatively new, LangChain unlocks more advanced LLM applicatio\n",
      "----------------------------------------\n",
      "Page 74:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "51\n",
      "An LLMCheckerChain verifies statements to reduce inaccurate responses using a technique\n",
      "----------------------------------------\n",
      "Page 75:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "52\n",
      "What are agents?\n",
      "Agents are a key concept in LangChain for creating system\n",
      "----------------------------------------\n",
      "Page 76:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "53\n",
      "In the section about the limitations of LLMs, we’ve seen that for calculations, a simpl\n",
      "----------------------------------------\n",
      "Page 77:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "54\n",
      "What is memory?\n",
      "In LangChain, memory refers to the persisting state betwee\n",
      "----------------------------------------\n",
      "Page 78:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "55\n",
      "LangChain’s memory integrations, from short-term caching to long-term databases, enable\n",
      "----------------------------------------\n",
      "Page 79:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "56\n",
      "•\t\n",
      "Table processing: APIs built with pandas DataFrames enable language mod\n",
      "----------------------------------------\n",
      "Page 80:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "57\n",
      "After discussing chains, agents, memory, and tools, let’s put this all together to get \n",
      "----------------------------------------\n",
      "Page 81:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "58\n",
      "•\t\n",
      "Agents: Goal-driven systems that use LLMs to plan actions based on envi\n",
      "----------------------------------------\n",
      "Page 82:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "59\n",
      "Data loaders include modules for storing data and utilities for interacting with extern\n",
      "----------------------------------------\n",
      "Page 83:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "60\n",
      "Comparing LangChain with other frameworks\n",
      "LLM application frameworks have \n",
      "----------------------------------------\n",
      "Page 84:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "61\n",
      "LangChain excels at chaining LLMs together using agents to delegate actions to models. \n",
      "----------------------------------------\n",
      "Page 85:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LangChain for LLM Apps\n",
      "62\n",
      "As we saw in this chapter, chains allow sequencing calls to LLMs, database\n",
      "----------------------------------------\n",
      "Page 86:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 2\n",
      "63\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with t\n",
      "----------------------------------------\n",
      "Page 87:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 88:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 3\n",
      "Getting Started with LangChain\n",
      "In this book, we’ll write a lot of code and test many different int\n",
      "----------------------------------------\n",
      "Page 89:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "66\n",
      "Please make sure you have Python version 3.10 or higher installed.\n",
      "----------------------------------------\n",
      "Page 90:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "67\n",
      "Conda handles intricate dependencies efficiently, although it can be excruciatingly slo\n",
      "----------------------------------------\n",
      "Page 91:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "68\n",
      "2.\t\n",
      "Use a virtual environment for isolation (for example, venv).\n",
      "3\n",
      "----------------------------------------\n",
      "Page 92:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "69\n",
      "With the help of LangChain, we can interact with all of these – for example, through Ap\n",
      "----------------------------------------\n",
      "Page 93:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "70\n",
      "LangChain implements three different interfaces – we can use chat \n",
      "----------------------------------------\n",
      "Page 94:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "71\n",
      "To permanently set the environment variable in Linux or macOS, you would need to add th\n",
      "----------------------------------------\n",
      "Page 95:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "72\n",
      "Now, let’s go through a few prominent model providers in turn. We’\n",
      "----------------------------------------\n",
      "Page 96:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "73\n",
      "As you can see, we connect a tool, a Python Read-Eval-Print Loop (REPL), that will be c\n",
      "----------------------------------------\n",
      "Page 97:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "74\n",
      "We will use OpenAI for our applications but will also try LLMs fro\n",
      "----------------------------------------\n",
      "Page 98:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "75\n",
      "We can use the OpenAI language model class to set up an LLM to interact with. Let’s cre\n",
      "----------------------------------------\n",
      "Page 99:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "76\n",
      "Hugging Face also provides the Hugging Face Hub, a platform for ho\n",
      "----------------------------------------\n",
      "Page 100:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "77\n",
      "The LLM takes a text input, a question in this case, and returns a completion. The mode\n",
      "----------------------------------------\n",
      "Page 101:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "78\n",
      "Let’s run a model!\n",
      "from langchain.llms import VertexAI\n",
      "from langch\n",
      "----------------------------------------\n",
      "Page 102:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "79\n",
      "•\t\n",
      "code-gecko suggests code completions. It has a max input length of 2,048 tokens and \n",
      "----------------------------------------\n",
      "Page 103:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "80\n",
      "Would you hire code-bison for your team?\n",
      "Jina AI\n",
      "Jina AI, founded \n",
      "----------------------------------------\n",
      "Page 104:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "81\n",
      "We get examples for client calls in Python and cURL, and a demo, where we can ask a que\n",
      "----------------------------------------\n",
      "Page 105:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "82\n",
      "        HumanMessage(\n",
      "            content=\"I like pasta with chees\n",
      "----------------------------------------\n",
      "Page 106:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "83\n",
      "Ben Firshman, who drove open-source product efforts at Docker, and Andreas Jansson, a f\n",
      "----------------------------------------\n",
      "Page 107:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "84\n",
      "I got this image:\n",
      "Figure 3.4: A book cover for a book about genera\n",
      "----------------------------------------\n",
      "Page 108:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "85\n",
      "After setting up, the models should be accessible through the AzureOpenAI() class inter\n",
      "----------------------------------------\n",
      "Page 109:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "86\n",
      "Hugging Face Transformers\n",
      "I’ll quickly show the general recipe for\n",
      "----------------------------------------\n",
      "Page 110:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "87\n",
      "question = \"What is electroencephalography?\"\n",
      "print(llm_chain.run(question))\n",
      "In this exa\n",
      "----------------------------------------\n",
      "Page 111:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "88\n",
      "To get the Llama model weights, you need to sign up with the T&Cs \n",
      "----------------------------------------\n",
      "Page 112:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "89\n",
      "As for model support, GPT4All supports a large array of Transformer architectures:\n",
      "•\t\n",
      "G\n",
      "----------------------------------------\n",
      "Page 113:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "90\n",
      "Generative AI can assist customer service agents in several ways:\n",
      "\n",
      "----------------------------------------\n",
      "Page 114:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "91\n",
      "Text: {sentence}\n",
      "Sentiment:\n",
      "LLMs can also be highly effective at summarization, much be\n",
      "----------------------------------------\n",
      "Page 115:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "92\n",
      "    for rank, model in enumerate(\n",
      "        list_models(filter=task,\n",
      "----------------------------------------\n",
      "Page 116:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "93\n",
      "Its once elegant exterior was marred by the scars of travel, resembling a \n",
      "war-torn sol\n",
      "----------------------------------------\n",
      "Page 117:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "94\n",
      "Here are the 5 most popular models for summarization as well (down\n",
      "----------------------------------------\n",
      "Page 118:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "95\n",
      "This summary is just passable, but not very convincing. There is still a lot of ramblin\n",
      "----------------------------------------\n",
      "Page 119:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Getting Started with LangChain\n",
      "96\n",
      "I hope it was exciting to see how quickly we can throw a few model\n",
      "----------------------------------------\n",
      "Page 120:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 3\n",
      "97\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with t\n",
      "----------------------------------------\n",
      "Page 121:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 122:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 4\n",
      "Building Capable Assistants\n",
      "As LLMs continue to advance, a key challenge is transforming their imp\n",
      "----------------------------------------\n",
      "Page 123:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "100\n",
      "Mitigating hallucinations through fact-checking\n",
      "As discussed in prev\n",
      "----------------------------------------\n",
      "Page 124:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "101\n",
      "Pre-trained LLMs contain extensive world knowledge that can be prompted for facts. Add\n",
      "----------------------------------------\n",
      "Page 125:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "102\n",
      "LLMCheckerChain does this all by itself, as this example shows:\n",
      "from\n",
      "----------------------------------------\n",
      "Page 126:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "103\n",
      "So, while this technique does not guarantee correct answers, it can put a stop to some\n",
      "----------------------------------------\n",
      "Page 127:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "104\n",
      "This is similar to what we saw in Chapter 3, Getting Started with La\n",
      "----------------------------------------\n",
      "Page 128:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "105\n",
      "We can implement this in LangChain Expression Language (LCEL):\n",
      "from langchain import P\n",
      "----------------------------------------\n",
      "Page 129:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "106\n",
      "Guidelines:\n",
      "- The first summary should be long (4-5 sentences, ~80 w\n",
      "----------------------------------------\n",
      "Page 130:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "107\n",
      "Map-Reduce pipelines\n",
      "LangChain supports a map reduce approach for processing documents\n",
      "----------------------------------------\n",
      "Page 131:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "108\n",
      "This approach’s implications are that it allows the parallel process\n",
      "----------------------------------------\n",
      "Page 132:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "109\n",
      "The text would be the summaries from the map steps. An instruction like that would hel\n",
      "----------------------------------------\n",
      "Page 133:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "110\n",
      "For example, ChatGPT models, like GPT-3.5-Turbo, specialize in dialo\n",
      "----------------------------------------\n",
      "Page 134:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "111\n",
      "Completion Tokens: 28\n",
      "Total Cost (USD): $0.00072\n",
      "You can change the parameters of the \n",
      "----------------------------------------\n",
      "Page 135:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "112\n",
      "This can be extremely helpful for understanding how much money you a\n",
      "----------------------------------------\n",
      "Page 136:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "113\n",
      "The default format for the schema is a dictionary, but we can also define properties a\n",
      "----------------------------------------\n",
      "Page 137:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "114\n",
      "Please note that you should set up your environment according to the\n",
      "----------------------------------------\n",
      "Page 138:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "115\n",
      "pdf_file_path = \"<pdf_file_path>\"\n",
      "pdf_loader = PyPDFLoader(pdf_file_path)\n",
      "docs = pdf_l\n",
      "----------------------------------------\n",
      "Page 139:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "116\n",
      "Answering questions with tools\n",
      "LLMs are trained on general corpus da\n",
      "----------------------------------------\n",
      "Page 140:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "117\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "def load_agent() -> AgentExecutor:\n",
      "    ll\n",
      "----------------------------------------\n",
      "Page 141:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "118\n",
      "Please note that to use Wolfram Alpha, you have to set up an account\n",
      "----------------------------------------\n",
      "Page 142:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "119\n",
      "Figure 4.4: Question-answering app in Streamlit\n",
      "The search works quite well although, \n",
      "----------------------------------------\n",
      "Page 143:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "120\n",
      "Here’s the log output (shortened) for the correct reasoning:\n",
      "> Enter\n",
      "----------------------------------------\n",
      "Page 144:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "121\n",
      "While our LLM app can provide answers to simple questions, its reasoning abilities are\n",
      "----------------------------------------\n",
      "Page 145:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "122\n",
      "Figure 4.5: Tool-augmented LLM paradigm\n",
      "The tools are the available \n",
      "----------------------------------------\n",
      "Page 146:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "123\n",
      "•\t\n",
      "Action agents reason iteratively based on observations after each action.\n",
      "•\t\n",
      "Plan-a\n",
      "----------------------------------------\n",
      "Page 147:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "124\n",
      "Observation-dependent reasoning involves making judgments, predictio\n",
      "----------------------------------------\n",
      "Page 148:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "125\n",
      "The Planner (an LLM), which can be fine-tuned for planning and tool usage, produces a \n",
      "----------------------------------------\n",
      "Page 149:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "126\n",
      "    if strategy == \"plan-and-solve\":\n",
      "        planner = load_chat_pla\n",
      "----------------------------------------\n",
      "Page 150:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "127\n",
      "We should see how Streamlit starts up our application. If we open our browser on the i\n",
      "----------------------------------------\n",
      "Page 151:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "128\n",
      "4.\t\n",
      "Recognize the importance of plans and solve agents in LLMs: Plan\n",
      "----------------------------------------\n",
      "Page 152:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 4\n",
      "129\n",
      "Summary\n",
      "In this chapter, we first talked about the problem of hallucinations and autom\n",
      "----------------------------------------\n",
      "Page 153:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building Capable Assistants\n",
      "130\n",
      "Questions\n",
      "Please have a look to see if you can come up with the answ\n",
      "----------------------------------------\n",
      "Page 154:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 5\n",
      "Building a Chatbot like ChatGPT\n",
      "Chatbots powered by LLMs have demonstrated impressive fluency in c\n",
      "----------------------------------------\n",
      "Page 155:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "132\n",
      "•\t\n",
      "Loading and retrieving in LangChain\n",
      "•\t\n",
      "Implementing a chatbot\n",
      "----------------------------------------\n",
      "Page 156:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "133\n",
      "Some use cases for chatbots in customer service include providing 24/7 support, handli\n",
      "----------------------------------------\n",
      "Page 157:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "134\n",
      "There is an important distinction between chatbots that merely r\n",
      "----------------------------------------\n",
      "Page 158:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "135\n",
      "By grounding LLMs with use-case-specific information through RAG, the quality and accu\n",
      "----------------------------------------\n",
      "Page 159:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "136\n",
      "Embeddings can be created using different methods. For texts, on\n",
      "----------------------------------------\n",
      "Page 160:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "137\n",
      "Today, for most domains including texts and images, embeddings usually come from trans\n",
      "----------------------------------------\n",
      "Page 161:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "138\n",
      "In this case, the embed_documents() method is used to retrieve e\n",
      "----------------------------------------\n",
      "Page 162:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "139\n",
      "In these examples, we’ve used OpenAI embeddings – in the examples further on, we’ll us\n",
      "----------------------------------------\n",
      "Page 163:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "140\n",
      "Let’s dive into a few of these concepts a bit more. There are th\n",
      "----------------------------------------\n",
      "Page 164:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "141\n",
      "•\t\n",
      "Locality sensitive hashing (LSH): This is a hashing-based method that maps similar \n",
      "----------------------------------------\n",
      "Page 165:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "142\n",
      "Here’s a quick overview of some open-source libraries for vector\n",
      "----------------------------------------\n",
      "Page 166:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "143\n",
      "•\t\n",
      "SPTAG by Microsoft implements a distributed ANN. It comes with a k-d tree and rela-\n",
      "----------------------------------------\n",
      "Page 167:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "144\n",
      "These databases are popular because they are optimized for scala\n",
      "----------------------------------------\n",
      "Page 168:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "145\n",
      "Some examples of vector databases are listed in Table 5.1. I took the liberty of highl\n",
      "----------------------------------------\n",
      "Page 169:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "146\n",
      "Pinecone\n",
      "Fast and scalable \n",
      "applications using \n",
      "embeddings from \n",
      "----------------------------------------\n",
      "Page 170:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "147\n",
      "Figure 5.4: Star history of open-source vector databases on GitHub\n",
      "You can see that mi\n",
      "----------------------------------------\n",
      "Page 171:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "148\n",
      "3.\t\n",
      "We can query the vector store to retrieve similar vectors:\n",
      "s\n",
      "----------------------------------------\n",
      "Page 172:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "149\n",
      "The relationship between them is illustrated in the diagram here (source: LangChain do\n",
      "----------------------------------------\n",
      "Page 173:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "150\n",
      "Document loaders have a load() method that loads data from the c\n",
      "----------------------------------------\n",
      "Page 174:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "151\n",
      "•\t\n",
      "TF-IDF retriever: This retriever uses the TF-IDF (Term Frequency-Inverse Document \n",
      "\n",
      "----------------------------------------\n",
      "Page 175:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "152\n",
      "Once the retriever is created, you can use it to retrieve releva\n",
      "----------------------------------------\n",
      "Page 176:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "153\n",
      "Host genetic polymorphisms involved in long-term symptoms of COVID-19.\n",
      "Association Bet\n",
      "----------------------------------------\n",
      "Page 177:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "154\n",
      "To implement a simple chatbot in LangChain, you can follow this \n",
      "----------------------------------------\n",
      "Page 178:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "155\n",
      "        \".doc\": UnstructuredWordDocumentLoader\n",
      "    }\n",
      "This gives us interfaces to read \n",
      "----------------------------------------\n",
      "Page 179:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "156\n",
      "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=15\n",
      "----------------------------------------\n",
      "Page 180:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "157\n",
      "The base compressor is responsible for compressing the contents of individual document\n",
      "----------------------------------------\n",
      "Page 181:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "158\n",
      "Please note that I’ve just made up a new variable, use_compressi\n",
      "----------------------------------------\n",
      "Page 182:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "159\n",
      "    \"\"\"Read documents, configure retriever, and the chain.\"\"\"\n",
      "    docs = []\n",
      "    temp_d\n",
      "----------------------------------------\n",
      "Page 183:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "160\n",
      "This gives us a chatbot with retrieval that’s usable via a visua\n",
      "----------------------------------------\n",
      "Page 184:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "161\n",
      "By storing knowledge from message sequences, memory permits extracting insights to imp\n",
      "----------------------------------------\n",
      "Page 185:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "162\n",
      "In this example, we create a conversation chain with memory usin\n",
      "----------------------------------------\n",
      "Page 186:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "163\n",
      "We can see the message with memory.load_memory_variables({}).\n",
      "We can also customize th\n",
      "----------------------------------------\n",
      "Page 187:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "164\n",
      "Remembering conversation summaries\n",
      "ConversationSummaryMemory is \n",
      "----------------------------------------\n",
      "Page 188:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "165\n",
      "We’ll instantiate the ConversationKGMemory class and pass your LLM instance as the llm\n",
      "----------------------------------------\n",
      "Page 189:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "166\n",
      "Current conversation:\n",
      "{chat_history_lines}\n",
      "Human: {input}\n",
      "AI:\"\"\"\n",
      "----------------------------------------\n",
      "Page 190:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "167\n",
      "For example, in Python, you can initialize a ZepMemory instance as follows:\n",
      "from langc\n",
      "----------------------------------------\n",
      "Page 191:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "168\n",
      "Moderation and having a constitution are important in chatbots f\n",
      "----------------------------------------\n",
      "Page 192:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "169\n",
      "Next, you would create an instance of the LLMChain class or of a Runnable instance, wh\n",
      "----------------------------------------\n",
      "Page 193:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Building a Chatbot like ChatGPT\n",
      "170\n",
      "In the context of LLMs, guardrails (rails for short) refer to sp\n",
      "----------------------------------------\n",
      "Page 194:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 5\n",
      "171\n",
      "Additionally, we discussed memory mechanisms for maintaining knowledge and the state o\n",
      "----------------------------------------\n",
      "Page 195:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 196:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 6\n",
      "Developing Software with \n",
      "Generative AI\n",
      "While this book is about integrating generative AI particu\n",
      "----------------------------------------\n",
      "Page 197:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "174\n",
      "Software development and AI\n",
      "The emergence of powerful AI \n",
      "----------------------------------------\n",
      "Page 198:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "175\n",
      "•\t\n",
      "Code summarization/documentation: This task aims to generate a natural language sum\n",
      "----------------------------------------\n",
      "Page 199:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "176\n",
      "Microsoft’s GitHub Copilot, which is based on OpenAI’s Co\n",
      "----------------------------------------\n",
      "Page 200:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "177\n",
      "Figure 6.1: Model comparison on HumanEval coding task benchmark\n",
      "You can see lines mark\n",
      "----------------------------------------\n",
      "Page 201:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "178\n",
      "Although HumanEval has been broadly used as a benchmark f\n",
      "----------------------------------------\n",
      "Page 202:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "179\n",
      "This reflection and memory of past outcomes guides better future decisions. On coding \n",
      "----------------------------------------\n",
      "Page 203:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "180\n",
      "This screenshot shows the model in a playground on Huggin\n",
      "----------------------------------------\n",
      "Page 204:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "181\n",
      "    def __post_init__(self):\n",
      "        \"\"\"This method runs automatically once upon insta\n",
      "----------------------------------------\n",
      "Page 205:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "182\n",
      "        self.lastname =''.join([' '*(4-len(l))+' '+ l[-2:\n",
      "----------------------------------------\n",
      "Page 206:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "183\n",
      "We get the following result:\n",
      "@dataclass(frozen=True)  # frozen means that the object c\n",
      "----------------------------------------\n",
      "Page 207:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "184\n",
      "            return self.fullname < other.fullname\n",
      "       \n",
      "----------------------------------------\n",
      "Page 208:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "185\n",
      "This screenshot shows an example in StarChat, but please note that not all the code is\n",
      "----------------------------------------\n",
      "Page 209:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "186\n",
      "You can find the complete code listing on GitHub.\n",
      "For thi\n",
      "----------------------------------------\n",
      "Page 210:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "187\n",
      "Figure 6.5: Hugging Face chat with Llama 2 at https://huggingface.co/chat/\n",
      "Please note\n",
      "----------------------------------------\n",
      "Page 211:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "188\n",
      ")\n",
      "text = \"\"\"\n",
      "def calculate_primes(n):\n",
      "    \\\"\\\"\\\"Create a \n",
      "----------------------------------------\n",
      "Page 212:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "189\n",
      "Other models that have been instruction-tuned and are available for chat can act as yo\n",
      "----------------------------------------\n",
      "Page 213:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "190\n",
      "Action: Python_REPL\n",
      "Action Input:\n",
      "def is_prime(n):\n",
      "    fo\n",
      "----------------------------------------\n",
      "Page 214:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "191\n",
      "For example, the MetaGPT library approaches this with an agent simulation, where diffe\n",
      "----------------------------------------\n",
      "Page 215:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "192\n",
      "GPT Pilot\n",
      "https://github.com/Pythagora-\n",
      "io/gpt-pilot\n",
      "Pyth\n",
      "----------------------------------------\n",
      "Page 216:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "193\n",
      "The code-It project by Paolo Rechia and GPT Engineer by Anton Osika both follow a patt\n",
      "----------------------------------------\n",
      "Page 217:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "194\n",
      "We’ve discussed the basics of these two agent architectur\n",
      "----------------------------------------\n",
      "Page 218:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "195\n",
      "DEV_PROMPT = (\n",
      "    \"You are a software engineer who writes Python code given tasks or \n",
      "----------------------------------------\n",
      "Page 219:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "196\n",
      "Over multiple generations, the human-LLM loop allows for \n",
      "----------------------------------------\n",
      "Page 220:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "197\n",
      "    def write_code(self, task: str) -> str:\n",
      "        return self.llm_chain.run(task)\n",
      "  \n",
      "----------------------------------------\n",
      "Page 221:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "198\n",
      "Write a basic tetris game in Python with no syntax errors\n",
      "----------------------------------------\n",
      "Page 222:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "199\n",
      "    Tool(\n",
      "        name=\"DDGSearch\",\n",
      "        func=ddg_search.run,\n",
      "        description=(\n",
      "----------------------------------------\n",
      "Page 223:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "200\n",
      "window = pygame.display.set_mode((window_width, window_he\n",
      "----------------------------------------\n",
      "Page 224:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 6\n",
      "201\n",
      "LLMs can produce reasonable sets of test cases from high-level descriptions. But human\n",
      "----------------------------------------\n",
      "Page 225:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Developing Software with Generative AI\n",
      "202\n",
      "Join our community on Discord\n",
      "Join our community’s Discor\n",
      "----------------------------------------\n",
      "Page 226:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 7\n",
      "LLMs for Data Science\n",
      "This chapter is about how generative AI can automate data science. Generativ\n",
      "----------------------------------------\n",
      "Page 227:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "204\n",
      "Before delving into how data science can be automated, let’s start by disc\n",
      "----------------------------------------\n",
      "Page 228:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "205\n",
      "•\t\n",
      "Democratization of AI: Generative models allow many more people to leverage AI by \n",
      "\n",
      "----------------------------------------\n",
      "Page 229:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "206\n",
      "LLMs and generative AI can play a crucial role in automated data science b\n",
      "----------------------------------------\n",
      "Page 230:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "207\n",
      "One notable example is Microsoft’s Fabric, which incorporates a chat interface powered\n",
      "----------------------------------------\n",
      "Page 231:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "208\n",
      "Automating various aspects of the data science workflow allows data scient\n",
      "----------------------------------------\n",
      "Page 232:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "209\n",
      "It should be plain to see that having a chat like that at your fingertips to ask quest\n",
      "----------------------------------------\n",
      "Page 233:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "210\n",
      "Visualization and EDA\n",
      "EDA involves manually exploring and summarizing data\n",
      "----------------------------------------\n",
      "Page 234:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "211\n",
      "AutoML\n",
      "AutoML frameworks represent a noteworthy leap in the evolution of machine learn\n",
      "----------------------------------------\n",
      "Page 235:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "212\n",
      "Since the release of Auto-Weka, the landscape has vastly diversified with \n",
      "----------------------------------------\n",
      "Page 236:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "213\n",
      "With respect to ease of use, while AutoML with integrated LLMs offers simplified inter\n",
      "----------------------------------------\n",
      "Page 237:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "214\n",
      "With PythonREPLTool, we can create simple visualizations of toy data or tr\n",
      "----------------------------------------\n",
      "Page 238:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "215\n",
      "y_data = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
      "for epoch in range(1000):  # Train\n",
      "----------------------------------------\n",
      "Page 239:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "216\n",
      "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
      "\n",
      "----------------------------------------\n",
      "Page 240:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "217\n",
      "Data exploration with LLMs\n",
      "Data exploration is a crucial and foundational step in data\n",
      "----------------------------------------\n",
      "Page 241:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "218\n",
      "I’ve included instructions for the model to indicate uncertainty and to fo\n",
      "----------------------------------------\n",
      "Page 242:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "219\n",
      "We can also ask to see the distributions of the columns visually, which will give us t\n",
      "----------------------------------------\n",
      "Page 243:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "220\n",
      "We could extend this example by adding more instructions to the prompt abo\n",
      "----------------------------------------\n",
      "Page 244:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "221\n",
      "    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \n",
      "\"Italy\", \"Spa\n",
      "----------------------------------------\n",
      "Page 245:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): LLMs for Data Science\n",
      "222\n",
      "We can not only inquire about the nature of the dataset verbally but also \n",
      "----------------------------------------\n",
      "Page 246:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 7\n",
      "223\n",
      "In the next chapter, we’ll focus on conditioning techniques to improve the performance\n",
      "----------------------------------------\n",
      "Page 247:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 248:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 8\n",
      "Customizing LLMs and Their \n",
      "Output\n",
      "This chapter is about techniques and best practices to improve \n",
      "----------------------------------------\n",
      "Page 249:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "226\n",
      "The main sections in this chapter are:\n",
      "•\t\n",
      "Conditioning LLMs\n",
      "•\t\n",
      "----------------------------------------\n",
      "Page 250:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "227\n",
      "Conditioning can be applied at different points in a model’s life cycle. One strategy \n",
      "----------------------------------------\n",
      "Page 251:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "228\n",
      "In this chapter, we emphasize fine-tuning and prompting, as th\n",
      "----------------------------------------\n",
      "Page 252:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "229\n",
      "3.\t\n",
      "RL fine-tuning: The LM is fine-tuned via reinforcement learning to maximize the ex\n",
      "----------------------------------------\n",
      "Page 253:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "230\n",
      "In the next section, we’ll discuss methods to condition LLMs a\n",
      "----------------------------------------\n",
      "Page 254:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "231\n",
      "•\t\n",
      "Context-sensitive applications: Dynamic and context-specific applications like pers\n",
      "----------------------------------------\n",
      "Page 255:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "232\n",
      "Prompting enables conditioning models on new knowledge with lo\n",
      "----------------------------------------\n",
      "Page 256:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "233\n",
      "In this section, we’ll fine-tune a model for question answering. This recipe is not sp\n",
      "----------------------------------------\n",
      "Page 257:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "234\n",
      "We can install these libraries from the Colab notebook as foll\n",
      "----------------------------------------\n",
      "Page 258:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "235\n",
      "import os\n",
      "os.environ[\"WANDB_PROJECT\"] = \"finetuning\"\n",
      "To authenticate with W&B, you nee\n",
      "----------------------------------------\n",
      "Page 259:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "236\n",
      "We are taking both training and validation splits. The Squad V\n",
      "----------------------------------------\n",
      "Page 260:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "237\n",
      "model_id = \"openlm-research/open_llama_3b_v2\"\n",
      "new_model_name = f\"openllama-3b-peft-{da\n",
      "----------------------------------------\n",
      "Page 261:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "238\n",
      "We can set our output directory for model checkpoints and logs\n",
      "----------------------------------------\n",
      "Page 262:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "239\n",
      "    load_best_model_at_end=True,\n",
      "    report_to=\"wandb\"\n",
      ")\n",
      "A few comments to explain som\n",
      "----------------------------------------\n",
      "Page 263:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "240\n",
      "The training can take quite a while, even running on a TPU dev\n",
      "----------------------------------------\n",
      "Page 264:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "241\n",
      "from langchain.llms import HuggingFacePipeline\n",
      "model_id = 'openlm-research/open_llama_\n",
      "----------------------------------------\n",
      "Page 265:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "242\n",
      "Similarly, you can fine-tune the GPT-3.5 model for text classi\n",
      "----------------------------------------\n",
      "Page 266:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "243\n",
      "Figure 8.3: Prompt examples, particularly knowledge probing in close form, and summari\n",
      "----------------------------------------\n",
      "Page 267:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "244\n",
      "Instead of focusing on what not to do, clearly specify the des\n",
      "----------------------------------------\n",
      "Page 268:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "245\n",
      "CoT\n",
      "Prefix responses with \n",
      "intermediate reasoning \n",
      "steps\n",
      "Gives the model space to \n",
      "rea\n",
      "----------------------------------------\n",
      "Page 269:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "246\n",
      "In the next few subsections, we’ll go through a few of the afo\n",
      "----------------------------------------\n",
      "Page 270:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "247\n",
      "Let’s extend the previous example for sentiment classification with few-shot prompting\n",
      "----------------------------------------\n",
      "Page 271:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "248\n",
      "To choose examples tailored to each input, FewShotPromptTempla\n",
      "----------------------------------------\n",
      "Page 272:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "249\n",
      "content='Step 1: Originally, there were 5 apples.\\nStep 2: I ate 2 \n",
      "apples.\\nStep 3: S\n",
      "----------------------------------------\n",
      "Page 273:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "250\n",
      "In the first step, we’ll create multiple solutions to a questi\n",
      "----------------------------------------\n",
      "Page 274:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "251\n",
      "Let’s put these two chains together with a SequentialChain:\n",
      "from langchain.chains impo\n",
      "----------------------------------------\n",
      "Page 275:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "252\n",
      "Let’s first generate solutions:\n",
      "solutions_template = \"\"\"\n",
      "Gener\n",
      "----------------------------------------\n",
      "Page 276:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "253\n",
      "Finally, we can rank these solutions given our reasoning so far:\n",
      "ranking_template = \"\"\n",
      "----------------------------------------\n",
      "Page 277:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "254\n",
      "Please note how each output_key corresponds to an input_key in\n",
      "----------------------------------------\n",
      "Page 278:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 8\n",
      "255\n",
      "Prompt design is highly significant for unlocking LLM reasoning capabilities, and it o\n",
      "----------------------------------------\n",
      "Page 279:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Customizing LLMs and Their Output\n",
      "256\n",
      "Join our community on Discord\n",
      "Join our community’s Discord spa\n",
      "----------------------------------------\n",
      "Page 280:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 9\n",
      "Generative AI in Production\n",
      "As we’ve discussed in this book, LLMs have gained significant attentio\n",
      "----------------------------------------\n",
      "Page 281:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "258\n",
      "The main sections of this chapter are:\n",
      "•\t\n",
      "How to get LLM apps ready \n",
      "----------------------------------------\n",
      "Page 282:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "259\n",
      "•\t\n",
      "Model behavior standards: Models must align with ethical guidelines beyond basic fu\n",
      "----------------------------------------\n",
      "Page 283:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "260\n",
      "We need to optimize infrastructure and resource usage by employing d\n",
      "----------------------------------------\n",
      "Page 284:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "261\n",
      "Foundational Model Orchestration (FOMO) specifically addresses the challenges faced wh\n",
      "----------------------------------------\n",
      "Page 285:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "262\n",
      "The goal of evaluating LLMs is to understand their strengths and wea\n",
      "----------------------------------------\n",
      "Page 286:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "263\n",
      "Figure 9.1: Result of an evaluation with Claude as an evaluating language model\n",
      "In the\n",
      "----------------------------------------\n",
      "Page 287:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "264\n",
      "Other evaluators allow assessing model outputs based on specific cri\n",
      "----------------------------------------\n",
      "Page 288:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "265\n",
      "The output from the evaluator should look as follows:\n",
      "     {'reasoning': \"Both assista\n",
      "----------------------------------------\n",
      "Page 289:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "266\n",
      "LangChain supports both custom criteria and predefined principles fo\n",
      "----------------------------------------\n",
      "Page 290:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "267\n",
      "and subtext, Response A is the better response.\\n\\n[[A]]', 'value': 'A', \n",
      "'score': 1}\n",
      "\n",
      "----------------------------------------\n",
      "Page 291:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "268\n",
      "We can also use LangSmith, a companion project for LangChain that ai\n",
      "----------------------------------------\n",
      "Page 292:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "269\n",
      "We can create a dataset from existing agent runs with the create_example_from_run() fu\n",
      "----------------------------------------\n",
      "Page 293:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "270\n",
      ")\n",
      "for q in questions:\n",
      "    client.create_example(inputs={\"input\": q},\n",
      "----------------------------------------\n",
      "Page 294:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "271\n",
      "  llm_or_chain_factory=construct_chain,\n",
      "  evaluation=evaluation_config\n",
      ")\n",
      "Similarly, we\n",
      "----------------------------------------\n",
      "Page 295:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "272\n",
      "example, many natural substances can be harmful or deadly, such as c\n",
      "----------------------------------------\n",
      "Page 296:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "273\n",
      "This concludes the topic of evaluation here. Now that we’ve evaluated our agent, let’s\n",
      "----------------------------------------\n",
      "Page 297:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "274\n",
      "Some tools with infrastructure offer the full package. For example, \n",
      "----------------------------------------\n",
      "Page 298:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "275\n",
      "Steamship\n",
      "ML infrastructure platform for deploying and scaling \n",
      "models\n",
      "Cloud service\n",
      "L\n",
      "----------------------------------------\n",
      "Page 299:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "276\n",
      "For the greatest degree of flexibility, Infrastructure as Code (IaC)\n",
      "----------------------------------------\n",
      "Page 300:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "277\n",
      "First, we’ll import the necessary dependencies, including FastAPI for creating the web\n",
      "----------------------------------------\n",
      "Page 301:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "278\n",
      "An endpoint for handling HTTP GET requests at the root path (/) is d\n",
      "----------------------------------------\n",
      "Page 302:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "279\n",
      "Here’s a snapshot of the chatbot application we’ve just deployed:\n",
      "Figure 9.3: Chatbot \n",
      "----------------------------------------\n",
      "Page 303:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "280\n",
      "Ray\n",
      "Ray provides a flexible framework to meet the infrastructure cha\n",
      "----------------------------------------\n",
      "Page 304:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "281\n",
      "# Index vectors using FAISS via LangChain\n",
      "db = FAISS.from_documents(chunks, embeddings\n",
      "----------------------------------------\n",
      "Page 305:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "282\n",
      "class SearchDeployment:\n",
      "  def __init__(self):\n",
      "    self.db = db\n",
      "    s\n",
      "----------------------------------------\n",
      "Page 306:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "283\n",
      "For me, the server fetches the Ray use cases page at: https://docs.ray.io/en/latest/ra\n",
      "----------------------------------------\n",
      "Page 307:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "284\n",
      "As models and LLM apps grow more sophisticated and highly interwoven\n",
      "----------------------------------------\n",
      "Page 308:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "285\n",
      "The chief aim for monitoring and observability is to provide insights into LLM app per\n",
      "----------------------------------------\n",
      "Page 309:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "286\n",
      "When monitoring LLMs and LLM applications, organizations can rely on\n",
      "----------------------------------------\n",
      "Page 310:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "287\n",
      "Offline metrics such as AUC do not always correlate with online impacts on conversion \n",
      "----------------------------------------\n",
      "Page 311:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "288\n",
      "Tracking the trajectory of agents can be challenging due to their br\n",
      "----------------------------------------\n",
      "Page 312:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "289\n",
      ")\n",
      "result = agent(\"What's the latency like for https://langchain.com?\")\n",
      "The agent repor\n",
      "----------------------------------------\n",
      "Page 313:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "290\n",
      "•\t\n",
      "LLMonitor: Tracks lots of metrics including cost and usage analyt\n",
      "----------------------------------------\n",
      "Page 314:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "291\n",
      "Most of these integrations are very easy to integrate into LLM pipelines. For example,\n",
      "----------------------------------------\n",
      "Page 315:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "292\n",
      "On the LangSmith web interface, we can get a large set of graphs for\n",
      "----------------------------------------\n",
      "Page 316:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "293\n",
      "Here’s a tracing example in LangSmith for the benchmark dataset run that we saw in the\n",
      "----------------------------------------\n",
      "Page 317:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "294\n",
      "PromptWatch\n",
      "PromptWatch records information about response caching, \n",
      "----------------------------------------\n",
      "Page 318:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "295\n",
      "Figure 9.7: Prompt tracking at PromptWatch.io\n",
      "We can see the prompt together with the \n",
      "----------------------------------------\n",
      "Page 319:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Generative AI in Production\n",
      "296\n",
      "The evaluation of LLMs is important to assess their performance and \n",
      "----------------------------------------\n",
      "Page 320:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 9\n",
      "297\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with \n",
      "----------------------------------------\n",
      "Page 321:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 322:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): 10\n",
      "The Future of Generative Models\n",
      "In this book, so far, we have discussed generative models for bui\n",
      "----------------------------------------\n",
      "Page 323:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "300\n",
      "The main sections of this chapter are:\n",
      "•\t\n",
      "The current state of g\n",
      "----------------------------------------\n",
      "Page 324:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "301\n",
      "Here is a table summarizing the key strengths and deficiencies of current generative \n",
      "----------------------------------------\n",
      "Page 325:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "302\n",
      "Challenges\n",
      "The profound potential of generative AI systems indic\n",
      "----------------------------------------\n",
      "Page 326:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "303\n",
      "Governance\n",
      "Compliance frameworks and ethical development \n",
      "governance\n",
      "Table 10.2: Chal\n",
      "----------------------------------------\n",
      "Page 327:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "304\n",
      "Trends in model development\n",
      "The current doubling time in trainin\n",
      "----------------------------------------\n",
      "Page 328:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "305\n",
      "However, future progress may depend more on data efficiency and model quality than sh\n",
      "----------------------------------------\n",
      "Page 329:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "306\n",
      "The rapidly decreasing costs of AI model training represent a si\n",
      "----------------------------------------\n",
      "Page 330:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "307\n",
      "These innovations collectively lower barriers that have so far impeded real-world gen\n",
      "----------------------------------------\n",
      "Page 331:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "308\n",
      "The central question is how these scenarios will coexist and evo\n",
      "----------------------------------------\n",
      "Page 332:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "309\n",
      "Artificial General Intelligence\n",
      "Not all abilities in LLMs scale predictably with mode\n",
      "----------------------------------------\n",
      "Page 333:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "310\n",
      "Given current model limitations and the lack of agency, the noti\n",
      "----------------------------------------\n",
      "Page 334:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "311\n",
      "Here are some key predictions about how jobs may be impacted by advances in language \n",
      "----------------------------------------\n",
      "Page 335:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "312\n",
      "As AI models become more sophisticated and economical to operate\n",
      "----------------------------------------\n",
      "Page 336:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "313\n",
      "Let’s look at various sectors where generative models will have profound near-term im\n",
      "----------------------------------------\n",
      "Page 337:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "314\n",
      "In advertising, AIGC unlocks new potential for efficient, custom\n",
      "----------------------------------------\n",
      "Page 338:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "315\n",
      "Education\n",
      "One potential near-future scenario is that the rise of personalized AI tuto\n",
      "----------------------------------------\n",
      "Page 339:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "316\n",
      "Manufacturing\n",
      "In the automotive sector, generative models are em\n",
      "----------------------------------------\n",
      "Page 340:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "317\n",
      "The advent of highly capable generative AI will likely transform many aspects of soci\n",
      "----------------------------------------\n",
      "Page 341:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "318\n",
      "One of the major problems that I can see is misinformation, eith\n",
      "----------------------------------------\n",
      "Page 342:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "319\n",
      "Regulations and implementation challenges\n",
      "Realizing the potential of generative AI in\n",
      "----------------------------------------\n",
      "Page 343:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "320\n",
      "A current German law on fake news, which imposes a 24-hour timef\n",
      "----------------------------------------\n",
      "Page 344:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Chapter 10\n",
      "321\n",
      "The road ahead\n",
      "The forthcoming era of generative AI models offers a plethora of intri\n",
      "----------------------------------------\n",
      "Page 345:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): The Future of Generative Models\n",
      "322\n",
      "For corporations, effective governance frameworks have yet to be\n",
      "----------------------------------------\n",
      "Page 346:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): packt.com\n",
      "Subscribe to our online digital library for full access to over 7,000 books and videos, as\n",
      "----------------------------------------\n",
      "Page 347:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n",
      "Page 348:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Other Books \n",
      " \n",
      "You May Enjoy\n",
      "If you enjoyed this book, you may be interested in these other books by\n",
      "----------------------------------------\n",
      "Page 349:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Other Books You May Enjoy\n",
      "326\n",
      "Building LLM Apps\n",
      "Valentina Alto\n",
      "ISBN: 9781835462317\n",
      "•\t\n",
      "Core component\n",
      "----------------------------------------\n",
      "Page 350:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Other Books You May Enjoy\n",
      "327\n",
      "Generative AI Engineering\n",
      "Konrad Banachewicz\n",
      "ISBN: 9781805120513\n",
      "•\t\n",
      "Ge\n",
      "----------------------------------------\n",
      "Page 351:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Other Books You May Enjoy\n",
      "328\n",
      "Packt is searching for authors like you\n",
      "If you’re interested in becomi\n",
      "----------------------------------------\n",
      "Page 352:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "Symbols\n",
      "2021 AI boom/bust cycle  312\n",
      "A\n",
      "AgentOps  261\n",
      "agents  52, 53\n",
      "benefits  52\n",
      "AI, for softw\n",
      "----------------------------------------\n",
      "Page 353:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "330\n",
      "Boom Phase  312\n",
      "Bust Phase   312\n",
      "Byte-Pair Encoding (BPE)  25\n",
      "C\n",
      "Chain of Density (CoD)  10\n",
      "----------------------------------------\n",
      "Page 354:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "331\n",
      "dependencies\n",
      "setting up  65-67\n",
      "DocArray  156\n",
      "Docker\n",
      "cons  66\n",
      "pros  66\n",
      "reference link  68\n",
      "u\n",
      "----------------------------------------\n",
      "Page 355:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "332\n",
      "impact, on data science  204-206\n",
      "need for  8-11\n",
      "techniques and approaches, for making \n",
      "acc\n",
      "----------------------------------------\n",
      "Page 356:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "333\n",
      "memory  54, 55\n",
      "retrievers  148-151\n",
      "tools  55, 56\n",
      "working  57-59\n",
      "LangChain API\n",
      "reference li\n",
      "----------------------------------------\n",
      "Page 357:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "334\n",
      "locality sensitive hashing (LSH)  141\n",
      "local models\n",
      "exploring  85\n",
      "low-rank adaptation  229,\n",
      "----------------------------------------\n",
      "Page 358:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "335\n",
      "self-consistency prompting  249-251\n",
      "Tree-of-Thought (ToT) prompting  251-255\n",
      "zero-shot pro\n",
      "----------------------------------------\n",
      "Page 359:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Index\n",
      "336\n",
      "text-to-image models  27-32\n",
      "applications  27\n",
      "tokenization  24\n",
      "token usage\n",
      "monitoring  109-\n",
      "----------------------------------------\n",
      "Page 360:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): Download a free PDF copy of this book\n",
      "Thanks for purchasing this book!\n",
      "Do you like to read on the go\n",
      "----------------------------------------\n",
      "Page 361:\n",
      "  Width: 540.0\n",
      "  Height: 666.0\n",
      "  Rotation: 0\n",
      "  Text (first 100 chars): \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def get_page_metadata(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Create a list to store metadata for each page\n",
    "    page_metadata = []\n",
    "    \n",
    "    # Iterate over each page\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        \n",
    "        # Get metadata for the current page\n",
    "        metadata = {\n",
    "            'page_number': page_num + 1,\n",
    "            'width': page.rect.width,\n",
    "            'height': page.rect.height,\n",
    "            'rotation': page.rotation,\n",
    "            'text': page.get_text()  # Extracting text (optional)\n",
    "        }\n",
    "        \n",
    "        # Add metadata to the list\n",
    "        page_metadata.append(metadata)\n",
    "    \n",
    "    # Close the document\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return page_metadata\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'example.pdf'\n",
    "\n",
    "# Get and print the metadata for each page\n",
    "metadata_list = get_page_metadata(pdf_path)\n",
    "for metadata in metadata_list:\n",
    "    print(f\"Page {metadata['page_number']}:\")\n",
    "    print(f\"  Width: {metadata['width']}\")\n",
    "    print(f\"  Height: {metadata['height']}\")\n",
    "    print(f\"  Rotation: {metadata['rotation']}\")\n",
    "    print(f\"  Text (first 100 chars): {metadata['text'][:100]}\")\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 2:\n",
      "Topics:\n",
      "  Generative AI with LangChain\n",
      "Build large language model (LLM) apps with Python, \n",
      "ChatGPT, and other LLMs\n",
      "Ben Auffarth\n",
      "BIRMINGHAM—MUMBAI\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 3:\n",
      "Topics:\n",
      "  Packt Publishing has endeavored to provide trademark information about all of the companies and products \n",
      "mentioned in this book by the appropriate use of capitals.\n",
      "  Manju Arasan\n",
      "Presentation Designer:\n",
      "  Tanya D’cruz and Elliot Dallow\n",
      "Copy Editor: Safis Editing\n",
      "Technical Editor: Kushal Sharma\n",
      "Proofreader: Safis Editing\n",
      "Indexer:\n",
      "  Neither the author, nor Packt Publishing or its dealers and distributors, will be held liable for any \n",
      "damages caused or alleged to have been caused directly or indirectly by this book.\n",
      "\n",
      "  Ajay Patule\n",
      "Developer Relations Marketing Executive: Monika Sangwan\n",
      "First published: December 2023\n",
      "Production reference: 1141223\n",
      "Published by Packt Publishing Ltd.\n",
      "Grosvenor House\n",
      "11 St Paul’s Square\n",
      "Birmingham\n",
      "B3 1RB, UK.\n",
      "ISBN 978-1-83508-346-8\n",
      "www.packt.com\n",
      "\n",
      "  Senior Publishing Product Manager: Tushar Gupta\n",
      "Acquisition Editor – Peer Reviews: Tejas Mhasvekar\n",
      "Project Editor: Namrata Katare\n",
      "Content Development Editors:\n",
      "  However, Packt Publishing cannot guarantee \n",
      "the accuracy of this information.\n",
      "\n",
      "  Generative AI with LangChain\n",
      "Copyright © 2023 Packt Publishing\n",
      "All rights reserved.\n",
      "Subtopics:\n",
      "  However, the information contained in this book is sold without warranty, either express or \n",
      "implied.\n",
      "  Every effort has been made in the preparation of this book to ensure the accuracy of the information \n",
      "presented.\n",
      "  No part of this book may be reproduced, stored in a retrieval system, or transmitted in \n",
      "any form or by any means, without the prior written permission of the publisher, except in the case of brief \n",
      "quotations embedded in critical articles or reviews.\n",
      "\n",
      "----------------------------------------\n",
      "Page 4:\n",
      "Topics:\n",
      "  To Diane and Nico\n",
      "– Ben Auffarth\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 5:\n",
      "Topics:\n",
      "  I am equally delighted with my astute editors — Tanya, Elliot, and Kushal.\n",
      "  Ben has analyzed terabytes of data, simulated brain activity on supercomputers \n",
      "with up to 64k cores, designed and conducted wet lab experiments, built production systems \n",
      "processing underwriting applications, and trained neural networks on millions of documents. \n",
      "\n",
      "  Tanya, in particular, was instrumental in guiding me \n",
      "through the writing process, continually challenging me to clarify my thoughts and significantly \n",
      "shaping the final product.\n",
      "\n",
      "  Foremost, I extend my heartfelt gratitude to Leo, whose insightful feedback significantly \n",
      "refined this book.\n",
      "  Contributors\n",
      "About the author\n",
      "Ben Auffarth is a seasoned data science leader with a background and Ph.D. in computational \n",
      "neuroscience.\n",
      "  He now works in insurance at Hastings Direct.\n",
      "\n",
      "  He’s the author of the books Machine Learning for Time Series and Artificial Intelligence with Python \n",
      "Cookbook.\n",
      "Subtopics:\n",
      "  Creating this book has been a long and sometimes arduous journey, but also an exciting one.\n",
      "  It has \n",
      "been enriched immeasurably by the contributions of several key individuals to whom I owe great \n",
      "thanks.\n",
      "  Their \n",
      "efforts went above and beyond expectations.\n",
      "----------------------------------------\n",
      "Page 6:\n",
      "Topics:\n",
      "  Currently, she is leveraging her skills as a product marketing \n",
      "manager in the rapidly evolving field of data science and AI at HP.\n",
      "  Thank you, Mom \n",
      "and Dad, for always being there for me.\n",
      "\n",
      "  He is \n",
      "an active contributor to LangChain and several other open-source projects.\n",
      "  Ruchi Bhatia is a computer engineer with a Master’s degree in information systems management \n",
      "from Carnegie Mellon University.\n",
      "  She takes pride in being the \n",
      "youngest triple Kaggle Grandmaster across the Notebooks, Datasets, and Discussion categories. \n",
      "\n",
      "  Her previous role as the Leader of Data Science at OpenMined allowed her to steer a team of data \n",
      "scientists to create innovative and impactful solutions.\n",
      "\n",
      "  About the reviewers\n",
      "Leonid Ganeline is a machine learning engineer with extensive experience in natural language \n",
      "processing.\n",
      "  Without their belief in my abilities \n",
      "and their constant guidance, I wouldn’t have achieved the milestones I have today.\n",
      "  His interest lies in \n",
      "model evaluation, especially in LLM evaluation\n",
      "I would like to express my gratitude to my parents, for teaching me how to think rationally, and to \n",
      "my wife, for supporting me in this endeavor.\n",
      "\n",
      "Subtopics:\n",
      "  Their unwavering support and \n",
      "encouragement throughout my journey have been invaluable.\n",
      "  He has worked in several start-ups, creating models and production systems.\n",
      "  I want to take a moment to express my heartfelt thanks to my parents.\n",
      "----------------------------------------\n",
      "Page 7:\n",
      "Topics:\n",
      "  Join our community on Discord\n",
      "Join our community's Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 8:\n",
      "Topics:\n",
      "  • 8\n",
      "Understanding LLMs......................................................................................................... 11\n",
      "\n",
      "  • 20\n",
      "Pre-training • 23\n",
      "Tokenization • 24\n",
      "Scaling • 25\n",
      "Conditioning • 26\n",
      "How to try out these models • 27\n",
      "What are text-to-image models?\n",
      "  Table of Contents\n",
      "Preface\n",
      " xv\n",
      "Chapter 1: What Is Generative AI?\n",
      "  33\n",
      "Summary.......................................................................................................................... 34\n",
      "Questions..........................................................................................................................  35\n",
      "\n",
      "  2\n",
      "What are generative models?\n",
      "  • 4\n",
      "Why now?\n",
      " \n",
      " 1\n",
      "Introducing generative AI...................................................................................................\n",
      "  What is a GPT?\n",
      "  • 13\n",
      "Other LLMs • 16\n",
      "Major players • 18\n",
      "How do GPT models work?\n",
      "  27\n",
      "\n",
      "Subtopics:\n",
      " ....................................................................................\n",
      " .......................................................................................  \n",
      "  What can AI do in other domains?\n",
      "----------------------------------------\n",
      "Page 9:\n",
      "Topics:\n",
      "  • 54\n",
      "\n",
      "  • 42\n",
      "\n",
      "  • 43\n",
      "\n",
      "  • 52\n",
      "\n",
      "  How does LangChain work?\n",
      "  50\n",
      "\n",
      "  46\n",
      "\n",
      "  What is LangChain?\n",
      "  62\n",
      "Chapter 3: Getting Started with LangChain\n",
      " 65\n",
      "How to set up the dependencies for this book................................................................... 65\n",
      "pip • 67\n",
      "Poetry • 68\n",
      "Conda • 68\n",
      "Docker • 68\n",
      "Exploring API model integrations..................................................................................... 69\n",
      "Fake LLM • 72\n",
      "OpenAI • 73\n",
      "Hugging Face • 75\n",
      "Google Cloud Platform • 77\n",
      "Jina AI • 80\n",
      "Replicate • 82\n",
      "Others • 84\n",
      "\n",
      "  Exploring key components of LangChain.........................................................................\n",
      "  • 50\n",
      "\n",
      "  Table of Contents\n",
      "viii\n",
      "Chapter 2: LangChain for LLM Apps\n",
      " 37\n",
      "Going beyond stochastic parrots....................................................................................... 38\n",
      "\n",
      "  57\n",
      "Comparing LangChain with other frameworks................................................................ 60\n",
      "Summary..........................................................................................................................  61\n",
      "Questions..........................................................................................................................\n",
      "Subtopics:\n",
      " ..........................................................................................................\n",
      "  • 38\n",
      "\n",
      " ..............................................................................................  \n",
      "  • 55\n",
      "\n",
      "  What is an LLM app?\n",
      "  How can we mitigate LLM limitations?\n",
      "  What are chains?\n",
      "  What are the limitations of LLMs?\n",
      "  What are tools?\n",
      "  What is memory?\n",
      "  What are agents?\n",
      "----------------------------------------\n",
      "Page 10:\n",
      "Topics:\n",
      "  89\n",
      "Summary..........................................................................................................................\n",
      "  Basic prompting • 103\n",
      "Prompt templates • 104\n",
      "Chain of density • 105\n",
      "Map-Reduce pipelines • 107\n",
      "Monitoring token usage • 109\n",
      "Extracting information from documents.........................................................................\n",
      "  Understanding retrieval and vectors................................................................................ 134\n",
      "Embeddings • 135\n",
      "Vector storage • 139\n",
      "\n",
      " \n",
      "............................................................................................................ 132\n",
      "\n",
      "  96\n",
      "Chapter 4: Building Capable Assistants\n",
      " 99\n",
      "Mitigating hallucinations through fact-checking........................................................... 100\n",
      "Summarizing information..............................................................................................  103\n",
      "\n",
      "  112\n",
      "Answering questions with tools....................................................................................... 116\n",
      "Information retrieval with tools • 116\n",
      "Building a visual interface • 118\n",
      "\n",
      "  96\n",
      "Questions..........................................................................................................................\n",
      "  121\n",
      "Summary......................................................................................................................... 129\n",
      "Questions........................................................................................................................  130\n",
      "Chapter 5: Building a Chatbot like ChatGPT\n",
      " 131\n",
      "\n",
      "  Table of Contents\n",
      "ix\n",
      "Azure • 84\n",
      "Anthropic • 85\n",
      "Exploring local models..................................................................................................... 85\n",
      "Hugging Face Transformers • 86\n",
      "llama.cpp • 87\n",
      "GPT4All • 88\n",
      "Building an application for customer service....................................................................\n",
      "Subtopics:\n",
      "  Exploring reasoning strategies\n",
      ".........................................................................................\n",
      "  What is a chatbot?\n",
      "----------------------------------------\n",
      "Page 11:\n",
      "Topics:\n",
      "  Remembering conversation summaries • 164\n",
      "Storing knowledge graphs • 164\n",
      "Combining several memory mechanisms • 165\n",
      "Long-term persistence • 166\n",
      "Moderating responses...................................................................................................... 167\n",
      "Summary........................................................................................................................  170\n",
      "Questions.........................................................................................................................  171\n",
      "Chapter 6: Developing Software with Generative AI\n",
      " 173\n",
      "Software development and AI..........................................................................................\n",
      "  179\n",
      "StarCoder • 179\n",
      "StarChat • 184\n",
      "Llama 2 • 186\n",
      "Small local model • 187\n",
      "\n",
      "  Table of Contents\n",
      "x\n",
      "Vector indexing • 140\n",
      "Vector libraries • 141\n",
      "Vector databases • 143\n",
      "Loading and retrieving in LangChain\n",
      "..............................................................................  148\n",
      "Document loaders • 149\n",
      "Retrievers in LangChain • 150\n",
      "kNN retriever • 151\n",
      "PubMed retriever • 152\n",
      "Custom retrievers • 153\n",
      "Implementing a chatbot.................................................................................................. 153\n",
      "Document loader • 154\n",
      "Vector storage • 155\n",
      "Memory • 160\n",
      "Conversation buffers • 161\n",
      "\n",
      "  174\n",
      "Code LLMs • 175\n",
      "Writing code with LLMs..................................................................................................\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 12:\n",
      "Topics:\n",
      "  Table of Contents\n",
      "xi\n",
      "Automating software development.................................................................................  189\n",
      "Summary........................................................................................................................  201\n",
      "Questions........................................................................................................................  201\n",
      "Chapter 7: LLMs for Data Science\n",
      " 203\n",
      "\n",
      "  Conditioning LLMs......................................................................................................... 226\n",
      "Methods for conditioning • 227\n",
      "Reinforcement learning with human feedback • 228\n",
      "Low-rank adaptation • 229\n",
      "Inference-time conditioning • 230\n",
      "Fine-tuning\n",
      ".....................................................................................................................\n",
      "  232\n",
      "Setup for fine-tuning • 233\n",
      "Open-source models • 236\n",
      "Commercial models • 241\n",
      "Prompt engineering........................................................................................................ 242\n",
      "Prompt techniques • 244\n",
      "Zero-shot prompting • 246\n",
      "Few-shot learning • 246\n",
      "\n",
      "  The impact of generative models on data science........................................................... 204\n",
      "Automated data science.................................................................................................. 207\n",
      "Data collection • 209\n",
      "Visualization and EDA • 210\n",
      "Preprocessing and feature extraction • 210\n",
      "AutoML • 211\n",
      "Using agents to answer data science questions................................................................ 213\n",
      "Data exploration with LLMs............................................................................................ 217\n",
      "Summary........................................................................................................................ 222\n",
      "Questions........................................................................................................................  223\n",
      "Chapter 8: Customizing LLMs and Their Output\n",
      " 225\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 13:\n",
      "Topics:\n",
      "  Table of Contents\n",
      "xii\n",
      "Chain-of-thought prompting • 248\n",
      "Self-consistency • 249\n",
      "Tree-of-thought • 251\n",
      "Summary........................................................................................................................  255\n",
      "Questions........................................................................................................................  255\n",
      "Chapter 9: Generative AI in Production\n",
      " 257\n",
      "How to get LLM apps ready for production..................................................................... 258\n",
      "Terminology • 260\n",
      "How to evaluate LLM apps............................................................................................... 261\n",
      "\n",
      "  Comparing two outputs • 264\n",
      "Comparing against criteria • 265\n",
      "String and semantic comparisons • 267\n",
      "Running evaluations against datasets • 268\n",
      "How to deploy LLM apps\n",
      ".................................................................................................  273\n",
      "FastAPI web server • 276\n",
      "Ray • 280\n",
      "How to observe LLM apps............................................................................................... 284\n",
      "Tracking responses • 287\n",
      "Observability tools • 289\n",
      "LangSmith • 291\n",
      "PromptWatch • 294\n",
      "Summary........................................................................................................................\n",
      "  295\n",
      "Questions........................................................................................................................ 296\n",
      "Chapter 10: The Future of Generative Models\n",
      " 299\n",
      "The current state of generative AI..................................................................................  300\n",
      "Challenges • 302\n",
      "Trends in model development • 304\n",
      "Big Tech vs. small enterprises • 307\n",
      "Artificial General Intelligence • 309\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 14:\n",
      "Topics:\n",
      "  Table of Contents\n",
      "xiii\n",
      "Economic consequences.................................................................................................  310\n",
      "Creative industries and advertising • 313\n",
      "Education • 315\n",
      "Law • 315\n",
      "Manufacturing • 316\n",
      "Medicine • 316\n",
      "Military • 316\n",
      "Societal implications.......................................................................................................\n",
      "  317\n",
      "Misinformation and cybersecurity • 318\n",
      "Regulations and implementation challenges • 319\n",
      "\n",
      "  321\n",
      "Other Books You May Enjoy\n",
      " 325\n",
      "Index\n",
      " 329\n",
      "\n",
      "Subtopics:\n",
      "  The road ahead................................................................................................................\n",
      "----------------------------------------\n",
      "Page 15:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 16:\n",
      "Topics:\n",
      "  As we embark on this journey together, let us prime ourselves to shape and be shaped by the \n",
      "generative AI narrative that’s unfolding at this very moment–a narrative where you, armed with \n",
      "knowledge and foresight, stand at the forefront of this exhilarating technological evolution.\n",
      "\n",
      "  Venture into the depths of deep learning, where unstructured data comes alive, and discover \n",
      "how LLMs like GPT-4 and others are carving a path for AI’s impact on businesses, societies, and \n",
      "individuals.\n",
      "  Preface\n",
      "In the dynamic and rapidly advancing field of AI, generative AI stands out as a disruptive force \n",
      "poised to transform how we interact with technology.\n",
      "  This book is an expedition into the intri-\n",
      "cate world of large language models (LLMs) – the powerful engines driving this transformation \n",
      "– designed to equip developers, researchers, and AI aficionados with the knowledge needed to \n",
      "harness these tools.\n",
      "\n",
      "  Written for a diverse audience, from \n",
      "those taking their first steps in AI to seasoned developers, the text melds theoretical concepts \n",
      "with practical, code-rich examples, preparing you to not only grasp LLMs intellectually but to \n",
      "also apply them inventively and responsibly.\n",
      "\n",
      "Subtopics:\n",
      "  It is written in a clear and concise style, and it includes plenty of code examples \n",
      "to help you learn by doing. \n",
      "\n",
      "  We provide a prelude to their vast applications, the elegance of their underlying \n",
      "architecture, and the powerful implications of their existence.\n",
      "  This book serves as your compass, pointing you toward understanding the technical scaffolds that \n",
      "uphold LLMs.\n",
      "  With the tech industry and media abuzz with the capabilities and potential of these \n",
      "models, it’s an opportune moment to explore how they function, thrive, and propel us toward \n",
      "future horizons.\n",
      "\n",
      "  Who this book is for\n",
      "The book is intended for developers, researchers, and anyone else who is interested in learning \n",
      "more about LLMs.\n",
      "----------------------------------------\n",
      "Page 17:\n",
      "Topics:\n",
      "  Addressing limitations like outdated knowledge, action limitations, and hallucination \n",
      "risks, the chapter highlights how LangChain integrates external data and interventions for more \n",
      "coherent AI applications.\n",
      "  It begins with installation guidance for Dock-\n",
      "er, Conda, Pip, and Poetry.\n",
      "  The chapter then details integrating models from various providers \n",
      "like OpenAI’s ChatGPT and Hugging Face, including obtaining necessary API keys.\n",
      "  It explores the Chain of \n",
      "Density for information extraction and discusses LangChain decorators and expression language \n",
      "for customizing behavior.\n",
      "  The chapter outlines the evolution \n",
      "of AI, Transformer architecture, text-to-image models like Stable Diffusion, and touches on sound \n",
      "and video applications.\n",
      "\n",
      "  Chapter 4, Building Capable Assistants, tackles turning LLMs into reliable assistants by weaving \n",
      "in fact-checking to reduce misinformation, employing sophisticated prompting strategies for \n",
      "summarization, and integrating external tools for enhanced knowledge.\n",
      "  What this book covers\n",
      "Chapter 1, What Is Generative AI?, explains how generative AI has revolutionized the processing of \n",
      "text, images, and video, with deep learning at its core.\n",
      "  Preface\n",
      "xvi\n",
      "Whether you are a beginner or an experienced developer, this book will be a valuable resource \n",
      "for anyone who wants to get the most out of LLMs and to stay ahead of the curve about LLMs \n",
      "and LangChain.\n",
      "\n",
      "  The chapter culminates in constructing an LLM app \n",
      "to assist customer service agents, exemplifying how LangChain can streamline operations and \n",
      "enhance the accuracy of responses.\n",
      "\n",
      "  Chapter 2, LangChain for LLM Apps, uncovers the need to expand beyond the stochastic parrots \n",
      "of LLMs–models that mimic language without true understanding–by harnessing LangChain’s \n",
      "framework.\n",
      "  Chapter 3, Getting Started with LangChain, provides foundational knowledge for you to set up \n",
      "your environment to run all examples in the book.\n",
      "  The chapter introduces map-reduce in LangChain for handling long \n",
      "documents and discusses token monitoring to manage API usage costs. \n",
      "\n",
      "Subtopics:\n",
      "  This chapter covers the theory behind these models, highlighting neural networks and \n",
      "training approaches, and the creation of human-like content.\n",
      "  The chapter critically engages with the concept of stochastic parrots, \n",
      "revealing the deficiencies in models that produce fluent but meaningless language, and explicates \n",
      "how prompting, chain-of-thought reasoning, and retrieval grounding augment LLMs to address \n",
      "issues of contextuality, bias, and intransparency.\n",
      "\n",
      "  It also deals \n",
      "with running open-source models locally.\n",
      "  This chapter introduces generative models \n",
      "such as LLMs, detailing their technical underpinnings and transformative potential across various \n",
      "sectors.\n",
      "----------------------------------------\n",
      "Page 18:\n",
      "Topics:\n",
      "  Chapter 5, Building a Chatbot like ChatGPT, delves into enhancing chatbot capabilities with re-\n",
      "trieval-augmented generation (RAG), a method that provides LLMs with access to external \n",
      "knowledge, improving their accuracy and domain-specific proficiency.\n",
      "  Preface\n",
      "xvii\n",
      "It looks at implementing a Streamlit application to create interactive LLM applications and using \n",
      "function calling and tool usage to transcend basic text generation.\n",
      "  Chapter 7, LLMs for Data Science, explores the intersection of generative AI and data science, spot-\n",
      "lighting LLMs’ potential to amplify productivity and drive scientific discovery.\n",
      "  Critical reflections on the agent’s performance emphasize \n",
      "the importance of human oversight for error mitigation and high-level design, setting the stage \n",
      "for a future where AI and human developers work symbiotically.\n",
      "\n",
      "  The chapter not only provides concrete examples of fine-tuning and \n",
      "prompting but also discusses the future of LLM advancements and their applications in the field.\n",
      "\n",
      "  The chatbot, available on GitHub, serves as a basis for exploring \n",
      "advanced topics like dialogue memory and context management.\n",
      "\n",
      "  Chapter 6, Developing Software with Generative AI, examines the burgeoning role of LLMs in software \n",
      "development, highlighting the potential for AI to automate coding tasks and serve as dynamic \n",
      "coding assistants.\n",
      "  Chapter 8, Customizing LLMs and Their Output, delves into conditioning techniques like fine-tuning \n",
      "and prompting, essential for tailoring LLM performance to complex reasoning and specialized \n",
      "tasks.\n",
      "  It covers practical methods for LLMs to conduct exploratory data analysis, run SQL queries, \n",
      "and visualize statistical data.\n",
      "  This chapter discusses \n",
      "document vectorization, efficient indexing, and the use of vector databases like Milvus and Pine-\n",
      "cone for semantic search.\n",
      "  We unpack fine-tuning, where an LLM is further trained on task-specific data, and prompt \n",
      "engineering, which strategically guides the LLM to generate desired outputs.\n",
      "  It explores the current state of AI-driven software development, experiments \n",
      "with models to generate code snippets, and introduces a design for an automated software de-\n",
      "velopment agent using LangChain.\n",
      "  Two distinct agent paradigms, \n",
      "plan-and-solve and zero-shot, are implemented to demonstrate decision-making strategies. \n",
      "\n",
      "Subtopics:\n",
      "  Advanced prompting \n",
      "strategies such as few-shot learning and chain-of-thought are implemented, enhancing the rea-\n",
      "soning capabilities of LLMs.\n",
      "  Finally, the use of agents and tools demonstrates how LLMs can \n",
      "address complex data-centric questions.\n",
      "\n",
      "  The chapter outlines \n",
      "the current scope of automation in data science through AutoML and extends this notion with \n",
      "the integration of LLMs for advanced tasks like augmenting datasets and generating executable \n",
      "code.\n",
      "  We implement a chatbot, incorporating moderation chains to ensure \n",
      "responsible communication.\n",
      "----------------------------------------\n",
      "Page 19:\n",
      "Topics:\n",
      "  The code bundle for the book is hosted on GitHub at https://github.com/benman1/generative_\n",
      "ai_with_langchain.\n",
      "  These tools can provide automated evaluation \n",
      "and metrics that support the responsible adoption of generative AI across sectors.\n",
      "\n",
      "  This final chapter emphasizes the importance of steering AI development toward augmenting \n",
      "human potential while addressing risks such as deepfakes, bias, and the weaponization of AI.\n",
      "  Chapter 10, The Future of Generative Models, ventures into the potential advancements and so-\n",
      "cio-technical challenges of generative AI.\n",
      "  It underscores the importance of \n",
      "evaluation, observability, and systematic operation to make generative AI beneficial in customer \n",
      "engagement and decision-making with financial consequences.\n",
      "  Download the color images\n",
      "We also provide a PDF file that has color images of the screenshots/diagrams used in this book. \n",
      "\n",
      "  You can download it here: https://packt.link/gbp/9781835083468.\n",
      "\n",
      "  It also outlines practical strat-\n",
      "egies for deployment and ongoing monitoring of LLM apps using tools like Fast API, Ray, and \n",
      "newcomers such as LangServe and LangSmith.\n",
      "  Preface\n",
      "xviii\n",
      "Chapter 9, Generative AI in Production, addresses the complexities of deploying LLMs within re-\n",
      "al-world applications, covering best practices for ensuring performance, meeting regulatory \n",
      "requirements, robustness at scale, and effective monitoring.\n",
      "Subtopics:\n",
      "  As various sectors brace for disruptive AI-induced changes, it reflects on the respon-\n",
      "sibility of corporations, lawmakers, and technologists to forge effective governance frameworks. \n",
      "\n",
      "  It \n",
      "highlights the urgency for transparency, ethical deployment, and equitable access to guide the \n",
      "generative AI revolution positively.\n",
      "\n",
      "  To get the most out of this book\n",
      "To benefit from the value this book offers, it is essential to have a foundational understanding of \n",
      "Python.\n",
      "  Conventions used\n",
      "There are a number of text conventions used throughout this book.\n",
      "\n",
      "  It examines the economic and societal impacts of these \n",
      "technologies, debating job displacement, misinformation, and ethical concerns like human value \n",
      "alignment.\n",
      "  Additionally, possessing some basic knowledge of machine learning is recommended.\n",
      "Download the example code files\n",
      "\n",
      "  We also have other code bundles from our rich catalog of books and videos \n",
      "available at https://github.com/PacktPublishing/.\n",
      "  Check them out!\n",
      "\n",
      "----------------------------------------\n",
      "Page 20:\n",
      "Topics:\n",
      "  \"\n",
      "When we wish to draw your attention to a particular part of a code block, the relevant lines or \n",
      "items are set in bold:\n",
      "from pandasai.llm.openai import OpenAI\n",
      "llm = OpenAI(api_token=\"YOUR_API_TOKEN\")\n",
      "\n",
      "  For example: “Select System info from \n",
      "the Administration panel.”\n",
      "\n",
      "  Any command-line input or output is written as follows:\n",
      "pip install -r requirements.txt\n",
      "Bold: Indicates a new term, an important word, or words that you see on the screen.\n",
      "  A block of code is set as follows:\n",
      "from langchain.chains import LLMCheckerChain\n",
      "from langchain.llms import OpenAI\n",
      "llm = OpenAI(temperature=0.7)\n",
      "text = \"What type of mammal lays the biggest eggs?\n",
      "  Warnings or important notes appear like this.\n",
      "\n",
      "  Indicates code words in text, database table names, folder names, filenames, file \n",
      "extensions, pathnames, dummy URLs, user input, and Twitter handles.\n",
      "Subtopics:\n",
      "  For instance, \n",
      "words in menus or dialog boxes appear in the text like this.\n",
      "  pandas_ai = PandasAI(llm)\n",
      "\n",
      "  Preface\n",
      "xix\n",
      "CodeInText:\n",
      "  Tips and tricks appear like this.\n",
      "\n",
      "  For example: “Mount the \n",
      "downloaded WebStorm-10*.dmg disk image file as another disk in your system.”\n",
      "\n",
      "----------------------------------------\n",
      "Page 21:\n",
      "Topics:\n",
      "  Please visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\n",
      "\n",
      "  Preface\n",
      "xx\n",
      "Get in touch\n",
      "Feedback from our readers is always welcome.\n",
      "\n",
      "  If you have questions about any aspect of this book, please email us at questions@\n",
      "packtpub.com.\n",
      "Errata: Although we have taken every care to ensure the accuracy of our content, mistakes do \n",
      "happen.\n",
      "  Share your thoughts\n",
      "Once you’ve read Generative AI with LangChain, we’d love to hear your thoughts!\n",
      "  Please click \n",
      "here to go straight to the Amazon review page for this book and share your feedback.\n",
      "\n",
      "Subtopics:\n",
      "  Piracy: If you come across any illegal copies of our works in any form on the internet, we would \n",
      "be grateful if you would provide us with the location address or website name.\n",
      "  Your review is important to us and the tech community and will help us make sure we’re deliv-\n",
      "ering excellent quality content.\n",
      "\n",
      "  General feedback: Email feedback@packtpub.com and mention the book’s title in the subject of \n",
      "your message.\n",
      "  If you are interested in becoming an author: If there is a topic that you have expertise in and you \n",
      "are interested in either writing or contributing to a book, please visit http://authors.packtpub.\n",
      "com.\n",
      "\n",
      "  Please contact us \n",
      "at copyright@packtpub.com with a link to the material.\n",
      "\n",
      "  If you have found a mistake in this book, we would be grateful if you reported this to us. \n",
      "\n",
      "----------------------------------------\n",
      "Page 22:\n",
      "Topics:\n",
      "  Scan the QR code or visit the link below\n",
      "https://packt.link/free-ebook/9781835083468\n",
      "2.\t\n",
      "Submit your proof of purchase\n",
      "3.\t\n",
      "\n",
      "  We’ll send your free PDF and other benefits to your email directly\n",
      "\n",
      "  Download a free PDF copy of this book\n",
      "Thanks for purchasing this book!\n",
      "\n",
      "  Is your eBook \n",
      "purchase not compatible with the device of your choice?\n",
      "Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n",
      "\n",
      "  The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \n",
      "content in your inbox daily\n",
      "Follow these simple steps to get the benefits:\n",
      "1.\t\n",
      "\n",
      "Subtopics:\n",
      "  Do you like to read on the go but are unable to carry your print books everywhere?\n",
      "  Read anywhere, any place, on any device.\n",
      "  That’s it!\n",
      "  Search, copy, and paste code from your favorite technical \n",
      "books directly into your application. \n",
      "\n",
      "----------------------------------------\n",
      "Page 23:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 24:\n",
      "Topics:\n",
      "  These advanced AI models have gained popularity in various \n",
      "industries, and include large language models (LLMs).\n",
      "  Let’s start from the beginning – by introducing the terminology!\n",
      "\n",
      "  1\n",
      "What Is Generative AI?\n",
      "Over the last decade, deep learning has evolved massively to process and generate unstructured \n",
      "data like text, images, and video.\n",
      "  There is currently a significant level of \n",
      "fanfare in both the media and the industry surrounding AI, and there’s a fair case to be made \n",
      "that Artificial Intelligence (AI), with these advancements, is about to have a wide-ranging and \n",
      "major impact on businesses, societies, and individuals alike.\n",
      "Subtopics:\n",
      "  With this foundation, readers will be \n",
      "better prepared to consider both the opportunities and challenges posed by this rapidly advanc-\n",
      "ing technology.\n",
      "\n",
      "  We’ll provide an overview \n",
      "of the technical concepts and training approaches that power these models’ ability to produce \n",
      "novel content.\n",
      "  The goal is to demystify the underlying magic that allows these models to generate \n",
      "remarkably human-like content across various domains.\n",
      "  In this chapter, we’ll explore generative models and their application.\n",
      "  We’ll follow this structure:\n",
      "•\t\n",
      "Introducing generative AI\n",
      "•\t\n",
      "Understanding LLMs\n",
      "•\t\n",
      "What are text-to-image models?\n",
      "•\t\n",
      "What can AI do in other domains?\n",
      "\n",
      "  This is driven by numerous factors, \n",
      "including advancements in technology, high-profile applications, and the potential for transfor-\n",
      "mative impacts across multiple sectors.\n",
      "\n",
      "  While we won’t be diving deep into generative models for sound or video, we aim \n",
      "to convey a high-level understanding of how techniques like neural networks, large datasets, \n",
      "and computational scale enable generative models to reach new capabilities in text and image \n",
      "generation.\n",
      "----------------------------------------\n",
      "Page 25:\n",
      "Topics:\n",
      "  These range from advancements in Natural Language Processing (NLP) and computer \n",
      "vision to the development of sophisticated language models like GPT-4.\n",
      "  Benchmarks capturing task performance in different domains have been major drivers of the de-\n",
      "velopment of these models.\n",
      "  The following graph, inspired by a blog post titled GPT-4 Predictions \n",
      "by Stephen McAleese on LessWrong, shows the improvements of LLMs in the Massive Multitask \n",
      "Language Understanding (MMLU) benchmark, which was designed to quantify knowledge and \n",
      "problem-solving ability in elementary mathematics, US history, computer science, law, and more:\n",
      "Figure 1.1: Average performance on the MMLU benchmark of LLMs\n",
      "Generative AI refers to algorithms that can generate novel content, as opposed to \n",
      "analyzing or acting on existing data like more traditional, predictive machine learn-\n",
      "ing or AI systems.\n",
      "\n",
      "  What Is Generative AI?\n",
      "2\n",
      "Introducing generative AI\n",
      "In the media, there is substantial coverage of AI-related breakthroughs and their potential impli-\n",
      "cations.\n",
      "Subtopics:\n",
      "  This allows cost savings with automation and allows humans to leverage their \n",
      "creativity to an unprecedented level.\n",
      "\n",
      "  These same \n",
      "models also provide wide functionality including semantic search, content manipulation, and \n",
      "classification.\n",
      "  Particularly, generative \n",
      "models have received a lot of attention due to their ability to generate text, images, and other \n",
      "creative content that is often indistinguishable from human-generated content.\n",
      "----------------------------------------\n",
      "Page 26:\n",
      "Topics:\n",
      "  As AI models like OpenAI’s GPT continue to improve, they could become indispensable assets to \n",
      "teams in need of diverse knowledge and skills. \n",
      "\n",
      "  Generative Pre-trained Transformer (GPT) models, like OpenAI’s GPT-4, are prime examples of \n",
      "AI advancements in the sphere of LLMs.\n",
      "  First and foremost, the massive scaling in parameters from \n",
      "1.5 billion (GPT-2) to 175 billion (GPT-3) to more than a trillion (GPT-4) enables models to learn \n",
      "more complex patterns; however, another major change in early 2022 was the post-training \n",
      "fine-tuning of models based on human instructions, which teaches the model how to perform a \n",
      "task by providing demonstrations and feedback.\n",
      "\n",
      "  Particularly, it highlights \n",
      "the progress of the models provided through a public user interface by OpenAI, especially the \n",
      "improvements between releases, from GTP-2 to GPT-3 and GPT-3.5 to GPT-4, although the results \n",
      "should be taken with a grain of salt, since they are self-reported and are obtained either by 5-shot \n",
      "or zero-shot conditioning.\n",
      "  These AI-based chatbots can generate human-like responses as real-time feedback to \n",
      "customers and can be applied to a wide range of use cases, from software development to writing \n",
      "poetry and business communications.\n",
      "\n",
      "  Zero-shot means the models were prompted with the question, while \n",
      "in 5-shot settings, models were additionally given 5 question-answer examples.\n",
      "  Chapter 1\n",
      "3\n",
      "You can see significant improvements in recent years in this benchmark.\n",
      "  Please note that while most benchmark results come from 5-shot, a few, like the \n",
      "GPT-2, PaLM, and PaLM-2 results, refer to zero-shot conditioning.\n",
      "\n",
      "  These added \n",
      "examples could naively account for about 20% of performance according to Measuring Massive \n",
      "Multitask Language Understanding (Hendrycks and colleagues, revised 2023).\n",
      "\n",
      "Subtopics:\n",
      "  These achievements \n",
      "of human engineering are impressive; however, it should be noted that the performance of these \n",
      "models depends on the field; most models are still performing poorly on the GSM8K benchmark \n",
      "of grade school math word problems.\n",
      "\n",
      "  There are a few differences between these models and their training that can account for these \n",
      "boosts in performance, such as scale, instruction-tuning, a tweak to the attention mechanisms, \n",
      "and more and different training data.\n",
      "  ChatGPT has been widely adopted by the general pub-\n",
      "lic, showing greatly improved chatbot capabilities enabled by being much bigger than previous \n",
      "models.\n",
      "  Across benchmarks, a few models have recently started to perform better than an average human \n",
      "rater, but generally still haven’t reached the performance of a human expert.\n",
      "----------------------------------------\n",
      "Page 27:\n",
      "Topics:\n",
      "  In 2018, Elon Musk resigned from the \n",
      "board citing a potential conflict of interest with his role at Tesla.\n",
      "  The most significant achievements of the company include OpenAI Gym for training \n",
      "reinforcement algorithms, and – more recently – the GPT-n models and the DALL-E \n",
      "generative models, which generate images from text.\n",
      "\n",
      "  It \n",
      "was established in 2015 with the support of several influential figures and companies, \n",
      "who pledged over $1 billion to the venture.\n",
      "  In 2019, OpenAI \n",
      "transitioned to become a for-profit organization, and subsequently Microsoft made \n",
      "significant investments in OpenAI, leading to the integration of OpenAI systems \n",
      "with Microsoft’s Azure-based supercomputing platform and the Bing search engine. \n",
      "\n",
      "  As these AI models become more proficient and easily accessible, they are \n",
      "likely to play a significant role in shaping the future of work and learning.\n",
      "\n",
      "  In theoretical and applied research circles, it is often joked that AI is just a fancy word for ML, or \n",
      "AI is ML in a suit, as illustrated in this image:\n",
      "OpenAI is a US AI research company that aims to promote and develop friendly AI.\n",
      "  What Is Generative AI?\n",
      "4\n",
      "For example, GPT-4 could be considered a polymath that works tirelessly without demanding \n",
      "compensation (beyond subscription or API fees), providing competent assistance in subjects like \n",
      "mathematics and statistics, macroeconomics, biology, and law (the model performs well on the \n",
      "Uniform Bar Exam).\n",
      "  Let’s clear up the terminology a bit and explain in more detail what is meant by generative model, \n",
      "artificial intelligence, deep learning, and machine learning.\n",
      "\n",
      "Subtopics:\n",
      "  By making knowledge more accessible and adaptable, these models have the potential to level \n",
      "the playing field and create new opportunities for people from all walks of life.\n",
      "  What are generative models?\n",
      "\n",
      "  In popular media, the term artificial intelligence is used a lot when referring to these new models. \n",
      "\n",
      "  As for generative models with images, they have pushed the boundaries in their capabilities to \n",
      "assist in creating visual content, and their performance in computer vision tasks such as object \n",
      "detection, segmentation, captioning, and much more.\n",
      "\n",
      "  The organization initially committed to \n",
      "being non-profit, collaborating with other institutions and researchers by making \n",
      "its patents and research open to the public.\n",
      "  These models have \n",
      "shown potential in areas that require higher levels of reasoning and understanding, although \n",
      "progress varies depending on the complexity of the tasks involved.\n",
      "\n",
      "----------------------------------------\n",
      "Page 28:\n",
      "Topics:\n",
      "  •\t\n",
      "Generative Models are a type of ML model that can generate new data based on patterns \n",
      "learned from input data.\n",
      "\n",
      "  Generated by a model on replicate.com, Diffusers Stable Diffusion v2.1\n",
      "It’s worth distinguishing more clearly between the terms generative model, artificial intelligence, \n",
      "machine learning, deep learning, and language model:\n",
      "•\t\n",
      "Artificial Intelligence (AI) is a broad field of computer science focused on creating intel-\n",
      "ligent agents that can reason, learn, and act autonomously.\n",
      "\n",
      "  •\t\n",
      "Machine Learning (ML) is a subset of AI focused on developing algorithms that can \n",
      "learn from data.\n",
      "\n",
      "  Chapter 1\n",
      "5\n",
      "  \n",
      "Figure 1.2: ML in a suit.\n",
      "  •\t\n",
      "Language Models (LMs) are statistical models used to predict words in a sequence of \n",
      "natural language.\n",
      "  •\t\n",
      "Deep Learning (DL) uses deep neural networks, which have many layers, as a mechanism \n",
      "for ML algorithms to learn complex patterns from data.\n",
      "\n",
      "Subtopics:\n",
      "  Some language models utilize deep learning and are trained on massive \n",
      "datasets, becoming large language models (LLMs).\n",
      "\n",
      "----------------------------------------\n",
      "Page 29:\n",
      "Topics:\n",
      "  Generative models are a powerful type of AI that can generate new data that resembles the train-\n",
      "ing data.\n",
      "  What Is Generative AI?\n",
      "6\n",
      "This class diagram illustrates how LLMs combine deep learning techniques like neural networks \n",
      "with sequence modeling objectives from language modeling, at a very large scale:\n",
      "Figure 1.3: Class diagram of different models.\n",
      "  Examples: DALL-E 2, \n",
      "Stable Diffusion, and Imagen.\n",
      "•\t\n",
      "Text-to-audio: Models that generate audio clips and music from text.\n",
      "  Examples: Jukebox, \n",
      "AudioLM, and MusicGen.\n",
      "\n",
      "  Generative models facilitate the cre-\n",
      "ation of synthetic data to train AI models when real data is scarce or restricted.\n",
      "  Microsoft Research took this \n",
      "approach (Textbooks Are All You Need, June 2023) to training their phi-1 model, where they used \n",
      "GPT-3.5 to create synthetic Python textbooks and exercises.\n",
      "\n",
      "  Ex-\n",
      "amples: LLaMa 2, GPT-4, Claude, and PaLM 2.\n",
      "\n",
      "Subtopics:\n",
      "  •\t\n",
      "Text-to-image:\n",
      "  This enables applications like generating text, images, music, and video.\n",
      "\n",
      "  LLMs represent the intersection of deep learning \n",
      "techniques with language modeling objectives.\n",
      "\n",
      "  Models that generate images from text captions.\n",
      "  There are many types of generative models, handling different data modalities across various \n",
      "domains.\n",
      "  They are:\n",
      "•\t\n",
      "Text-to-text: Models that generate text from input text, like conversational agents.\n",
      "  These models can handle different data modalities and are \n",
      "employed across various domains, including text, image, music, and video.\n",
      "\n",
      "  The key distinction is that generative models synthesize new data rather than just making pre-\n",
      "dictions or decisions.\n",
      "  This type of data \n",
      "generation reduces labeling costs and improves training efficiency.\n",
      "  Some language models are generative, while some are not.\n",
      "  Generative AI models have come a long way, enabling the generation of new examples \n",
      "from scratch using patterns in data.\n",
      "----------------------------------------\n",
      "Page 30:\n",
      "Topics:\n",
      "  •\t\n",
      "Speech-to-text:\n",
      "  •\t\n",
      "Image-to-text:\n",
      "  Example: \n",
      "Phenaki and Emu Video.\n",
      "\n",
      "  Examples: Stable Dif-\n",
      "fusion and DALL-E 3.\n",
      "•\t\n",
      "Video-to-audio: Models that analyze video and generate matching audio.\n",
      "  The LLM categories are the \n",
      "main focus of this book; however, we’ll also occasionally look at other models, text-to-image in \n",
      "particular.\n",
      "  Chapter 1\n",
      "7\n",
      "•\t\n",
      "Text-to-video: Models that generate video content from text descriptions.\n",
      "  A few models are specialized in scientific text, such as Minerva or Galactica, or algorithm \n",
      "discovery, such as AlphaTensor.\n",
      "\n",
      "  These models typically use a Transformer architecture trained on massive datasets \n",
      "via self-supervised learning.\n",
      "\n",
      "  Examples: WaveNet \n",
      "and Tacotron.\n",
      "\n",
      "  Models that transcribe speech to text [also called Automatic Speech \n",
      "Recognition (ASR)].\n",
      "  An example of a model that \n",
      "demonstrates generative capabilities in multimodal input is OpenAI’s GPT-4V model (GPT-4 with \n",
      "vision), released in September 2023, which takes both text and images and comes with better \n",
      "Optical Character Recognition (OCR) than previous versions to read text from images.\n",
      "  Further, we could consider subcategories of text, such as text-to-math, which generates \n",
      "mathematical expressions from text, where some models such as ChatGPT and Claude shine, or \n",
      "text-to-code, which are models that generate programming code from text, such as AlphaCode or \n",
      "Codex.\n",
      "  Examples: CLIP and \n",
      "DALL-E 3.\n",
      "•\t\n",
      "Image-to-image: Applications for this type of model are data augmentation such as su-\n",
      "per-resolution, style transfer, and inpainting.\n",
      "\n",
      "Subtopics:\n",
      "  A few models work with several modalities for input or output.\n",
      "  Models that generate programming code from text.\n",
      "  There are a lot more combinations of modalities to consider; these are just some that I have come \n",
      "across.\n",
      "  Images \n",
      "can be translated into descriptive words, then existing text filters are applied.\n",
      "  This mitigates the \n",
      "risk of generating unconstrained image captions.\n",
      "\n",
      "  Examples: Whisper and SpeechGPT.\n",
      "\n",
      "  •\t\n",
      "Text-to-code:\n",
      "  The outputs can also be converted back into text or within the same \n",
      "modality.\n",
      "  Models that generate image captions from images.\n",
      "  •\t\n",
      "Text-to-speech:\n",
      "  LLMs have driven rapid progress for text-focused domains.\n",
      "  As the list shows, text is a common input modality that can be converted into various outputs \n",
      "like image, audio, and video.\n",
      "  These models enable a \n",
      "diverse range of capabilities via different modalities and domains.\n",
      "  Models that synthesize speech audio from input text.\n",
      "  Example: Soun-\n",
      "dify.\n",
      "\n",
      "----------------------------------------\n",
      "Page 31:\n",
      "Topics:\n",
      "  However, it was the advent of DL, a type of neural network with numerous layers, \n",
      "that marked a significant turning point in the performance and capabilities of these models. \n",
      "\n",
      "  The backpropagation algorithm introduced in the 1980s \n",
      "by Geoffrey Hinton, David Rumelhart, and Ronald Williams is one such example.\n",
      "  This \n",
      "graph shows the cost of computer storage over time for different mediums such as disks, solid \n",
      "state, flash, and internal memory in terms of price in dollars per terabyte (adapted from Our \n",
      "World in Data by Max Roser, Hannah Ritchie, and Edouard Mathieu; https://ourworldindata.\n",
      "org/grapher/historical-cost-of-computer-memory-and-storage:\n",
      "\n",
      "  In the 2000s, neural networks began to regain popularity as researchers developed more complex \n",
      "architectures.\n",
      "  The success of generative AI coming into the public spotlight in 2022 can be attributed to several \n",
      "interlinked drivers.\n",
      "  But there are key challenges such as data availability, compute requirements, \n",
      "bias in data, evaluation difficulties, potential misuse, and other societal impacts that need to \n",
      "be addressed going forward, which we’ll discuss in Chapter 10, The Future of Generative Models.\n",
      "\n",
      "  What Is Generative AI?\n",
      "8\n",
      "The rapid progress shows the potential of generative AI across diverse domains.\n",
      "  Within the in-\n",
      "dustry, there is a growing sense of excitement around AI’s capabilities and its potential impact on \n",
      "business operations.\n",
      "  Interestingly, although the concept of DL has existed for some time, the development and expan-\n",
      "sion of generative models correlate with significant advances in hardware, particularly Graphics \n",
      "Processing Units (GPUs), which have been instrumental in propelling the field forward.\n",
      "\n",
      "Subtopics:\n",
      "  This is because DL models require a lot of computing power \n",
      "to train and run.\n",
      "  This concerns all aspects of processing power, memory, and disk space.\n",
      "  Why now?\n",
      "\n",
      "  As mentioned, the availability of cheaper and more powerful hardware has been a key factor in \n",
      "the development of deeper models.\n",
      "  It provided a \n",
      "way to effectively train multi-layer neural networks.\n",
      "\n",
      "  The development and success of generative models have relied on improved \n",
      "algorithms, considerable advances in compute power and hardware design, the availability of \n",
      "large, labeled datasets, and an active and collaborative research community helping to evolve a \n",
      "set of tools and techniques.\n",
      "Developing more sophisticated mathematical and computational methods has played a vital \n",
      "role in advancing generative models.\n",
      "  Let’s delve a bit more into this progress and pose the question why now ?\n",
      "\n",
      "----------------------------------------\n",
      "Page 32:\n",
      "Topics:\n",
      "  Particularly, it seems that in models with between 2 and 7 billion parameters, new \n",
      "capabilities emerge such as the ability to generate different creative text in formats \n",
      "like poems, code, scripts, musical pieces, emails, and letters, and to answer even \n",
      "open-ended and challenging questions in an informative way.\n",
      "\n",
      "  While, in the past, training a DL model was prohibitively expensive, as the cost of hardware has \n",
      "come down, it has become possible to train bigger models on much larger datasets.\n",
      "  As a simple example of these higher-order correlations, an \n",
      "LLM could learn that the word “cat” is more likely to be followed by the word “dog” \n",
      "if it is preceded by the word “chase,” even if there are other words in between.\n",
      "  Chapter 1\n",
      "9\n",
      "Figure 1.4: Cost of computer storage since the 1950s in dollars (unadjusted) per terabyte\n",
      "\n",
      "Subtopics:\n",
      "  Gen-\n",
      "erally, the lower a model’s perplexity, the better it will perform, for example, in terms \n",
      "of answering questions.\n",
      "\n",
      "  The model \n",
      "size is one of the factors determining how well a model can approximate (as measured in per-\n",
      "plexity) the training dataset.\n",
      "\n",
      "  The importance of the number of parameters in an LLM: The more parameters \n",
      "a model has, the higher its capacity to capture relationships between words and \n",
      "phrases as knowledge.\n",
      "----------------------------------------\n",
      "Page 33:\n",
      "Topics:\n",
      "  The development of transfer learning techniques, which allow a model pre-trained on one task \n",
      "to be fine-tuned on another, similar task, has also been significant.\n",
      "  Many LLM papers describe the use of NVIDIA A100s for training.\n",
      "\n",
      "  In the 2010s, several types of generative models started gaining traction.\n",
      "  What Is Generative AI?\n",
      "10\n",
      "This trend toward larger models started around 2009, when NVIDIA catalyzed what is often \n",
      "called the Big Bang of DL. GPUs are particularly well suited for the matrix/vector computations \n",
      "necessary to train deep learning neural networks, therefore significantly increasing the speed and \n",
      "efficiency of these systems by several orders of magnitude and reducing running times from weeks \n",
      "to days.\n",
      "  Around the \n",
      "same time, GANs were proposed by Ian Goodfellow and others in 2014.\n",
      "\n",
      "  Over the past decade, significant advancements have been made in the fundamental algorithms \n",
      "used in DL, such as better optimization methods, more sophisticated model architectures, and \n",
      "improved regularization techniques.\n",
      "  Transformer models, introduced in 2017, built upon this \n",
      "progress and enabled the creation of large-scale models like GPT-3.\n",
      "  Moreover, part of the rise of gen-\n",
      "erative models can be attributed to the development of software libraries and tools (TensorFlow, \n",
      "PyTorch, and Keras) specifically designed to work with these artificial neural networks, stream-\n",
      "lining the process of building, training, and deploying them.\n",
      "\n",
      "  These \n",
      "models, such as Google’s BERT and OpenAI’s GPT series, can generate highly coherent and con-\n",
      "textually relevant text.\n",
      "\n",
      "  The explosion of data available from the internet, particularly in the last decade, has \n",
      "created a suitable environment for such models to thrive.\n",
      "  In particular, NVIDIA’s CUDA platform, which allows direct programming of GPUs, has \n",
      "made it easier than ever for researchers and developers to experiment with and deploy complex \n",
      "generative models facilitating breakthroughs in vision, speech recognition, and – more recently \n",
      "– LLMs.\n",
      "  Autoencoders, a kind of \n",
      "neural network that can learn to compress data from the input layer to a representation, and then \n",
      "reconstruct the input, served as a basis for more advanced models like Variational Autoencoders \n",
      "(VAEs), which were first proposed in 2013.\n",
      "Subtopics:\n",
      "  These techniques have made \n",
      "it more efficient and practical to train large generative models.\n",
      "  As the internet has become more popular, \n",
      "it has become easier to collect large datasets of text, images, and other data. \n",
      "\n",
      "  This \n",
      "is because DL models, particularly generative ones, require vast amounts of text data for effective \n",
      "training.\n",
      "  VAEs, unlike traditional autoencoders, use variational \n",
      "inference to learn the distribution of data, also called the latent space of input data.\n",
      "  Transformers rely on atten-\n",
      "tion mechanisms and resulted in a further leap in the performance of generative models.\n",
      "  In addition to the availability of cheaper and more powerful hardware, the availability of large \n",
      "datasets of labeled data has also been a key factor in the development of generative models.\n",
      "----------------------------------------\n",
      "Page 34:\n",
      "Topics:\n",
      "  These models have practical applications in \n",
      "fields like content creation and NLP, where the ultimate goal is to create algorithms capable of \n",
      "understanding and generating natural language text.\n",
      "\n",
      "  To further drive the development of generative models, the research \n",
      "community has been developing benchmarks and other challenges, like the mentioned MMLU \n",
      "and ImageNet for image classification, and has started to do the same for generative models.\n",
      "\n",
      "  Representation learning is about a model learning its internal representations of \n",
      "raw data to perform a machine learning task, rather than relying only on engineered \n",
      "feature extraction.\n",
      "  Chapter 1\n",
      "11\n",
      "This has made it possible to train generative models on much larger datasets than would have \n",
      "been possible in the past.\n",
      "  These models form \n",
      "the backbone of larger NLP tasks, such as content creation, translation, summarization, machine \n",
      "translation, and text-editing tasks such as spelling correction.\n",
      "\n",
      "  At its core, language modeling, and more broadly NLP, relies heavily on the quality of representa-\n",
      "tion learning.\n",
      "  Understanding LLMs\n",
      "Text generation models, such as GPT-4 by OpenAI, can generate coherent and grammatically \n",
      "correct text in different languages and formats.\n",
      "Subtopics:\n",
      "  The model isn’t told explicitly what features to look for – it \n",
      "learns representations of the raw pixel data that help it make predictions.\n",
      "\n",
      "  I am excited to \n",
      "see what the future holds for this field.\n",
      "\n",
      "  In summary, generative modeling is a fascinating and rapidly evolving field.\n",
      "  It has the potential \n",
      "to revolutionize the way we interact with computers and create original content.\n",
      "  Language modeling aims to predict the next word, character, or even sentence based on the pre-\n",
      "vious ones in a sequence.\n",
      "  For example, an image classification model based on representa-\n",
      "tion learning might learn to represent images according to visual features like edges, \n",
      "shapes, and textures.\n",
      "  In this sense, language modeling serves as a way of encoding the rules \n",
      "and structures of a language in a way that can be understood by a machine.\n",
      "  A generative language model encodes information about the text that it has been \n",
      "trained on and generates new text based on those learnings, thereby taking on the task of text \n",
      "generation.\n",
      "\n",
      "  LLMs capture the \n",
      "structure of human language in terms of grammar, syntax, and semantics.\n",
      "----------------------------------------\n",
      "Page 35:\n",
      "Topics:\n",
      "  •\t\n",
      "Automatic summarization:\n",
      "  •\t\n",
      "Semantic search: LLMs can focus on understanding meaning within individual documents. \n",
      "\n",
      "  •\t\n",
      "Topic modeling: LLMs can discover abstract topics and themes across a corpus of docu-\n",
      "ments.\n",
      "  More broadly, applications of language models \n",
      "involve multiple areas, such as:\n",
      "•\t\n",
      "Question answering: AI chatbots and virtual assistants can provide personalized and \n",
      "efficient assistance, reducing response times in customer support and thereby enhanc-\n",
      "ing customer experience.\n",
      "  New generative models can \n",
      "perform on par with commercial products (for example, Google Translate).\n",
      "\n",
      "  •\t\n",
      "Machine translation: Language models can translate texts from one language into an-\n",
      "other, supporting businesses in their global expansion efforts.\n",
      "  We’ll talk about hallucinations in Chapter 5, Building a Chatbot \n",
      "Like ChatGPT, but for now, let’s discuss the technical background of LLMs in some more detail.\n",
      "\n",
      "  What Is Generative AI?\n",
      "12\n",
      "Recently, LLMs have found applications for tasks like essay generation, code development, trans-\n",
      "lation, and understanding genetic sequences.\n",
      "  It uses NLP to interpret words and concepts for improved search relevance.\n",
      "\n",
      "Subtopics:\n",
      "  It remains uncertain whether continually in-\n",
      "creasing the scale of language models will inevitably lead to new reasoning capabilities.\n",
      "  This is a feature as well as a bug since it \n",
      "highlights their creative potential.\n",
      "  •\t\n",
      "Sentiment analysis: By analyzing opinions and emotions in texts, language models can \n",
      "help businesses understand customer feedback and opinions more efficiently.\n",
      "\n",
      "  Further, \n",
      "LLMs are known to return the most probable answers within the context, which can sometimes \n",
      "yield fabricated information, termed hallucinations.\n",
      "  Despite the remarkable achievements, language models still face limitations when dealing with \n",
      "complex mathematical or logical reasoning tasks.\n",
      "  These systems can be used in specific contexts like restaurant \n",
      "reservations and ticket booking.\n",
      "\n",
      "  Language models can create concise summaries of articles, \n",
      "research papers, and other content, enabling users to consume and understand infor-\n",
      "mation rapidly.\n",
      "\n",
      "  It identifies word clusters and latent semantic structures.\n",
      "\n",
      "----------------------------------------\n",
      "Page 36:\n",
      "Topics:\n",
      "  Chapter 1\n",
      "13\n",
      "What is a GPT?\n",
      "LLMs are deep neural networks adept at understanding and generating human language.\n",
      "  A transformer is a DL architecture, first introduced in 2017 by researchers at Google and the \n",
      "University of Toronto (in an article called Attention Is All You Need; Vaswani and colleagues), that \n",
      "comprises self-attention and feed-forward neural networks, allowing it to effectively capture \n",
      "the word relationships in a sentence.\n",
      "  Models have evolved rapidly, \n",
      "enabling the creation of versatile foundational AI models suitable for a wide range of downstream \n",
      "tasks and modalities, ultimately driving innovation across various applications and industries.\n",
      "\n",
      "  This \n",
      "combination of unsupervised and supervised learning enables GPT models to perform better \n",
      "across a range of NLP tasks and reduces the challenges associated with training LLMs.\n",
      "\n",
      "  This is referred to as a hallucination and is just \n",
      "one of the concerns around LLMs.\n",
      "\n",
      "  Generative Pre-Trained Transformers (GPTs), on the other hand, were introduced by research-\n",
      "ers at OpenAI in 2018 together with the first of their eponymous GPT models, GPT-1 (Improving \n",
      "Language Understanding by Generative Pre-Training; Radford and others).\n",
      "  However, \n",
      "ChatGPT has been observed to “sometimes write plausible sounding but incorrect or nonsensical \n",
      "answers,” as expressed in a disclaimer by OpenAI.\n",
      "Subtopics:\n",
      "  The \n",
      "current generation of LLMs such as ChatGPT are deep neural network architectures that utilize \n",
      "the transformer model and undergo pre-training using unsupervised learning on extensive text \n",
      "data, enabling the model to learn language patterns and structures.\n",
      "  The attention mechanism enables the model to focus on \n",
      "various parts of the input sequence.\n",
      "\n",
      "  The notable strength of the latest generation of LLMs as conversational interfaces (chatbots) lies \n",
      "in their ability to generate coherent and contextually appropriate responses, even in open-ended \n",
      "conversations.\n",
      "  By generating the next word based on the preceding words repeatedly, the model \n",
      "produces fluent and coherent text often indistinguishable from text produced by humans.\n",
      "  The pre-training process \n",
      "involves predicting the next word in a text sequence, enhancing the model’s grasp of language \n",
      "as measured in the quality of the output.\n",
      "  Following pre-training, the model can be fine-tuned \n",
      "for specific language processing tasks like sentiment analysis, language translation, or chat.\n",
      "----------------------------------------\n",
      "Page 37:\n",
      "Topics:\n",
      "  BERT, released in the same \n",
      "year, was trained on a combined corpus of BookCorpus and English Wikipedia, totaling 3.3 billion \n",
      "words.\n",
      "  Now, training corpora for LLMs reach up to trillions of tokens.\n",
      "\n",
      "  This graph illustrates how LLMs have been growing:\n",
      "Figure 1.5: LLMs from BERT to GPT-4 – size, training budget, and organizations.\n",
      "  A \n",
      "petaFLOP/s day is a unit of throughput that consists of performing 10 to the power of 15 opera-\n",
      "tions per day.\n",
      "  GPT-1, introduced by \n",
      "OpenAI in 2018, was trained on BookCorpus with 985 million words.\n",
      "  Training operations in the calculations are estimated as the approximate number \n",
      "of addition and multiplication operations based on the GPU utilization efficiency.\n",
      "\n",
      "  What Is Generative AI?\n",
      "14\n",
      "The size of the training corpus for LLMs has been increasing drastically.\n",
      "Subtopics:\n",
      "  For the pro-\n",
      "prietary models, parameter sizes are often estimates.\n",
      "\n",
      "  The size of the data points indicates training cost in terms of petaFLOPs and petaFLOP/s-days.\n",
      "----------------------------------------\n",
      "Page 38:\n",
      "Topics:\n",
      "  GPT models can also work with modalities \n",
      "beyond text for input and output, as seen in GPT-4’s ability to process image input alongside \n",
      "text.\n",
      "  In GPT models, this pre-training is done via self-super-\n",
      "vised learning.\n",
      "\n",
      "  Chapter 1\n",
      "15\n",
      "For some models, especially proprietary and closed-source models, this information is not known \n",
      "– in these cases, I’ve placed a cross.\n",
      "  Sam Altman, the CEO of OpenAI, has stated that the \n",
      "cost of training GPT-4 was more than $100 million.\n",
      "ChatGPT, a conversation model, was released by OpenAI in November 2022.\n",
      "  For example, for XLNet, the paper doesn’t give information \n",
      "about compute in flops; however, the training was done on 512 TPU v3 chips over 2.5 days.\n",
      "\n",
      "  Further, OpenAI \n",
      "was able to keep costs reasonable by utilizing a Mixture of Experts (MoE) model consisting of \n",
      "16 experts within their model, each having about 111 billion parameters.\n",
      "\n",
      "  The development of GPT models has seen considerable progress, with OpenAI’s GPT-n series \n",
      "leading the way in creating foundational AI models.\n",
      "  Additionally, they serve as a foundation for text-to-image technologies like diffusion and \n",
      "parallel decoding, enabling the development of Visual Foundation Models (VFMs) for systems \n",
      "that work with images.\n",
      "\n",
      "  However, different estimates suggest it has \n",
      "between 200 and 500 billion parameters.\n",
      "  Another substantial advancement came in March 2023 with GPT-4.\n",
      "  Trained on 300 billion tokens, GPT-3 has 175 billion parameters, an unprecedented size for DL \n",
      "models.\n",
      "  GPT-4 provides superior \n",
      "performance on various evaluation tasks coupled with significantly better response avoidance \n",
      "to malicious or provocative queries due to six months of iterative alignment during training.\n",
      "\n",
      "  OpenAI has been coy about the technical details; however, information has been circulating that, \n",
      "with about 1.8 trillion parameters, GPT-4 is more than 10x the size of GPT-3.\n",
      "  GPT-4 is the most recent in the series, though its size and training details have not been \n",
      "published due to competitive and safety concerns.\n",
      "  Based on prior GPT \n",
      "models (particularly GPT-3) and optimized for dialogue, it uses a combination of human-generat-\n",
      "ed roleplaying conversations and a dataset of human labeler demonstrations of the desired model \n",
      "behavior.\n",
      "Subtopics:\n",
      "  The model exhibits excellent capabilities such as wide-ranging knowledge retention \n",
      "and precise context tracking in multi-turn dialogues.\n",
      "\n",
      "  A foundation model (sometimes known as a base model) is a large model that was \n",
      "trained on an immense quantity of data at scale so that it can be adapted to a wide \n",
      "range of downstream tasks.\n",
      "----------------------------------------\n",
      "Page 39:\n",
      "Topics:\n",
      "  PaLM 2, released in May 2023, was trained with the focus of improving multilingual and reasoning \n",
      "capabilities while being more compute efficient.\n",
      "  Extensive benchmarking across different model sizes has shown that PaLM 2 has significantly \n",
      "improved quality on downstream tasks, including multilingual common sense and mathematical \n",
      "reasoning, coding, and natural language generation, compared to its predecessor PaLM.\n",
      "\n",
      "  There’s also a multi-modal version of GPT-4 that incorporates a separate vision encoder, trained \n",
      "on joined image and text data, giving the model the capability to read web pages and transcribe \n",
      "what’s in images and video.\n",
      "\n",
      "  Training was conducted for 2 \n",
      "epochs for text-based data and 4 for code-based data.\n",
      "  Although GPT-4 leads most benchmarks in perfor-\n",
      "mance, these and other models demonstrate a comparable performance in some tasks and have \n",
      "contributed to advancements in generative transformer-based language models.\n",
      "\n",
      "  PaLM 2 is smaller and exhibits faster and more efficient inference, \n",
      "allowing for broader deployment and faster response times for a more natural pace of interaction.\n",
      "\n",
      "  For fine-tuning, the dataset consisted of \n",
      "millions of rows of instruction fine-tuning data.\n",
      "  Other LLMs\n",
      "Other notable foundational GPT models besides OpenAI’s include Google DeepMind’s PaLM \n",
      "2, the model behind Google’s chatbot Bard.\n",
      "  Using evaluations at different compute scales, \n",
      "the authors (Anil and others; PaLM 2 Technical Report) estimated an optimal scaling of training \n",
      "data sizes and parameters.\n",
      "  Another rumor, again to be taken with a grain of \n",
      "salt, is that OpenAI might be applying speculative decoding on GPT-4’s inference, with the idea \n",
      "that a smaller model (oracle model) could be predicting the large model’s responses, and these \n",
      "predicted responses could help speed up decoding by feeding them into the larger model, thereby \n",
      "skipping tokens.\n",
      "  As can be seen in Figure 1.5, there are quite a few models besides OpenAI’s, some of which are \n",
      "suitable as a substitute for the OpenAI closed-source models, which we will have a look at.\n",
      "\n",
      "  What Is Generative AI?\n",
      "16\n",
      "Apparently, GPT-4 was trained on about 13 trillion tokens.\n",
      "Subtopics:\n",
      "  However, these are not unique tokens \n",
      "since they count repeated presentation of the data in each epoch.\n",
      "  This is a risky strategy because – depending on the threshold of the confidence \n",
      "of the oracle’s responses – the quality could deteriorate.\n",
      "\n",
      "----------------------------------------\n",
      "Page 40:\n",
      "Topics:\n",
      "  LLaMa 2, since its recent release, has already inspired several very competitive coding models, \n",
      "such as WizardCoder.\n",
      "\n",
      "  The \n",
      "pre-training corpus size has increased by 40% (2 trillion tokens of data), the context length of \n",
      "the model has doubled, and grouped-query attention has been adopted. \n",
      "\n",
      "  While LLaMa was released under a non-commercial license, the LLaMa 2  are open to the general \n",
      "public for research and commercial use.\n",
      "\n",
      "  The LLaMa 2 70B model performs on par or better than PaLM (540B) on \n",
      "almost all benchmarks, but there is still a large performance gap between LLaMa 2 70B and \n",
      "GPT-4 and PaLM-2-L.\n",
      "LLaMa 2 is an updated version of LLaMa 1 trained on a new mix of publicly available data.\n",
      "  LLaMa \n",
      "triggered the creation of models such as Vicuna, Koala, RedPajama, MPT, Alpaca, and Gorilla. \n",
      "\n",
      "  The exams used \n",
      "were for Chinese (HSK 7-9 Writing and HSK 7-9 Overall), Japanese (J-Test A-C Overall), Italian \n",
      "(PLIDA C2 Writing and PLIDA C2 Overall), French (TCF Overall), and Spanish (DELE C2 Writing \n",
      "and DELE C2 Overall).\n",
      "  Variants of LLaMa 2 with different parameter sizes (7B, 13B, 34B, and 70B) have been released. \n",
      "\n",
      "  Human raters judged the safety violations of model generations across approxi-\n",
      "mately 2,000 adversarial prompts, including both single and multi-turn prompts.\n",
      "\n",
      "  The releases of the LLaMa and LLaMa 2 series of models, with up to 70B parameters, by Meta AI \n",
      "in February and July 2023, respectively, have been highly influential by enabling the community \n",
      "to build on top of them, thereby kicking off a Cambrian explosion of open-source LLMs.\n",
      "  LLaMa 2-Chat has undergone safety evaluation results compared to other open-source and closed-\n",
      "source models.\n",
      "  Chapter 1\n",
      "17\n",
      "PaLM 2 was also tested on various professional language-proficiency exams.\n",
      "Subtopics:\n",
      "  Across these exams, which were designed to test C2-level proficiency, \n",
      "considered mastery or advanced professional level according to the CEFR (Common European \n",
      "Framework of Reference for Languages), PaLM 2 achieved mostly high-passing grades.\n",
      "\n",
      "  Optimized for dialogue use cases, at their release, the LLMs outperformed other open-source \n",
      "chat models on most benchmarks and seem on par with some closed-source models based on \n",
      "human evaluations.\n",
      "----------------------------------------\n",
      "Page 41:\n",
      "Topics:\n",
      "  Evaluations suggest Claude 2, released \n",
      "in July 2023, is one of the best GPT-4 competitors in the market.\n",
      "  Only a few companies, such as those shown in Figure 1.5, have been able to successfully train and \n",
      "deploy very large models.\n",
      "  Key model improvements include \n",
      "an expanded context size of up to 200K tokens, far larger than most available models, and being \n",
      "commercial or open source.\n",
      "  Meta’s LLaMa 2 model, with a size of up \n",
      "to 70 billion parameters, was trained on 1.4 trillion tokens, while PaLM 2, reportedly consisting \n",
      "of 340 billion parameters – smaller than their previous LLMs – appears to have a larger scale of \n",
      "training data in at least 100 languages.\n",
      "  Universities, such as KAUST, \n",
      "Carnegie Mellon University, Nanyang Technological University, and Tel Aviv University, have also \n",
      "contributed to the development of these models.\n",
      "  Major companies like Microsoft and Google have invested in start-ups \n",
      "and collaborations to support the development of these models.\n",
      "  What Is Generative AI?\n",
      "18\n",
      "Claude and Claude 2 are AI assistants created by Anthropic.\n",
      "  The model card Anthropic has created is fairly detailed, showing Claude 2 still has limitations in \n",
      "areas like confabulation, bias, factual errors, and potential for misuse, problems it has in com-\n",
      "mon with all LLMs.\n",
      "  It \n",
      "also performs well on standardized tests like GRE and MBE.\n",
      "  Some projects are developed through collab-\n",
      "orations between companies and universities, as seen in the cases of Stable Diffusion, Soundify, \n",
      "and DreamFusion.\n",
      "\n",
      "  Modern LLMs can cost anywhere from 10 million to over \n",
      "100 million US dollars in computing costs for training.\n",
      "\n",
      "Subtopics:\n",
      "  In the next section, we’ll look into who these organizations are.\n",
      "\n",
      "  Anthropic is working to address these through techniques like data filtering, \n",
      "debiasing, and safety interventions.\n",
      "\n",
      "  It also performs better on use cases like coding, summarization, and \n",
      "long document understanding.\n",
      "\n",
      "  Major players\n",
      "Training a large number of parameters on large-scale datasets requires significant compute power \n",
      "and a skilled data science and data engineering team.\n",
      "  The development of LLMs has been limited to a few players due to high computational require-\n",
      "ments.\n",
      "  It improves on previous versions \n",
      "in helpfulness, honesty, and lack of stereotype bias based on human feedback comparisons.\n",
      "----------------------------------------\n",
      "Page 42:\n",
      "Topics:\n",
      "  •\t\n",
      "Anthropic have released the Claude and Claude 2 models for public usage on \n",
      "their website.\n",
      "  •\t\n",
      "The French AI startup Mistral has unveiled its free-to-use, open-license 7B \n",
      "model, outperforming similar-sized models, generated from private datasets, \n",
      "and developed with the intent to support the open generative AI community, \n",
      "while also offering commercial products.\n",
      "•\t\n",
      "EleutherAI is a grassroots collection of researchers developing open-access \n",
      "models like GPT-Neo and GPT-J, fully open source and available to the public.\n",
      "\n",
      "  •\t\n",
      "Google (including Google’s DeepMind division) have developed a number \n",
      "of LLMs, starting from BERT and – more recently – Chinchilla, Gopher, PaLM, \n",
      "and PaLM2.\n",
      "  •\t\n",
      "Microsoft have developed models like Turing-NLG and Megatron-Turing \n",
      "NLG but have focused on integrating OpenAI models into products over re-\n",
      "leasing their own models.\n",
      "  The API is in private beta.\n",
      "  There are a few more notable institutions, such as the Technology Innovation In-\n",
      "stitute (TII), an Abu Dhabi government-funded research institution, which open-\n",
      "sourced Falcon LLM for research and commercial usage.\n",
      "\n",
      "  •\t\n",
      "Stability AI, the company behind Stable Diffusion, released the model \n",
      "weights under a non-commercial license.\n",
      "\n",
      "  •\t\n",
      "Meta have released models like RoBERTa, BART, and LLaMa 2, including \n",
      "parameters of the models (although often under a non-commercial license) \n",
      "and the source code for setting up and training the models.\n",
      "\n",
      "  Chapter 1\n",
      "19\n",
      "There are quite a few companies and organizations developing generative AI in gener-\n",
      "al, as well as LLMs, and they are releasing them on different terms – here’s just a few:\n",
      "•\t\n",
      "OpenAI have released GPT-2 as open source; however, subsequent mod-\n",
      "els have been closed source but open for public usage on their website or \n",
      "through an API.\n",
      "\n",
      "  •\t\n",
      "Aleph Alpha, Alibaba, and Baidu are providing API access or integrating \n",
      "their models into products rather than releasing parameters or training code.\n",
      "\n",
      "Subtopics:\n",
      "  The models themselves are closed \n",
      "source.\n",
      "\n",
      "  The training code and parameters for phi-1 have \n",
      "been released for research use.\n",
      "\n",
      "  They previously released the code and weights (parameters) of a \n",
      "few of their models under open-source licensing, even though recently they \n",
      "have moved toward more secrecy in their development.\n",
      "\n",
      "----------------------------------------\n",
      "Page 43:\n",
      "Topics:\n",
      "  It also has identical modules, with the same \n",
      "two sub-layers as the encoder. \n",
      "\n",
      "  How do GPT models work?\n",
      "Generative pre-training has been around for a while, employing methods such as Markov models \n",
      "or other techniques.\n",
      "  The encoder is made up of identical layers, each with two sub-layers.\n",
      "  What Is Generative AI?\n",
      "20\n",
      "The complexity of estimating parameters in generative AI models suggests that smaller companies \n",
      "or organizations without sufficient computation power and expertise may struggle to deploy \n",
      "these models successfully; although, recently, after the publication of the LLaMa models, we’ve \n",
      "seen smaller companies making significant breakthroughs, for example, in terms of coding ability.\n",
      "\n",
      "  How do GPT \n",
      "models work?\n",
      "\n",
      "  However, language models such as BERT and GPT were made possible by \n",
      "the transformer deep neural network architecture (Vaswani and others, Attention Is All You Need, \n",
      "2017), which has been a game-changer for NLP.\n",
      "  In the next section, we’ll review the progress that DL and generative models have been making \n",
      "over recent years that has led up to the current explosion of their apparent capabilities and the \n",
      "attention these models have been getting.\n",
      "\n",
      "  Designed to avoid recursion to allow parallel com-\n",
      "putation, the Transformer architecture, in different variations, continues to push the boundaries \n",
      "of what’s possible within the field of NLP and generative AI.\n",
      "\n",
      "  Neural Machine Translation (NMT) is a mainstream approach to machine translation \n",
      "that uses DL to capture long-range dependencies in a sentence.\n",
      "  Transformers have pushed the envelope in NLP, especially in translation and language under-\n",
      "standing.\n",
      "  The input embedding is \n",
      "passed through an attention mechanism, and the second sub-layer is a fully connected feed-for-\n",
      "ward network.\n",
      "Subtopics:\n",
      "  The hidden state representations consider not only the inherent meaning of \n",
      "the words (their semantic value) but also their context in the sequence.\n",
      "\n",
      "  The transformer model architecture has an encoder-decoder structure, where the encoder maps \n",
      "an input sequence to a sequence of hidden states, and the decoder maps the hidden states to an \n",
      "output sequence.\n",
      "  Let’s get into the nitty-gritty details – how do these LLMs work under the hood?\n",
      "  Each sub-layer is followed by a residual connection and layer normalization.\n",
      "  The \n",
      "output of each sub-layer is the sum of the input and the output of the sub-layer, which is then \n",
      "normalized.\n",
      "\n",
      "  Models based on transformers \n",
      "outperformed previous approaches, such as using recurrent neural networks, particularly Long \n",
      "Short-Term Memory (LSTM) networks.\n",
      "\n",
      "  The decoder uses this encoded information to generate the output sequence one item at a time, \n",
      "using the context of the previously generated items.\n",
      "----------------------------------------\n",
      "Page 44:\n",
      "Topics:\n",
      "  •\t\n",
      "Layer normalization: To stabilize the network’s learning, the transformer uses a tech-\n",
      "nique called layer normalization.\n",
      "  This masking, combined with the fact that the output embeddings are \n",
      "offset by one position, ensures that the predictions for position i can only depend on the known \n",
      "outputs at positions less than i. These are indicated in the diagram here (source: Yuening Jia, \n",
      "Wikimedia Commons):\n",
      "Figure 1.6: The Transformer architecture\n",
      "The architectural features that have contributed to the success of transformers are:\n",
      "•\t\n",
      "Positional encoding:\n",
      "  Chapter 1\n",
      "21\n",
      "In addition, the decoder has a third sub-layer that performs Multi-Head Attention (MHA) over \n",
      "the output of the encoder stack.\n",
      "Subtopics:\n",
      "  Since the transformer doesn’t process words sequentially but instead \n",
      "processes all words simultaneously, it lacks any notion of the order of words.\n",
      "  These encodings are added to the input embeddings repre-\n",
      "senting each word, thus allowing the model to consider the order of words in a sequence.\n",
      "\n",
      "  The decoder also uses residual connections and layer normaliza-\n",
      "tion.\n",
      "  To remedy \n",
      "this, information about the position of words in the sequence is injected into the model \n",
      "using positional encodings.\n",
      "  This technique normalizes the model’s inputs across \n",
      "the features dimension (instead of the batch dimension as in batch normalization), thus \n",
      "improving the overall speed and stability of learning.\n",
      "\n",
      "  The self-attention sub-layer in the decoder is modified to prevent positions from attending \n",
      "to subsequent positions.\n",
      "----------------------------------------\n",
      "Page 45:\n",
      "Topics:\n",
      "  What Is Generative AI?\n",
      "22\n",
      "•\t\n",
      "Multi-head attention: Instead of applying attention once, the transformer applies it \n",
      "multiple times in parallel – improving the model’s ability to focus on different types of \n",
      "information and thus capturing a richer combination of features.\n",
      "\n",
      "  Many LLMs use some form of Multi-Query Attention (MQA), including \n",
      "OpenAI’s GPT-series models, Falcon, SantaCoder, and StarCoder.\n",
      "\n",
      "  MQA \n",
      "improves the performance and efficiency of language models for various language tasks.\n",
      "  MQA is an extension of MHA, where attention computation is replicated multiple times.\n",
      "  However, as the context window or batch \n",
      "sizes increase, the memory costs associated with the KV cache size in MHA models also increase \n",
      "significantly.\n",
      "  LLaMa 2 and a few other models used Grouped-Query Attention (GQA), which is a practice \n",
      "used in autoregressive decoding to cache the key (K) and value (V) pairs for the previous tokens \n",
      "in the sequence, speeding up attention computation.\n",
      "  By re-\n",
      "moving the heads dimension from certain computations and optimizing memory usage, MQA \n",
      "allows for 11 times better throughput and 30% lower latency in inference tasks compared to \n",
      "baseline models without MQA.\n",
      "\n",
      "Subtopics:\n",
      "  Different mechanisms have been \n",
      "tried out to alleviate this.\n",
      "  The basic idea behind attention mechanisms is to compute a weighted sum of the values (usu-\n",
      "ally referred to as values or content vectors) associated with each position in the input sequence, \n",
      "based on the similarity between the current position and all other positions.\n",
      "  To address this, the key and value projections can be shared across multiple heads \n",
      "without much degradation of performance.\n",
      "\n",
      "  This weighted sum, \n",
      "known as the context vector, is then used as an input to the subsequent layers of the model, en-\n",
      "abling the model to selectively attend to relevant parts of the input during the decoding process.\n",
      "\n",
      "  There have been many other proposed approaches to obtain efficiency gains, such as sparse, low-\n",
      "rank self-attention, and latent bottlenecks, to name just a few.\n",
      "  Early attention mechanisms scaled quadratically with the length of the sequences (context size), \n",
      "rendering them inapplicable to settings with long sequences.\n",
      "  The individual context vectors \n",
      "from each head are then concatenated or combined in some way to form the final output.\n",
      "\n",
      "  Other work has tried to extend \n",
      "sequences beyond the fixed input size; architectures such as transformer-XL reintroduce recur-\n",
      "sion by storing hidden states of already encoded sentences to leverage them in the subsequent \n",
      "encoding of the next sentences.\n",
      "\n",
      "  To enhance the expressiveness of the attention mechanism, it is often extended to include mul-\n",
      "tiple so-called heads, where each head has its own set of query, key, and value vectors, allowing \n",
      "the model to capture various aspects of the input representation.\n",
      "  A key reason for the success of transformers has been their ability to maintain performance across \n",
      "longer sequences better than other models, for example, recurrent neural networks.\n",
      "\n",
      "----------------------------------------\n",
      "Page 46:\n",
      "Topics:\n",
      "  A lower NLL indicates that the network has successfully \n",
      "learned patterns from the training set, so it will accurately predict the labels of the training sam-\n",
      "ples.\n",
      "  Pre-training\n",
      "The transformer is trained in two phases using a combination of unsupervised pre-training and \n",
      "discriminative task-specific fine-tuning.\n",
      "  Negative Log-Likelihood (NLL) and Perplexity (PPL) are important metrics used in training and \n",
      "evaluating language models.\n",
      "  PPL, on the other hand, is an exponentiation of NLL, providing a more intuitive way to understand \n",
      "the model’s performance.\n",
      "  In Masked Language Modeling \n",
      "(MLM), introduced in BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\n",
      "standing by Devlin and others (2019), the input is masked out, and the model attempts to predict \n",
      "the missing tokens based on the context provided by the non-masked portion.\n",
      "  Chapter 1\n",
      "23\n",
      "The combination of these architectural features allows GPT models to successfully tackle tasks \n",
      "that involve understanding and generating text in human language and other domains.\n",
      "  NLL is a loss function used in ML algorithms, aimed at maximizing \n",
      "the probability of correct predictions.\n",
      "  It’s important to mention that NLL is a value constrained within a positive interval.\n",
      "\n",
      "Subtopics:\n",
      "  Let’s see how these LLMs are \n",
      "trained!\n",
      "\n",
      "  Smaller PPL values indicate a well-trained network that can predict \n",
      "accurately while higher values indicate poor learning performance.\n",
      "  The \n",
      "overwhelming majority of LLMs are transformers, as are many other state-of-the-art models \n",
      "we will encounter in the different sections of this chapter, including models for image, sound, \n",
      "and 3D objects.\n",
      "\n",
      "  The unsupervised pre-training can follow different objectives.\n",
      "  The goal during pre-training is to learn a general-purpose \n",
      "representation that transfers to a wide range of tasks.\n",
      "\n",
      "  Therefore, the goal in \n",
      "pre-training is to minimize perplexity, which means the model’s predictions align more with \n",
      "the actual outcomes.\n",
      "\n",
      "  Parameters in the models are then iteratively updated ac-\n",
      "cording to these comparisons.\n",
      "\n",
      "  For example, if \n",
      "the input sentence is “The cat [MASK] over the wall,” the model would ideally learn to predict \n",
      "“jumped” for the mask.\n",
      "\n",
      "  As the name suggests, a particularity of GPTs lies in pre-training.\n",
      "  In this case, the training objective minimizes the differences between predictions and the masked \n",
      "tokens according to a loss function.\n",
      "  Intuitively, we could say that \n",
      "a low perplexity means that the model is less surprised by the next word.\n",
      "----------------------------------------\n",
      "Page 47:\n",
      "Topics:\n",
      "  Tokens serve as the base elements for constructing \n",
      "sequences of text.\n",
      "\n",
      "  What Is Generative AI?\n",
      "24\n",
      "In comparing different language models, perplexity is often used as a benchmark metric across \n",
      "various tasks.\n",
      "  The first step in training an LLM is tokenization.\n",
      "  Before training the LLM, the tokenizer – more precisely, its dictionary – is typically fitted to the \n",
      "entire training dataset and then frozen.\n",
      "Subtopics:\n",
      "  It’s important to note that tokenizers do not produce \n",
      "arbitrary integers.\n",
      "  Instead, they output integers within a specific range – from 0 \n",
      " to \n",
      ", where  \n",
      "represents the vocabulary size of the tokenizer.\n",
      "\n",
      "  It gives an idea about how well the language model is performing, where a lower \n",
      "perplexity indicates the model is more certain of its predictions.\n",
      "  This process involves building a vocabulary, \n",
      "which maps tokens to unique numerical representations so that they can be processed by the \n",
      "model, given that LLMs are mathematical functions that require numerical inputs and outputs.\n",
      "\n",
      "  Tokenization: This refers to the process of splitting text into tokens.\n",
      "  A tokenizer \n",
      "splits on whitespace and punctuation to break text into individual tokens.\n",
      "\n",
      "  Examples:\n",
      "Consider the following text:\n",
      "“The quick brown fox jumps over the lazy dog!”\n",
      "\n",
      "  This would get split into the following tokens:\n",
      "\n",
      "  ]\n",
      "Each word is an individual token, as is the punctuation mark.\n",
      "\n",
      "  Definitions:\n",
      "Token: A token is an instance of a sequence of characters, typically forming a word, \n",
      "punctuation mark, or number.\n",
      "  [“The”, “quick”, “brown”, “fox”, “jumps”, “over”, “the”, “lazy”, “dog”, “!”\n",
      "  Hence, a model with lower per-\n",
      "plexity would be considered better performing in comparison to others with higher perplexity.\n",
      "\n",
      "  Tokenization\n",
      "Tokenizing a text means splitting it into tokens (words or subwords), which then are converted \n",
      "to IDs through a look-up table mapping words in text to corresponding lists of integers.\n",
      "\n",
      "----------------------------------------\n",
      "Page 48:\n",
      "Topics:\n",
      "  This implies that to improve performance by a certain factor, one of \n",
      "these elements must be scaled up by the power of that factor; however, for optimal performance, \n",
      "all three factors must be scaled in tandem to avoid bottlenecks.\n",
      "\n",
      "  In a paper from 2020 by researchers from OpenAI, \n",
      "Kaplan and others (Scaling laws for neural language models, 2020) discussed scaling laws and the \n",
      "choice of parameters.\n",
      "\n",
      "  The total vocabulary size is 32K tokens.\n",
      "\n",
      "  Chapter 1\n",
      "25\n",
      "There are a lot of tokenizers that work according to different principles, but common types of \n",
      "tokenizers employed in models are Byte-Pair Encoding (BPE), WordPiece, and SentencePiece. \n",
      "\n",
      "  This context window refers to the length of the longest \n",
      "sequence of tokens that an LLM can use.\n",
      "  Typical context window sizes for LLMs can range from \n",
      "about 1,000 to 10,000 tokens.\n",
      "\n",
      "  Researchers at DeepMind (An empirical analysis of compute-optimal large language model training; \n",
      "Hoffmann and others, 2022) analyzed the training compute and dataset size of LLMs and con-\n",
      "cluded that LLMs are undertrained in terms of compute budget and dataset size as suggested \n",
      "by scaling laws. \n",
      "\n",
      "  While recurrent networks plateau after less than 100 \n",
      "tokens, transformers improve throughout the whole context.\n",
      "  Scaling\n",
      "As we’ve seen in Figure 1.5, language models have been becoming bigger over time.\n",
      "  For example, LLaMa 2’s BPE tokenizer splits numbers into individual digits and uses bytes to \n",
      "decompose unknown UTF-8 characters.\n",
      "  Next, it is worth talking at least briefly about the scale of these architectures, and why these \n",
      "models are as large as they are.\n",
      "\n",
      "Subtopics:\n",
      "  Further, they found a power-law relationship between performance and each of the following \n",
      "factors: dataset size, model size (number of parameters), and the amount of computational re-\n",
      "sources required for training.\n",
      "  Interestingly, they compare lots of different architecture choices and, among other things, show \n",
      "that transformers outperform LSTMs as language models in terms of perplexity in no small part \n",
      "due to the improved use of long contexts.\n",
      "  That corre-\n",
      "sponds to a long-term trend in machine learning that models get bigger as computing resources \n",
      "get cheaper, enabling higher performance.\n",
      "  It is necessary to point out that LLMs can only generate outputs based on a sequence of tokens \n",
      "that does not exceed its context window.\n",
      "  Therefore, transformers not only \n",
      "come with better training and inference speed but also give better performance when looking \n",
      "at relevant contexts.\n",
      "\n",
      "----------------------------------------\n",
      "Page 49:\n",
      "Topics:\n",
      "  Prompt engineering and conditioning methods will be explored further in \n",
      "Chapter 8, Customizing LLMs and Their Output.\n",
      "\n",
      "  However, more recently, a team at Microsoft Research has challenged these conclusions and \n",
      "surprised everyone (Textbooks Are All You Need; Gunaseka and colleagues, June 2023), finding that \n",
      "a small network (350M parameters) trained on high-quality datasets can give very competitive \n",
      "performance.\n",
      "  Zero-shot \n",
      "prompting involves no examples, while few-shot prompting includes a small number of \n",
      "examples of relevant problem and solution pairs.\n",
      "\n",
      "  We’ll discuss this model again in Chapter 6, Developing Software with Generative AI, \n",
      "and we’ll discuss the implications of scaling in Chapter 10, The Future of Generative Models.\n",
      "\n",
      "  What Is Generative AI?\n",
      "26\n",
      "They predicted that large models would perform better if substantially smaller and trained for \n",
      "much longer, and – in fact – validated their prediction by comparing a 70-billion-parameter \n",
      "Chinchilla model on a benchmark to their Gopher model, which consists of 280 billion parameters.\n",
      "\n",
      "  It includes fine-tuning and \n",
      "prompting:\n",
      "•\t\n",
      "Fine-tuning involves modifying a pre-trained language model by training it on a specific \n",
      "task using supervised learning.\n",
      "  •\t\n",
      "Prompting techniques present problems in text form to generative models.\n",
      "  For fine-tuning, pre-trained models are usually trained \n",
      "again using Reinforcement Learning from Human Feedback (RLHF) to be helpful and \n",
      "harmless.\n",
      "\n",
      "Subtopics:\n",
      "  There are a \n",
      "lot of different prompting techniques, starting from simple questions to detailed instruc-\n",
      "tions.\n",
      "  It could be that there’s a saturation of performance \n",
      "at a certain size, which only changes in the approach can overcome.\n",
      "  However, we could see new \n",
      "scaling laws linking performance with data quality.\n",
      "\n",
      "  After pre-training, a major step is how models are prepared for specific tasks either by fine-tuning \n",
      "or prompting.\n",
      "  For example, to make a model more amenable to chats \n",
      "with humans, the model is trained on examples of tasks formulated as natural language \n",
      "instructions (instruction tuning).\n",
      "  Let’s see what this task conditioning is about!\n",
      "Conditioning\n",
      "Conditioning LLMs refers to adapting the model for specific tasks.\n",
      "  These conditioning methods continue to evolve, becoming more effective and useful for a wide \n",
      "range of applications.\n",
      "  It will be instructive to observe whether model sizes for LLMs keep increasing at the same rate \n",
      "as they have.\n",
      "  This is an important question since it determines if the development of LLMs will \n",
      "be firmly in the hands of large organizations.\n",
      "  Prompts can include examples of similar problems and their solutions.\n",
      "----------------------------------------\n",
      "Page 50:\n",
      "Topics:\n",
      "  Chapter 1\n",
      "27\n",
      "How to try out these models\n",
      "You can access OpenAI’s model through their website or their API.\n",
      "  What are text-to-image models?\n",
      "Text-to-image models are a powerful type of generative AI that creates realistic images from \n",
      "textual descriptions.\n",
      "  •\t\n",
      "Image-to-image translation: Converting input images to a different style or domain \n",
      "specified through text, like “make this photo look like a Monet painting.”\n",
      "•\t\n",
      "Image recognition: Large foundation models can be used to recognize images, including \n",
      "classifying scenes, but also object detection, for example, detecting faces.\n",
      "\n",
      "  We’ll fine-tune a model in Chapter 8, Customizing LLMs and \n",
      "Their Output.\n",
      "\n",
      "  Generative AI is extensively used in generating 3D images, avatars, videos, graphs, and illustra-\n",
      "tions for virtual or augmented reality, video games graphic design, logo creation, image editing, \n",
      "or enhancement.\n",
      "  You can access these models through Hugging Face or other providers, as we’ll see starting in \n",
      "Chapter 3, Getting Started with LangChain.\n",
      "Subtopics:\n",
      "  They have diverse use cases in creative industries and design for generating \n",
      "advertisements, product prototypes, fashion images, and visual effects.\n",
      "  There is a whole zoo of stuff \n",
      "out there!\n",
      "\n",
      "  I’ll highlight the progress made in the field so far, but also discuss existing challenges \n",
      "and potential future directions.\n",
      "\n",
      "  As mentioned, in this book, we’ll focus on LLMs, since \n",
      "they have the broadest practical application, but we’ll also have a look at image models, which \n",
      "sometimes can be quite useful.\n",
      "\n",
      "  The most popular model category here is for text-conditioned image synthesis, \n",
      "specifically text-to-image generation.\n",
      "  The main applications are:\n",
      "•\t\n",
      "Text-conditioned image generation: Creating original images from text prompts like “a \n",
      "painting of a cat in a field of flowers.”\n",
      "  You can even download these open-source models, \n",
      "fine-tune them, or fully train them.\n",
      "  In the next section, we’ll be reviewing state-of-the-art methods for text-conditioned image gen-\n",
      "eration.\n",
      "  This can restore damaged images (denoising, dehazing, and deblurring) \n",
      "or edit out unwanted elements.\n",
      "\n",
      "  If you want to try other LLMs \n",
      "on your laptop, open-source LLMs are a good place to get started.\n",
      "  This is used for art, design, prototyping, and visual \n",
      "effects.\n",
      "•\t\n",
      "Image inpainting: Filling in missing or corrupted parts of an image based on the sur-\n",
      "rounding context.\n",
      "----------------------------------------\n",
      "Page 51:\n",
      "Topics:\n",
      "  They consist of two networks that are pitted \n",
      "against each other in a game-like setting – the generator, which generates new images from text \n",
      "embeddings and noise, and the discriminator, which estimates the probability of the new data \n",
      "being real.\n",
      "  What Is Generative AI?\n",
      "28\n",
      "Models like Midjourney, DALL-E 2, and Stable Diffusion provide creative and realistic images \n",
      "derived from textual input or other images.\n",
      "  Popular models like Stable Diffusion and DALL-E 2 use a text encoder to map input text into an \n",
      "embedding space.\n",
      "  Two main classes of models are used: Generative Adversarial Networks (GANs) and diffusion \n",
      "models.\n",
      "  GAN models like StyleGAN or GANPaint Studio can produce highly realistic images, but \n",
      "training is unstable and computationally expensive.\n",
      "  The setup for training GANs is illustrated in this diagram (taken from A Survey on Text Genera-\n",
      "tion Using Generative Adversarial Networks, G de Rosa and J P. Papa, 2022; https://arxiv.org/\n",
      "pdf/2212.11119.pdf):\n",
      "Figure 1.7: GAN training\n",
      "Diffusion models have become popular and promising for a wide range of generative tasks, in-\n",
      "cluding text-to-image synthesis.\n",
      "  As these two networks compete, GANs get better at their task, generating realistic \n",
      "images and other types of data.\n",
      "\n",
      "Subtopics:\n",
      "  The final model output is a high-resolution \n",
      "image aligned with the textual description.\n",
      "\n",
      "  These models offer advantages over previous approaches, such \n",
      "as GANs, by reducing computation costs and sequential error accumulation.\n",
      "  These models work by training deep neural networks \n",
      "on large datasets of image-text pairs.\n",
      "  They follow a forward diffusion process by \n",
      "adding noise to an image until it becomes uncharacteristic and noisy.\n",
      "  Diffusion models \n",
      "operate through a process like diffusion in physics.\n",
      "  This process is analogous \n",
      "to an ink drop falling into a glass of water and gradually diffusing.\n",
      "\n",
      "  This text embedding is fed into a series of conditional diffusion models, which \n",
      "denoise and refine a latent image in successive stages.\n",
      "  The key technique used is diffusion models, which start with \n",
      "random noise and gradually refine it into an image through repeated denoising steps.\n",
      "\n",
      "----------------------------------------\n",
      "Page 52:\n",
      "Topics:\n",
      "  An example of this is the Imagen text-to-image model (Photorealistic Text-to-Image Diffusion \n",
      "Models with Deep Language Understanding by Google Research, May 2022), which incorporates \n",
      "frozen text embeddings from LLMs, pre-trained on text-only corpora.\n",
      "  You can see \n",
      "the image generation step by step, including the U-Net denoising process using the Denoising \n",
      "Diffusion Implicit Model (DDIM) sampling method, which repeatedly removes Gaussian noise, \n",
      "and then decodes the denoised output into pixel space.\n",
      "\n",
      "  The denoising process is demonstrated in this plot (source: user Benlisquare via Wikimedia Com-\n",
      "mons):\n",
      "Figure 1.8: European-style castle in Japan, created using the Stable Diffusion V1-5 AI diffusion \n",
      "model\n",
      "In Figure 1.8, only some steps within the 40-step generation process are shown.\n",
      "  Chapter 1\n",
      "29\n",
      "The unique aspect of generative image models is the reverse diffusion process, where the model \n",
      "attempts to recover the original image from a noisy, meaningless image.\n",
      "  A text encoder first maps \n",
      "the input text to a sequence of embeddings.\n",
      "Subtopics:\n",
      "  A cascade of conditional diffusion models takes the \n",
      "text embeddings as input and generates images.\n",
      "\n",
      "  By iteratively applying \n",
      "noise removal transformations, the model generates images of increasing resolutions that align \n",
      "with the given text input.\n",
      "  The final output is an image that has been modified based on the text \n",
      "input.\n",
      "  Although they \n",
      "sometimes produce striking results, the instability and inconsistency are a significant challenge \n",
      "to applying these models more broadly.\n",
      "\n",
      "  With diffusion models, you can see a wide variety of outcomes using only minimal changes to \n",
      "the initial setting of the model or – as in this case – numeric solvers and samplers.\n",
      "----------------------------------------\n",
      "Page 53:\n",
      "Topics:\n",
      "  4.\t\n",
      "Steps 2 and 3 are repeated for a set number of sampling steps, for instance, 40 times, as \n",
      "shown in the plot.\n",
      "\n",
      "  5.\t\n",
      "Finally, the decoder component of the VAE transforms the latent image back into pixel \n",
      "space, providing the final output image.\n",
      "\n",
      "  A VAE is a model that encodes data into a learned, smaller representation (encoding).\n",
      "  Significantly, Stable Diffusion introduced operations in latent (lower-dimensional) space repre-\n",
      "sentations, which capture the essential properties of an image, in order to improve computational \n",
      "efficiency.\n",
      "  By creating high-fidelity images from text on consumer GPUs, the Stable \n",
      "Diffusion model democratizes access.\n",
      "  What Is Generative AI?\n",
      "30\n",
      "Stable Diffusion was developed by the CompVis group at LMU Munich (High-Resolution Image \n",
      "Synthesis with Latent Diffusion Models by Blattmann and others, 2022).\n",
      "  This VAE is trained first.\n",
      "\n",
      "  2.\t A noise predictor (U-Net) takes in both the latent noisy image and the provided text prompt \n",
      "and predicts the noise.\n",
      "\n",
      "  3.\t\n",
      "\n",
      "  The Stable Diffusion model \n",
      "significantly cuts training costs and sampling time compared to previous (pixel-based) diffusion \n",
      "models.\n",
      "  The model can be run on consumer hardware equipped with a modest GPU (for example, \n",
      "the GeForce 40 series).\n",
      "  A VAE provides latent space compression (called perceptual compression in the paper), \n",
      "while a U-Net performs iterative denoising.\n",
      "\n",
      "  Stable Diffusion generates images from text prompts through several clear steps:\n",
      "1.\t\n",
      "It starts by producing a random tensor (random image) in the latent space, which serves \n",
      "as the noise for our initial image.\n",
      "\n",
      "Subtopics:\n",
      "  The model then subtracts the latent noise from the latent image.\n",
      "\n",
      "  Further, the model’s source code and even the weights have \n",
      "been released under the CreativeML OpenRAIL-M license, which doesn’t impose restrictions on \n",
      "reuse, distribution, commercialization, and adaptation.\n",
      "\n",
      "  These rep-\n",
      "resentations can then be used to generate new data similar to that used for training (decoding). \n",
      "\n",
      "----------------------------------------\n",
      "Page 54:\n",
      "Topics:\n",
      "  The U-Net takes a noisy image (seed) as input and processes it through a \n",
      "series of convolutional layers to extract features and learn semantic representations.\n",
      "\n",
      "  Chapter 1\n",
      "31\n",
      "For training the image generation model in the latent space itself (latent diffusion model), a \n",
      "loss function is used to evaluate the quality of the generated images.\n",
      "  A U-Net is a popular type of convolutional neural network (CNN) that has a sym-\n",
      "metric encoder-decoder structure.\n",
      "  One commonly used loss \n",
      "function is the Mean Squared Error (MSE) loss, which quantifies the difference between the \n",
      "generated image and the target image.\n",
      "  This training was performed on the LAION-5B dataset, consisting of billions of image-text pairs, \n",
      "derived from Common Crawl data, comprising billions of image-text pairs from sources such as \n",
      "Pinterest, WordPress, Blogspot, Flickr, and DeviantArt.\n",
      "\n",
      "  Once the contracting \n",
      "path reaches the bottleneck of the U-Net, it then expands through a symmetric \n",
      "expanding path.\n",
      "  These convolutional layers, typically organized in a contracting path, reduce the \n",
      "spatial dimensions while increasing the number of channels.\n",
      "  It is commonly used for image segmentation tasks, \n",
      "but in the context of Stable Diffusion, it can help to introduce and remove noise in \n",
      "the image.\n",
      "Subtopics:\n",
      "  In the expanding path, transposed convolutions (also known as \n",
      "upsampling or deconvolutions) are applied to progressively upsample the spatial \n",
      "dimensions while reducing the number of channels.\n",
      "\n",
      "  The model is optimized to minimize this loss, encouraging \n",
      "it to generate images that closely resemble the desired output.\n",
      "\n",
      "----------------------------------------\n",
      "Page 55:\n",
      "Topics:\n",
      "  Ramesh and others, Hierarchical Text-Conditional Image Generation with CLIP Latents, \n",
      "2022; https://arxiv.org/abs/2204.06125):\n",
      "Figure 1.9: Image generation from text prompts\n",
      "Overall, image generation models such as Stable Diffusion and Midjourney process textual \n",
      "prompts into generated images, leveraging the concept of forward and reverse diffusion processes \n",
      "and operating in a lower-dimensional latent space for efficiency.\n",
      "  What Is Generative AI?\n",
      "32\n",
      "The following images illustrate text-to-image generation from a text prompt with diffusion \n",
      "(source:\n",
      "  It’s out of the scope of this book to provide a comprehensive survey of generative AI models for \n",
      "all modalities.\n",
      "  These \n",
      "embeddings are then processed by a text transformer and fed to the noise predictor, steering it \n",
      "to produce an image that aligns with the text prompt.\n",
      "\n",
      "Subtopics:\n",
      "  However, let’s get a bit of an overview of what models can do for other domains.\n",
      "\n",
      "  The conditioning process allows these models to be influenced by specific input textual prompts \n",
      "or input types like depth maps or outlines for greater precision to create relevant images.\n",
      "  But what about the conditioning \n",
      "for the model in the text-to-image use case?\n",
      "\n",
      "----------------------------------------\n",
      "Page 56:\n",
      "Topics:\n",
      "  [Automatic Speech Recognition (ASR)].\n",
      "  Chapter 1\n",
      "33\n",
      "What can AI do in other domains?\n",
      "\n",
      "  Generative AI models have demonstrated impressive capabilities across modalities including \n",
      "sound, music, video, and 3D shapes.\n",
      "  The following table summarizes some recent models in these domains:\n",
      "Model\n",
      "Organization\n",
      "Year\n",
      "Domain\n",
      "Architecture\n",
      "Performance\n",
      "3D-GQN\n",
      "DeepMind\n",
      "2018\n",
      "3D\n",
      "Deep, iterative, latent \n",
      "variable density \n",
      "models\n",
      "3D scene \n",
      "generation from \n",
      "2D images\n",
      "Jukebox\n",
      "OpenAI\n",
      "2020\n",
      "Music\n",
      "VQ-VAE + transformer\n",
      "High-fidelity \n",
      "music generation \n",
      "in different styles\n",
      "Whisper\n",
      "OpenAI\n",
      "2022\n",
      "Sound/\n",
      "speech\n",
      "Transformer\n",
      "Near human-\n",
      "level speech \n",
      "recognition\n",
      "Imagen Video\n",
      "Google\n",
      "2022\n",
      "Video\n",
      "Frozen text \n",
      "transformers + video \n",
      "diffusion models\n",
      "High-definition \n",
      "video generation \n",
      "from text\n",
      "Phenaki\n",
      "Google & \n",
      "UCL\n",
      "2022\n",
      "Video\n",
      "Bidirectional masked \n",
      "transformer\n",
      "Realistic video \n",
      "generation from \n",
      "text\n",
      "TecoGAN\n",
      "U. Munich\n",
      "2022\n",
      "Video\n",
      "Temporal coherence \n",
      "module\n",
      "High-quality, \n",
      "smooth video \n",
      "generation\n",
      "DreamFusion\n",
      "Google\n",
      "2022\n",
      "3D\n",
      "NeRF + Diffusion\n",
      "High-fidelity 3D \n",
      "object generation \n",
      "from text\n",
      "\n",
      "  For video, AI systems can create photorealistic footage \n",
      "from text prompts and perform sophisticated editing like object removal.\n",
      "Subtopics:\n",
      "  Speech-to-text systems can convert spoken language into text \n",
      "\n",
      "  3D models learned to \n",
      "reconstruct scenes from images and generate intricate objects from textual descriptions.\n",
      "\n",
      "  In the audio domain, models can synthesize natural speech, \n",
      "generate original music compositions, and even mimic a speaker’s voice and the patterns of \n",
      "rhythm and sound (prosody).\n",
      "----------------------------------------\n",
      "Page 57:\n",
      "Topics:\n",
      "  The next chapter will explore the tooling of generative models, particularly LLMs, with the LangC-\n",
      "hain framework, focusing on the fundamentals, the implementation, and the use of this particular \n",
      "tool in exploiting and extending the capability of LLMs.\n",
      "\n",
      "  Leading AI labs at Google, OpenAI, Meta, and DeepMind are \n",
      "pushing the boundaries of what’s possible.\n",
      "\n",
      "  What Is Generative AI?\n",
      "34\n",
      "AudioLM\n",
      "Google\n",
      "2023\n",
      "Sound/\n",
      "speech\n",
      "Tokenizer + \n",
      "transformer LM + \n",
      "detokenizer\n",
      "High linguistic \n",
      "quality speech \n",
      "generation \n",
      "maintaining \n",
      "speaker’s identity\n",
      "AudioGen\n",
      "Meta AI\n",
      "2023\n",
      "Sound/\n",
      "speech\n",
      "Transformer + text \n",
      "guidance\n",
      "High-quality \n",
      "conditional and \n",
      "unconditional \n",
      "audio generation\n",
      "Universal \n",
      "Speech \n",
      "Model (USM)\n",
      "\n",
      "  Summary\n",
      "With the rise of computing power, deep neural networks, transformers, generative adversarial \n",
      "networks, and VAEs model the complexity of real-world data much more effectively than previ-\n",
      "ous generations of models, pushing the boundaries of what’s possible with AI algorithms.\n",
      "  In this \n",
      "chapter, we explored the recent history of DL and AI and generative models such as LLMs and GPTs, \n",
      "together with the theoretical ideas underpinning them, especially the Transformer architecture. \n",
      "\n",
      "  Google\n",
      "2023\n",
      "Sound/\n",
      "speech\n",
      "Encoder-decoder \n",
      "transformer\n",
      "State-of-the-art \n",
      "multilingual \n",
      "speech \n",
      "recognition\n",
      "Table 1.1: Models for audio, video, and 3D domains\n",
      "\n",
      "Subtopics:\n",
      "  Underlying many of these innovations are advances in deep generative architectures like GANs, \n",
      "diffusion models, and transformers.\n",
      "  We also explained the basic concepts of models for image generation, such as the Stable Diffu-\n",
      "sion model, and finally discussed applications beyond text and images, such as sound and video.\n",
      "\n",
      "----------------------------------------\n",
      "Page 58:\n",
      "Topics:\n",
      "  What does GPT stand for?\n",
      "9.\t\n",
      "\n",
      "  Which applications exist for generative models?\n",
      "3.\t\n",
      "\n",
      "  What is a transformer and what does it consist of?\n",
      "8.\t \n",
      "  How does Stable Diffusion work?\n",
      "10.\t \n",
      "  What is a VAE?\n",
      "\n",
      "  How can we get better performance from LLMs?\n",
      "5.\t\n",
      "\n",
      "  Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "  What is an LLM and what does it do?\n",
      "4.\t\n",
      "\n",
      "  Chapter 1\n",
      "35\n",
      "Questions\n",
      "I think it’s a good habit to check that you’ve digested the material when reading a technical book. \n",
      "\n",
      "  What are the conditions that make these models possible?\n",
      "6.\t\n",
      "Which companies and organizations are the big players in developing LLMs?\n",
      "7.\t\n",
      "\n",
      "  Let’s see if \n",
      "you can answer them:\n",
      "1.\t\n",
      "What is a generative model?\n",
      "2.\t \n",
      "Subtopics:\n",
      "  For this purpose, I’ve created a few questions relating to the content of this chapter.\n",
      "  If you struggle to answer these questions, please refer to the corresponding sections in this chapter \n",
      "to ensure that you’ve understood the material.\n",
      "\n",
      "----------------------------------------\n",
      "Page 59:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 60:\n",
      "Topics:\n",
      "  The main sections of this chapter are:\n",
      "•\t\n",
      "Going beyond stochastic parrots\n",
      "•\t\n",
      "What is LangChain?\n",
      "•\t\n",
      "Exploring key components of LangChain\n",
      "•\t\n",
      "How does LangChain work?\n",
      "•\t\n",
      "Comparing LangChain with other frameworks\n",
      "\n",
      "  We will walk through examples of how developers can use LangChain’s capabilities to cre-\n",
      "ate customized natural language processing solutions, outlining the components and concepts \n",
      "involved.\n",
      "\n",
      "  The goal is to illustrate how LangChain enables building dynamic, data-aware applications that \n",
      "go beyond what is possible by simply accessing LLMs via API calls.\n",
      "  LangChain provides \n",
      "solutions to these issues through different integrations and off-the-shelf components for specific \n",
      "tasks.\n",
      "  In this chapter, \n",
      "we will introduce LangChain as a way to overcome LLM limitations and build innovative lan-\n",
      "guage-based applications.\n",
      "  Lastly, we will talk about \n",
      "important concepts related to LangChain, such as chains, action plan generation, and memory, \n",
      "which are important concepts to understand how LangChain works.\n",
      "\n",
      "  2\n",
      "LangChain for LLM Apps\n",
      "Large Language Models (LLMs) like GPT-4 have demonstrated immense capabilities in generating \n",
      "human-like text.\n",
      "  We aim to demonstrate the potential of combining recent AI advance-\n",
      "ments with a robust framework like LangChain.\n",
      "\n",
      "Subtopics:\n",
      "  We will start by outlining some challenges faced when using LLMs on their own, like the lack of \n",
      "external knowledge, incorrect reasoning, and the inability to take action.\n",
      "  However, simply accessing LLMs via APIs has limitations.\n",
      "  Instead, combining \n",
      "them with other data sources and tools can enable more powerful applications.\n",
      "----------------------------------------\n",
      "Page 61:\n",
      "Topics:\n",
      "  LLMs struggle with challenges like the compositionality gap (Measuring and Narrowing the Com-\n",
      "positionality Gap in Language Models by Ofir Press and colleagues; 2023).\n",
      "  LangChain for LLM Apps\n",
      "38\n",
      "Going beyond stochastic parrots\n",
      "LLMs have gained significant attention and popularity due to their ability to generate human-like \n",
      "text and understand natural language, which makes them useful in scenarios that revolve around \n",
      "content generation, text classification, and summarization.\n",
      "  Coined by researchers Emily Bender, Timnit Gebru, \n",
      "Margaret Mitchell, and Angelina McMillan-Major in their influential paper On the Dangers of \n",
      "Stochastic Parrots (2021), the term critiques models that mindlessly mimic linguistic patterns. \n",
      "\n",
      "  We’ll continue here by looking at the limitations of LLMs, ways to overcome \n",
      "those limitations, and how LangChain facilitates applications that systematically mitigate the \n",
      "shortcomings and extend the functionality of LLMs.\n",
      "\n",
      "Subtopics:\n",
      "  Stochastic parrots refers to LLMs that can produce convincing language but lack any true com-\n",
      "prehension of the meaning behind words.\n",
      "  Understanding these limitations is crucial when devel-\n",
      "oping applications.\n",
      "  •\t\n",
      "Inability to take action: LLMs cannot perform interactive actions like searches, calcula-\n",
      "tions, or lookups.\n",
      "  Overcoming these obstacles requires \n",
      "augmenting LLMs with techniques that add true comprehension.\n",
      "  The concept of stochastic parrots \n",
      "helps to elucidate this fundamental issue.\n",
      "\n",
      "  Some pain points associated with LLMs include:\n",
      "•\t\n",
      "Outdated knowledge: LLMs rely solely on their training data.\n",
      "  This means LLMs cannot \n",
      "connect inferences or adapt responses to new situations.\n",
      "  Let’s look at this argument in a bit more detail.\n",
      "  As has been established, LLMs offer impressive capabilities but suffer from limitations that hinder \n",
      "their effectiveness in certain scenarios.\n",
      "  Without external integra-\n",
      "tion, they cannot provide recent real-world information.\n",
      "\n",
      "  This severely limits functionality.\n",
      "\n",
      "  Innovations like prompting, chain-of-\n",
      "thought reasoning, retrieval grounding, and others are needed to educate models.\n",
      "\n",
      "  If you wish to skip these details, please move on \n",
      "to the next section.\n",
      "  Raw model scale alone cannot \n",
      "transform stochastic parroting into beneficial systems.\n",
      "  Without being grounded in the real world, models can produce responses that are inaccurate, \n",
      "irrelevant, unethical, or make little logical sense.\n",
      "\n",
      "  What are the limitations of LLMs?\n",
      "\n",
      "  Simply scaling up compute and data does not impart reasoning capabilities or common sense. \n",
      "\n",
      "  However, their apparent fluency \n",
      "obscures serious deficiencies that constrain real-world utility.\n",
      "----------------------------------------\n",
      "Page 62:\n",
      "Topics:\n",
      "  For example, an LLM could fluently discuss macroeconomic \n",
      "principles used in financial analysis, but it would fail to actually conduct analysis by retrieving \n",
      "current performance data and computing relevant statistics.\n",
      "  Similarly, an LLM may eloquently describe \n",
      "a past news event but then falter if asked for the latest developments on the same story today.\n",
      "\n",
      "  An LLM \n",
      "would have zero awareness of current events that occurred after its training data cut-off date. \n",
      "\n",
      "  Even when discussing topics con-\n",
      "tained in its training data, an LLM struggles to incorporate real-time context and specifics with-\n",
      "out retrieving external knowledge.\n",
      "  Chapter 2\n",
      "39\n",
      "•\t\n",
      "Lack of context: LLMs struggle to incorporate relevant context like previous conversa-\n",
      "tions and the supplementary details that are needed for coherent and useful responses.\n",
      "•\t\n",
      "Hallucination risks: Insufficient knowledge on certain topics can lead to the generation \n",
      "of incorrect or nonsensical content by LLMs if not properly grounded.\n",
      "\n",
      "  •\t\n",
      "Biases and discrimination: Depending on the data they were trained on, LLMs can exhibit \n",
      "biases that can be religious, ideological, or political in nature.\n",
      "\n",
      "  •\t\n",
      "Lack of context: LLMs may struggle to understand and incorporate context from previous \n",
      "prompts or conversations.\n",
      "Subtopics:\n",
      "  Without dynamic lookup abilities, its \n",
      "financial discussion remains generic and theoretical.\n",
      "  Architecting solutions that combine LLMs with external data sources, analytical programs, and \n",
      "tool integrations can help overcome these limitations.\n",
      "  Their impressive natural \n",
      "language abilities need appropriate grounding and actions to produce substantive insights be-\n",
      "yond eloquent but hollow text.\n",
      "\n",
      "  As mentioned, \n",
      "LLMs face significant limitations in their lack of real-time knowledge and inability to take actions \n",
      "themselves, which restricts their effectiveness in many real-world contexts.\n",
      "  Additionally, LLMs cannot interact dynamically with the world around them.\n",
      "  Let’s illustrate some of these limitations a bit more since they are very important.\n",
      "  Asking an LLM about breaking news or the latest societal developments would leave it unable \n",
      "to construct responses without external grounding.\n",
      "\n",
      "  They cannot check \n",
      "the weather, look up local data, or access documents.\n",
      "  But in isolation, LLMs lack connection to \n",
      "the real-world context, which is often essential for useful applications.\n",
      "  They are confined to the training \n",
      "data used to develop them, which inevitably becomes increasingly outdated over time.\n",
      "  •\t\n",
      "Lack of transparency: The behavior of large, complex models can be opaque and difficult \n",
      "to interpret, posing challenges to alignment with human values.\n",
      "\n",
      "  For instance, LLMs \n",
      "have no inherent connection to external information sources.\n",
      "  They may not remember previously mentioned details or may \n",
      "fail to provide additional relevant information beyond the given prompt.\n",
      "\n",
      "  With no ability to perform web searches, \n",
      "interface with APIs, run calculations, or take any practical actions based on new prompts, LLMs \n",
      "operate solely within the confines of pre-existing information.\n",
      "----------------------------------------\n",
      "Page 63:\n",
      "Topics:\n",
      "  The cut-off day issue is illustrated here in \n",
      "the OpenAI ChatGPT interface asking about LangChain:\n",
      "Figure 2.1: ChatGPT – a lack of up-to-date information\n",
      "In this case, the model was able to correctly catch the problem and give the correct feedback – \n",
      "this is not always the case, though.\n",
      "  LangChain for LLM Apps\n",
      "40\n",
      "Let’s look at a few examples of problems with LLMs.\n",
      "  Asking the same question in \n",
      "the OpenAI playground, I got this response:\n",
      "Figure 2.2: OpenAI playground with GPT 3.5\n",
      "\n",
      "Subtopics:\n",
      "  If you access the model through other endpoints or use other \n",
      "models, it might just make up the information (hallucinate).\n",
      "  Also, it might not have knowledge \n",
      "about certain entities, or it may refer to different entities entirely.\n",
      "----------------------------------------\n",
      "Page 64:\n",
      "Topics:\n",
      "  Chapter 2\n",
      "41\n",
      "In this case, we can see that the model talks about a different LangChain, which is a decentralized \n",
      "blockchain-based translation platform.\n",
      "  Again, we can illustrate this with a simple demonstration:\n",
      "Figure 2.3: ChatGPT math solving\n",
      "As you can see, the model comes up with the correct response for the first question but fails with the \n",
      "second.\n",
      "  Just in case you were wondering what the true result is, if we use a calculator, we get this:\n",
      "Figure 2.4: Multiplication with a calculator (BC)\n",
      "\n",
      "Subtopics:\n",
      "  It can be remedied by accessing external data, such as weather APIs, user \n",
      "preferences, or relevant information from the web, and this is essential for creating personalized \n",
      "and accurate language-driven applications.\n",
      "\n",
      "  As an example, \n",
      "even advanced LLMs perform poorly at high-school level math and cannot perform simple math \n",
      "operations that they haven’t seen before.\n",
      "  This is a problem of relevance, which can be referred \n",
      "to as a hallucination.\n",
      "  LLMs struggle with certain tasks that involve logical reasoning or math problems.\n",
      "----------------------------------------\n",
      "Page 65:\n",
      "Topics:\n",
      "  Mitigating these limitations includes techniques like:\n",
      "•\t\n",
      "Retrieval augmentation: This technique accesses knowledge bases to supplement an \n",
      "LLM’s outdated training data, providing external context and reducing hallucination risk.\n",
      "\n",
      "  As for reasoning, for example, an LLM may correctly identify a fruit’s density and water’s density \n",
      "when asked about those topics independently, but it would struggle to synthesize those facts to \n",
      "determine if the fruit will float (this being a multi-hop question).\n",
      "  An LLM is not a suitable tool for the job in this case.\n",
      "\n",
      "  This involves the careful crafting of prompts by providing critical \n",
      "context that guides appropriate responses.\n",
      "•\t\n",
      "Monitoring, filtering, and reviews: This involves ongoing and effective oversight of \n",
      "emerging issues regarding the application’s input and output to detect issues.\n",
      "  For instance, Microsoft’s Tay chatbot was \n",
      "taken offline shortly after launch in 2016 due to offensive tweets resulting from toxic interactions.\n",
      "\n",
      "  •\t\n",
      "Memory:\n",
      "  Training and tuning the LLM on more appropriate data for the application \n",
      "domain and principles.\n",
      "  •\t\n",
      "Fine-tuning:\n",
      "  LangChain for LLM Apps\n",
      "42\n",
      "The LLM hasn’t stored the result of the calculation or hasn’t encountered it often enough in the \n",
      "training data for it to be reliably remembered as encoded in its weights.\n",
      "Subtopics:\n",
      "  Therefore, it fails to cor-\n",
      "rectly come up with the solution.\n",
      "  Let’s see how we can address these challenges.\n",
      "\n",
      "  •\t\n",
      "Prompt engineering:\n",
      "  •\t\n",
      "\n",
      "  Deploying chatbots and other applications using LLMs requires thoughtful design and monitoring \n",
      "to address risks like bias and inappropriate content.\n",
      "  c.\t\n",
      "Human reviews provide insight into model behavior and output.\n",
      "\n",
      "  b.\t Constitutional principles monitor and filter unethical or inappropriate content.\n",
      "\n",
      "  This adapts the model’s behavior for its specific purpose.\n",
      "\n",
      "  Retains conversation context by persisting conversation data and context across \n",
      "interactions.\n",
      "\n",
      "  This \n",
      "includes the following:\n",
      "a.\t\n",
      "Filters, like block lists, sensitivity classifiers, and banned word filters, can auto-\n",
      "matically flag issues.\n",
      "\n",
      "  Chaining: This technique integrates actions like searches and calculations.\n",
      "\n",
      "  The model fails to bridge its \n",
      "disjointed knowledge.\n",
      "\n",
      "  How can we mitigate LLM limitations?\n",
      "\n",
      "  Both man-\n",
      "ual reviews and automated filters then correct potential problems with the output.\n",
      "----------------------------------------\n",
      "Page 66:\n",
      "Topics:\n",
      "  Chapter 2\n",
      "43\n",
      "To re-emphasize what we previously mentioned, raw model scale alone cannot impart composi-\n",
      "tional reasoning or other missing capabilities.\n",
      "  Frameworks like LangChain simplify this while providing structure and oversight \n",
      "for responsible LLM use.\n",
      "  This is often done via a chain of \n",
      "one or multiple prompted calls to LLMs but can also make use of other external services (such \n",
      "as APIs or data sources) to achieve tasks.\n",
      "\n",
      "  Traditional software applications typically follow a multi-layer architecture:\n",
      "Figure 2.5:\n",
      "  Adopting constitutional AI principles also encourages build-\n",
      "ing models capable of behaving ethically.\n",
      "  With diligent augmentation, we can create AI systems pre-\n",
      "viously not viable due to innate model limitations.\n",
      "  Filters act as a first line of defense.\n",
      "Subtopics:\n",
      "  Approaches like self-ask \n",
      "prompting mitigate these flaws by encouraging models to methodically decompose problems.\n",
      "\n",
      "  Together, \n",
      "these transform stochastic parrots into reasoning engines.\n",
      "\n",
      "  Prompting \n",
      "supplies context, chaining enables inference steps, and retrieval incorporates facts.\n",
      "  This comprehensive approach combines preparation, \n",
      "vigilance, and inherently beneficial design.\n",
      "\n",
      "  This brings us to our next topic of discussion.\n",
      "\n",
      "  Ongoing \n",
      "monitoring then catches any emerging issues, both through automation and human review. \n",
      "\n",
      "  However, securely integrating sources like databases adds \n",
      "complexity.\n",
      "  Explicit techniques like elicit prompting and chain-\n",
      "of-thought reasoning are needed to overcome the compositionality gap.\n",
      "  A traditional software application\n",
      "The client layer handles user interaction.\n",
      "  Integrating such tools into training pipelines provides the otherwise lacking faculties.\n",
      "  The backend layer processes logic, APIs, computations, etc.\n",
      "  Lastly, the database stores and \n",
      "retrieves data.\n",
      "\n",
      "  They allow composing prompted model queries and data sources to \n",
      "surmount standalone LLM deficits.\n",
      "  What is an LLM app?\n",
      "\n",
      "  Connecting LLMs to external data further reduces hallucination risks and enhances responses \n",
      "with accurate, up-to-date information.\n",
      "  The frontend layer handles presentation and business \n",
      "logic.\n",
      "  Combining LLMs with other tools into applications using specialized tooling, LLM-powered \n",
      "applications have the potential to transform our digital world.\n",
      "  Thoughtful prompt engineering and fine-tuning prepare models for real-world use.\n",
      "----------------------------------------\n",
      "Page 67:\n",
      "Topics:\n",
      "  In the simplest possible cases, the frontend, parsing, and knowledge base parts are sometimes \n",
      "not explicitly defined, leaving us with just the client, the prompt, and the LLM:\n",
      "Figure 2.6: A simple LLM application\n",
      "LLM apps can integrate external services via:\n",
      "•\t\n",
      "Function APIs to access web tools and databases.\n",
      "\n",
      "  •\t\n",
      "A prompt engineering layer to construct prompts that guide the LLM.\n",
      "•\t\n",
      "An LLM backend to analyze prompts and produce relevant text responses.\n",
      "\n",
      "  Retrieval augmented generation (RAG), which we will discuss in Chapter 5, Building a Chatbot like \n",
      "ChatGPT, enhances the LLM with external knowledge.\n",
      "  •\t\n",
      "Optional integration with external services via function APIs, knowledge bases, and rea-\n",
      "soning algorithms to augment the LLM’s capabilities.\n",
      "\n",
      "  For instance:\n",
      "•\t\n",
      "Function calling allows parameterized API requests.\n",
      "\n",
      "  •\t\n",
      "Advanced reasoning algorithms for complex logic chains.\n",
      "•\t\n",
      "Retrieval augmented generation via knowledge bases.\n",
      "\n",
      "  •\t\n",
      "An output parsing layer to interpret LLM responses for the application interface.\n",
      "\n",
      "  •\t\n",
      "Reasoning algorithms like chain-of-thought facilitate multi-step logic.\n",
      "\n",
      "  These extensions expand the capabilities \n",
      "of LLM apps beyond the LLM’s knowledge alone.\n",
      "  LLM apps typically have the following components:\n",
      "•\t\n",
      "A client layer to collect user input as text queries or decisions.\n",
      "\n",
      "  •\t\n",
      "SQL functions enable conversational database queries.\n",
      "\n",
      "  This is illustrated here:\n",
      "Figure 2.7: An advanced LLM application\n",
      "\n",
      "  LangChain for LLM Apps\n",
      "44\n",
      "In contrast, an LLM app is an application that utilizes an LLM to understand natural language \n",
      "prompts and generate responsive text outputs.\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 68:\n",
      "Topics:\n",
      "  •\t\n",
      "Automated content creation:\n",
      "  Apps can leverage LLMs to generate content like articles, \n",
      "emails, code, and more based on a text prompt.\n",
      "\n",
      "  Prompt engineering constructs guide the LLM, considering external knowledge or capability (or \n",
      "earlier interactions) without changes to the model itself.\n",
      "  Chapter 2\n",
      "45\n",
      "As can be seen in the preceding figure, the client layer collects user text queries and decisions. \n",
      "\n",
      "  The key capability LLM apps use is the ability to understand nuanced language in prompts and \n",
      "generate coherent, human-like text responses.\n",
      "  •\t\n",
      "Responses can be personalized and contextualized based on past interactions.\n",
      "•\t\n",
      "Advanced reasoning algorithms enable complex, multi-step inference chains.\n",
      "\n",
      "  But responsible data practices are critical – PII should be kept off public platforms and models \n",
      "should be fine-tuned in-house when needed.\n",
      "  •\t\n",
      "Intelligent search engines: LLM apps can parse search queries written in natural language \n",
      "and generate relevant results.\n",
      "\n",
      "  •\t\n",
      "Dynamic responses based on the LLM or on up-to-date information retrieved in real time.\n",
      "\n",
      "  A knowledge base can enhance the LLM’s information, and optionally, like \n",
      "a database backend in a traditional app, information can be written to it.\n",
      "\n",
      "  The LLM provides human-like language capabilities without manual coding.\n",
      "  The LLM backend dynamically under-\n",
      "stands and responds to the prompts based on its training.\n",
      "  LLM applications are important for several reasons:\n",
      "•\t\n",
      "The LLM backend handles language in a nuanced, human-like way without hardcoded \n",
      "rules.\n",
      "\n",
      "  Output parsing interprets the LLM \n",
      "text for the frontend.\n",
      "Subtopics:\n",
      "  Future research \n",
      "must address concerns around potential misuse, biases, and limitations.\n",
      "\n",
      "  This facilitates more natural interactions and \n",
      "workflows compared to traditional code.\n",
      "\n",
      "  Both the frontend and the output parser could \n",
      "include moderation and enforcing rules about behavior, privacy, and security.\n",
      "  The integration \n",
      "of LLMs with external services, knowledge, and reasoning algorithms eases the development of \n",
      "innovative applications.\n",
      "\n",
      "  We will see a lot of examples of LLM apps throughout this book; here are a few that we’ll encounter:\n",
      "•\t\n",
      "Chatbots and virtual assistants: These apps use LLMs like ChatGPT to have natural \n",
      "conversations with users and assist with tasks like scheduling, customer service, and \n",
      "information lookup.\n",
      "\n",
      "  Therefore, there is \n",
      "no need to manually anticipate and code every language scenario in advance.\n",
      "----------------------------------------\n",
      "Page 69:\n",
      "Topics:\n",
      "  LangChain simplifies the development of sophisticated LLM applications by providing reusable \n",
      "components and pre-assembled chains.\n",
      "  The LangChain framework aims to enable \n",
      "precisely this kind of integration, facilitating the development of context-aware, reasoning-based \n",
      "applications.\n",
      "  •\t\n",
      "Sentiment analysis: You can analyze customer feedback, reviews, and social posts using \n",
      "an LLM app to summarize sentiment and extract key themes.\n",
      "\n",
      "  The project has attracted \n",
      "millions in venture capital funding from the likes of Sequoia Capital and Benchmark, who sup-\n",
      "plied funding to Apple, Cisco, Google, WeWork, Dropbox, and many other successful companies.\n",
      "\n",
      "  LangChain for LLM Apps\n",
      "46\n",
      "•\t\n",
      "Question answering: Users can ask an LLM app questions in plain language and receive \n",
      "informative answers that are quickly sourced from the model’s knowledge.\n",
      "\n",
      "  •\t\n",
      "Data analysis: You can use LLMs for automated data analysis and visualization to extract \n",
      "insights.\n",
      "•\t\n",
      "Code generation: You can set up software pair-programming assistants that can help \n",
      "solve business problems.\n",
      "\n",
      "  •\t\n",
      "Text summarization: You can automatically generate concise summaries of longer text \n",
      "documents and articles using an LLM backend.\n",
      "\n",
      "  LangChain reduces this learning curve through its \n",
      "abstractions and composable structure.\n",
      "\n",
      "  What is LangChain?\n",
      "Created in 2022 by Harrison Chase, LangChain is an open-source Python framework for building \n",
      "LLM-powered applications.\n",
      "  Building impactful LLM apps involves challenges like prompt engineering, bias mitigation, pro-\n",
      "ductionizing, and integrating external data.\n",
      "  LangChain addresses pain points associated with LLMs and provides an intuitive \n",
      "framework for creating customized NLP solutions.\n",
      "\n",
      "Subtopics:\n",
      "  It provides developers with modular, easy-to-use components for \n",
      "connecting language models with external data sources and services.\n",
      "  Developers can combine these building blocks to carry \n",
      "out complex workflows.\n",
      "\n",
      "  Its modular architecture abstracts access to LLMs and \n",
      "external services into a unified interface.\n",
      "  The true power of LLMs lies not in LLMs being used in isolation but in LLMs being combined \n",
      "with other sources of knowledge and computation.\n",
      "----------------------------------------\n",
      "Page 70:\n",
      "Topics:\n",
      "  In particular, LangChain’s support for chains, agents, tools, and memory allows developers to \n",
      "build applications that can interact with their environment in a more sophisticated way and store \n",
      "and reuse information over time.\n",
      "  •\t\n",
      "Memory and persistence for statefulness across executions.\n",
      "\n",
      "  As mentioned, LangChain is open source and written in Python, although companion projects \n",
      "exist that are implemented in JavaScript or – more precisely – TypeScript (LangChain.js), and the \n",
      "fledgling Langchain.rb project for Ruby, which comes with a Ruby interpreter for code execution. \n",
      "\n",
      "  The key benefits LangChain offers developers are:\n",
      "•\t\n",
      "Modular architecture for flexible and adaptable LLM integrations.\n",
      "\n",
      "  •\t\n",
      "Chaining together multiple services beyond just LLMs.\n",
      "\n",
      "  Chapter 2\n",
      "47\n",
      "Beyond basic LLM API usage, LangChain facilitates advanced interactions like conversational \n",
      "context and persistence through agents and memory.\n",
      "  There’s even a chatbot, ChatLangC-\n",
      "hain, that can answer questions about the LangChain documentation.\n",
      "  It’s built using LangChain \n",
      "and FastAPI and is available online through the documentation website!\n",
      "\n",
      "  There are active discussions on a Discord chat server, multiple blogs, and regular meetups taking \n",
      "place in various cities, including San Francisco and London.\n",
      "Subtopics:\n",
      "  This allows for chatbots, gathering external \n",
      "data, and more.\n",
      "\n",
      "  In this book, we focus on the Python flavor of the framework.\n",
      "\n",
      "  Support for action plans and strategies improves \n",
      "the performance and robustness of applications.\n",
      "  For many developers, \n",
      "the learning curve can be a blocking factor to impactfully leveraging LLMs.\n",
      "\n",
      "  Its modular design makes it easy to build complex applications \n",
      "that can be adapted to a variety of domains.\n",
      "  The support for memory and access to external \n",
      "information reduces hallucinations, thus enhancing reliability.\n",
      "\n",
      "  •\t\n",
      "Open-source access and community support.\n",
      "\n",
      "  While resources like documentation, courses, and communities help accelerate the learning pro-\n",
      "cess, developing expertise in applying LLMs takes dedicated time and effort.\n",
      "  •\t\n",
      "Goal-driven agent interactions instead of isolated calls.\n",
      "\n",
      "----------------------------------------\n",
      "Page 71:\n",
      "Topics:\n",
      "  LlamaHub and LangChainHub provide open libraries of reusable elements to build sophisticated \n",
      "LLM systems in a simplified manner.\n",
      "  LlamaHub is a library of data loaders, readers, and tools \n",
      "created by the LlamaIndex community.\n",
      "  langchain.com):\n",
      "Figure 2.8: LangChain integrations as of September 2023\n",
      "As for the broader ecosystem, LangSmith is a platform that complements LangChain by providing \n",
      "robust debugging, testing, and monitoring capabilities for LLM applications.\n",
      "  As mentioned, it has an immense number of integrations already, with many new ones being \n",
      "added every week.\n",
      "  LlamaHub simplifies the creation of customized data agents to unlock \n",
      "LLM capabilities.\n",
      "\n",
      "  LangChain for LLM Apps\n",
      "48\n",
      "LangChain comes with many extensions and a larger ecosystem that is developing around it. \n",
      "\n",
      "Subtopics:\n",
      "  It provides utilities to easily connect LLMs to diverse \n",
      "knowledge sources.\n",
      "  For example, devel-\n",
      "opers can quickly debug new chains by viewing detailed execution traces.\n",
      "  Alternative prompts \n",
      "and LLMs can be evaluated against datasets to ensure quality and consistency.\n",
      "  The loaders ingest data for retrieval, while tools enable models to read/write \n",
      "to external data services.\n",
      "  This screenshot highlights a few of the integrations (source: integrations.\n",
      "\n",
      "  Usage analytics \n",
      "empower data-driven decisions around optimizations.\n",
      "\n",
      "----------------------------------------\n",
      "Page 72:\n",
      "Topics:\n",
      "  Inspired by the Hugging Face Hub, it aims to be a one-stop resource for discover-\n",
      "ing high-quality building blocks to compose complex LLM apps.\n",
      "  LangChain and LangFlow can be deployed locally, for example, \n",
      "using the Chainlit library, or on different platforms, including Google Cloud.\n",
      "  LangFlow and Flowise are UIs that allow chaining LangChain components in an executable \n",
      "flowchart by dragging sidebar components onto the canvas and connecting them together to \n",
      "create your pipeline.\n",
      "  The langchain-\n",
      "serve library helps to deploy both LangChain and LangFlow on the Jina AI cloud as LLM-apps-\n",
      "as-a-service with a single command.\n",
      "\n",
      "  This is a quick way to experiment and prototype pipelines and is illustrated \n",
      "in the following screenshot of Flowise (source: https://github.com/FlowiseAI/Flowise):\n",
      "Figure 2.9: Flowise UI with an agent that uses an LLM, a calculator, and a search tool\n",
      "You can see an agent (discussed later in this chapter) that is connected to a search interface (Serp \n",
      "API), an LLM, and a calculator.\n",
      "  Future plans involve adding support for chains, agents, and other \n",
      "key LangChain components.\n",
      "\n",
      "  Chapter 2\n",
      "49\n",
      "LangChainHub is a central repository for sharing artifacts like prompts, chains, and agents used \n",
      "in LangChain.\n",
      "Subtopics:\n",
      "  The initial launch focuses on a \n",
      "collection of reusable prompts.\n",
      "----------------------------------------\n",
      "Page 73:\n",
      "Topics:\n",
      "  It aims to simplify what can otherwise \n",
      "be complex LLM application development.\n",
      "  For example, developers can put together multiple LLM calls and other components \n",
      "in a sequence to create complex applications for things like chatbot-like social interactions, data \n",
      "extraction, and data analysis.\n",
      "  The most innocuous example of a chain is probably the \n",
      "PromptTemplate, which passes a formatted response to a language model.\n",
      "\n",
      "  Chains can even enforce policies, like moderating toxic outputs or aligning with ethical princi-\n",
      "ples.\n",
      "  LangChain implements chains to make sure the content of the output is not toxic, does not \n",
      "otherwise violate OpenAI’s moderation rules (OpenAIModerationChain), or that it conforms to \n",
      "ethical, legal, or custom principles (ConstitutionalChain).\n",
      "\n",
      "  What are chains?\n",
      "Chains are a critical concept in LangChain for composing modular components into reusable \n",
      "pipelines.\n",
      "  By the end, you should have the level of un-\n",
      "derstanding that’s required to architect systems with LangChain.\n",
      "  Exploring key components of LangChain\n",
      "Chains, agents, memory, and tools enable the creation of sophisticated LLM applications that \n",
      "go beyond basic API calls to a single LLM.\n",
      "  Prompt chaining is a technique that can be used to improve the performance of LangChain \n",
      "applications, which involves chaining together multiple prompts to autocomplete a more com-\n",
      "plex response.\n",
      "  LangChain for LLM Apps\n",
      "50\n",
      "While still relatively new, LangChain unlocks more advanced LLM applications via its combina-\n",
      "tion of components like memory, chaining, and agents.\n",
      "  Hence, it is crucial at this point in the chapter that \n",
      "we shift focus to the workings of LangChain and its components.\n",
      "\n",
      "Subtopics:\n",
      "  In the following dedicated subsections on these key \n",
      "concepts, we’ll consider how they enable the development of capable systems by combining \n",
      "language models with external data and services.\n",
      "\n",
      "  These are called utility chains, because \n",
      "they combine language models with specific tools.\n",
      "\n",
      "  We won’t dive into implementation patterns in this chapter; however, we will discuss in more \n",
      "detail what some of these components are good for.\n",
      "  Let’s start with chains!\n",
      "\n",
      "  More complex chains integrate models with tools like LLMMath, for math-related \n",
      "queries, or SQLDatabaseChain, for querying databases.\n",
      "  In the most generic terms, a chain is a sequence of calls to compo-\n",
      "nents, which can include other chains.\n",
      "----------------------------------------\n",
      "Page 74:\n",
      "Topics:\n",
      "  •\t\n",
      "Readability: Each step in a pipeline is clear.\n",
      "\n",
      "  Chains deliver several key benefits:\n",
      "•\t\n",
      "Modularity: Logic is divided into reusable components.\n",
      "\n",
      "  Typically, developing a LangChain chain involves breaking down a workflow into logical steps, \n",
      "like data loading, processing, model querying, and so on.\n",
      "  In a paper \n",
      "by researchers at Carnegie Mellon, Allen Institute, University of Washington, NVIDIA, UC San \n",
      "Diego, and Google Research in May 2023 (SELF-REFINE: Iterative Refinement with Self-Feedback), \n",
      "this strategy has been found to improve task performance by about 20% on average across a \n",
      "benchmark including dialogue responses, math reasoning, and code reasoning.\n",
      "\n",
      "  Chapter 2\n",
      "51\n",
      "An LLMCheckerChain verifies statements to reduce inaccurate responses using a technique called \n",
      "self-reflection.\n",
      "  Let’s discuss agents next\n",
      "Subtopics:\n",
      "  Steps should be stateless functions \n",
      "to maximize reusability.\n",
      "  A RouterChain can dynamically select which retrieval system, \n",
      "such as prompts or indexes, to use.\n",
      "\n",
      "  •\t\n",
      "Tool integration: Easily incorporate LLMs, databases, APIs, etc.\n",
      "•\t\n",
      "Productivity:\n",
      "  Well-designed chains embrace sin-\n",
      "gle-responsibility components being pipelined together.\n",
      "  Configurations should be made customizable.\n",
      "  Monitoring and logging can be enabled with \n",
      "different mechanisms, including callbacks.\n",
      "\n",
      "  •\t\n",
      "Composability: Components can be sequenced flexibly.\n",
      "\n",
      "  •\t\n",
      "Maintainability: Steps can be added, removed, and swapped.\n",
      "•\t\n",
      "Reusability: Common pipelines become configurable chains.\n",
      "\n",
      "  The LLMCheckerChain can prevent hallucinations and reduce inaccurate respons-\n",
      "es by verifying the assumptions underlying the provided statements and questions.\n",
      "  Quickly build prototypes of configurable chains.\n",
      "\n",
      "  A few chains can make autonomous decisions.\n",
      "  Robust error handling \n",
      "with exceptions and errors is critical for reliability.\n",
      "  Like agents, router chains can decide which tool \n",
      "to use based on their descriptions.\n",
      "  Together, these benefits enable the encapsulation of complex workflows into easy-to-understand \n",
      "and adaptable chained pipelines.\n",
      "\n",
      "  and how they make their decisions!\n",
      "\n",
      "----------------------------------------\n",
      "Page 75:\n",
      "Topics:\n",
      "  The core idea \n",
      "in LangChain is the compositionality of LLMs and other components to work together.\n",
      "  Chains and agents are similar concepts and it’s worth unpicking their differences.\n",
      "  Agents are a key concept in LangChain for creating systems that interact dynamically with users \n",
      "and environments over time.\n",
      "  Tools (discussed later in this chapter) are functions the agent calls to take real-world actions. \n",
      "\n",
      "  LangChain for LLM Apps\n",
      "52\n",
      "\n",
      "  •\t\n",
      "Dynamic responses: Observing environment changes lets agents react and adapt.\n",
      "\n",
      "  The LLM is prompted with \n",
      "available tools, user input, and previous steps.\n",
      "Subtopics:\n",
      "  Providing the right tools and effectively describing them is critical for agents to accomplish goals.\n",
      "\n",
      "  The agent observes the environment, decides which chain to execute based \n",
      "on that observation, takes the chain’s specified action, and repeats.\n",
      "\n",
      "  While chains define reusable logic by \n",
      "sequencing components, agents leverage chains to take goal-driven actions.\n",
      "  Agents provide several key benefits:\n",
      "•\t\n",
      "Goal-oriented execution: Agents can plan chains of logic targeting specific goals.\n",
      "\n",
      "  Agents decide which actions to take using LLMs as reasoning engines.\n",
      "  Both extend LLMs, but agents do so by orches-\n",
      "trating chains while chains compose lower-level modules.\n",
      "  The agent executor runtime orchestrates the loop of querying the agent, executing tool actions, \n",
      "and feeding observations back.\n",
      "  •\t\n",
      "Robustness: Errors can be handled by catching exceptions and trying alternate chains.\n",
      "\n",
      "  •\t\n",
      "Statefulness: Agents can maintain memory and context across interactions.\n",
      "\n",
      "  It then selects the next action or final response.\n",
      "\n",
      "  This handles lower-level complexities like error handling, logging, \n",
      "and parsing.\n",
      "\n",
      "  •\t\n",
      "Composition: Agent logic combines reusable component chains.\n",
      "\n",
      "  Both \n",
      "chains and agents do that, but in different ways.\n",
      "  Agents combine and \n",
      "orchestrate chains.\n",
      "  What are agents?\n",
      "\n",
      "  An agent is an autonomous software entity that is capable of taking \n",
      "actions to accomplish goals and tasks.\n",
      "\n",
      "  Together, this enables agents to handle complex, multi-step workflows and continuously inter-\n",
      "active applications like chatbots.\n",
      "\n",
      "----------------------------------------\n",
      "Page 76:\n",
      "Topics:\n",
      "  Chapter 2\n",
      "53\n",
      "In the section about the limitations of LLMs, we’ve seen that for calculations, a simple calculator \n",
      "outperforms a model consisting of billions of parameters.\n",
      "  We’ll look more at the mechanics of how this works in Chapter 4, Building \n",
      "Capable Assistants.\n",
      "\n",
      "  We can see a simple app here, where an \n",
      "agent is connected to both an OpenAI model and a Python function:\n",
      "Figure 2.10: A simple LLM app with a Python function visualized in LangFlow\n",
      "Based on the input, the agent can decide to run a Python function.\n",
      "  Memory \n",
      "in LangChain refers to persisting information across chain executions to enable statefulness.\n",
      "\n",
      "Subtopics:\n",
      "  A key limitation of agents and chains is their statelessness – each execution occurs in isolation \n",
      "without retaining prior context.\n",
      "  This is where the concept of memory becomes critical.\n",
      "  Each agent also decides which \n",
      "tool to use and when.\n",
      "  In this case, an agent can decide to pass \n",
      "the calculation to a calculator or to a Python interpreter.\n",
      "----------------------------------------\n",
      "Page 77:\n",
      "Topics:\n",
      "  LangChain for LLM Apps\n",
      "54\n",
      "\n",
      "  •\t\n",
      "ConversationBufferWindowMemory retains only recent messages.\n",
      "•\t\n",
      "ConversationKGMemory summarizes exchanges as a knowledge graph for integration \n",
      "into prompts.\n",
      "•\t\n",
      "EntityMemory backed by a database persists agent state and facts.\n",
      "\n",
      "  In LangChain, memory refers to the persisting state between executions of a chain or agent.\n",
      "  For example, storing chat history context in memory improves the coherence and \n",
      "relevance of LLM responses over time.\n",
      "\n",
      "  LangChain provides a standard interface for memory, integrations with storage options like \n",
      "databases, and design patterns for effectively incorporating memory into chains and agents.\n",
      "\n",
      "  Several memory options exist – for example:\n",
      "•\t\n",
      "ConversationBufferMemory stores all messages in model history.\n",
      "  •\t\n",
      "NoSQL choices like MongoDB and Cassandra facilitate scalable unstructured data.\n",
      "\n",
      "  Beyond databases, purpose-built memory servers like Remembrall and Motörhead offer optimized \n",
      "conversational context.\n",
      "  This lowers API usage and costs, while still providing the agent or chain with the needed context.\n",
      "\n",
      "  •\t\n",
      "Managed cloud services like AWS DynamoDB remove infrastructure burdens.\n",
      "\n",
      "  Moreover, LangChain integrates many database options for durable storage:\n",
      "•\t\n",
      "SQL options like Postgres and SQLite enable relational data modeling.\n",
      "\n",
      "Subtopics:\n",
      "  Memory of objectives and completed \n",
      "tasks allows agents to track progress on multi-step goals across conversations.\n",
      "  This knowledge remains available even as real-world \n",
      "conditions change, keeping the agent contextually informed.\n",
      "  Robust \n",
      "memory approaches unlock key benefits for developers building conversational and interactive \n",
      "applications.\n",
      "  This increases latency \n",
      "and costs.\n",
      "\n",
      "  •\t\n",
      "Redis provides an in-memory database for high-performance caching.\n",
      "\n",
      "  What is memory?\n",
      "\n",
      "  Agents can also persist facts, relationships, and \n",
      "deductions about the world in memory.\n",
      "  Rather than treating each user input as an isolated prompt, chains can pass conversational mem-\n",
      "ory to models on each call to provide consistency.\n",
      "  The right memory approach depends on factors like persistence needs, \n",
      "data relationships, scale, and resources, but robustly retaining state is key for conversational and \n",
      "interactive applications.\n",
      "\n",
      "  In addition, re-\n",
      "taining information in memory reduces the number of calls to LLMs for repetitive information. \n",
      "\n",
      "----------------------------------------\n",
      "Page 78:\n",
      "Topics:\n",
      "  LangChain offers tools like document loaders, indexes, and vector stores, which \n",
      "facilitate the retrieval and storage of data for augmenting data retrieval in LLMs.\n",
      "\n",
      "  LangChain comes with a long list of tools \n",
      "that we can use in applications.\n",
      "  •\t\n",
      "Stocks: Connecting with stock market APIs like Alpha Vantage allows language models \n",
      "to query specific stock market information such as opening and closing prices, highest \n",
      "and lowest prices, and more.\n",
      "\n",
      "  Architecting effective memory patterns unlocks the \n",
      "next generation of capable and reliable AI systems.\n",
      "  Chapter 2\n",
      "55\n",
      "LangChain’s memory integrations, from short-term caching to long-term databases, enable the \n",
      "building of stateful, context-aware agents.\n",
      "  •\t\n",
      "Maps: By connecting with the Bing Map API or similar services, language models can \n",
      "retrieve location information, assist with route planning, provide driving distance cal-\n",
      "culations, and offer details about nearby points of interest.\n",
      "\n",
      "  •\t\n",
      "Weather: Weather APIs provide language models with real-time weather information \n",
      "for cities worldwide.\n",
      "Subtopics:\n",
      "  Tools can be combined with models to extend \n",
      "their capability.\n",
      "  •\t\n",
      "Machine translator:\n",
      "  This tool enables non-translation-dedicated \n",
      "language models to understand and answer questions in different languages.\n",
      "\n",
      "  •\t\n",
      "Slides: Language models equipped with slide-making tools can create slides using \n",
      "high-level semantics provided by APIs such as the python-pptx library or image retrieval \n",
      "from the internet based on given topics.\n",
      "  What are tools?\n",
      "Tools provide modular interfaces for agents to integrate external services like databases and \n",
      "APIs.\n",
      "  •\t\n",
      "Calculator: Language models can utilize a simple calculator tool to solve math problems. \n",
      "\n",
      "  There are many tools available, and here are just a few examples:\n",
      "\n",
      "  The calculator supports basic arithmetic operations, allowing the model to accurately \n",
      "solve mathematical queries in datasets specifically designed for math problem-solving.\n",
      "\n",
      "  Models can answer queries about current weather conditions or \n",
      "forecast the weather for specific locations within varying time periods.\n",
      "\n",
      "  These tools facilitate tasks related to slide creation \n",
      "that are required in various professional fields.\n",
      "\n",
      "  A short section will not be able to do this justice; however, I’ll \n",
      "attempt to give a brief overview.\n",
      "\n",
      "  A language model can use a machine translator to better compre-\n",
      "hend and process text in multiple languages.\n",
      "  Toolkits group tools that share resources.\n",
      "----------------------------------------\n",
      "Page 79:\n",
      "Topics:\n",
      "  •\t\n",
      "Search engine: By utilizing search engine APIs like Bing Search, language models can \n",
      "interact with search engines to extract information and provide answers to real-time \n",
      "queries.\n",
      "  These tools facilitate question-answering tasks using content re-\n",
      "trieved from Wikipedia.\n",
      "•\t\n",
      "Online shopping: Connecting language models with online shopping tools allows them \n",
      "to perform actions like searching for items, loading detailed information about products, \n",
      "selecting item features, going through shopping pages, and making purchase decisions \n",
      "based on specific user instructions.\n",
      "\n",
      "  These tools enhance the model’s ability to gather information from the web and \n",
      "deliver accurate responses.\n",
      "•\t\n",
      "Wikipedia: Language models equipped with Wikipedia search tools can search for specific \n",
      "entities on Wikipedia pages, look up keywords within a page, or disambiguate entities \n",
      "with similar names.\n",
      "  LangChain for LLM Apps\n",
      "56\n",
      "•\t\n",
      "Table processing: APIs built with pandas DataFrames enable language models to perform \n",
      "data analysis and visualization tasks on tables.\n",
      "  All these tools can give us advanced AI functionality, and there’s virtually no limit to the tools \n",
      "available.\n",
      "  •\t\n",
      "Knowledge graphs: Language models can query knowledge graphs using APIs that mimic \n",
      "human querying processes, such as finding candidate entities or relations, sending SPARQL \n",
      "queries, and retrieving results.\n",
      "  Additional tools include AI Painting, which allows language models to generate images using AI \n",
      "image generation models; 3D Model Construction, enabling language models to create 3D models \n",
      "using a sophisticated 3D rendering engine; Chemical Properties, assisting in resolving scientific \n",
      "inquiries about chemical properties using APIs like PubChem; and database tools that facilitate \n",
      "natural language access to database data for executing SQL queries and retrieving results.\n",
      "\n",
      "Subtopics:\n",
      "  These tools assist in answering questions based on factual \n",
      "knowledge stored in knowledge graphs.\n",
      "\n",
      "  By connecting to these tools, models can \n",
      "provide users with a more streamlined and natural experience for handling tabular data.\n",
      "\n",
      "  These various tools provide language models with additional functionalities and capabilities to \n",
      "perform tasks beyond text processing.\n",
      "  By connecting with these tools via APIs, language models \n",
      "can enhance their abilities in areas such as translation, math problem-solving, location-based \n",
      "queries, weather forecasting, stock market analysis, slide creation, table processing and analysis, \n",
      "image generation, text-to-speech conversion, and many more specialized tasks.\n",
      "\n",
      "  The use of different tools expands the scope of applications for language models \n",
      "and enables them to handle various real-world tasks more efficiently and effectively.\n",
      "\n",
      "  We can easily build custom tools to extend the capability of LLMs, as we’ll see in the \n",
      "next chapter.\n",
      "----------------------------------------\n",
      "Page 80:\n",
      "Topics:\n",
      "  •\t\n",
      "Document loaders: Ingest data from sources into documents with text and metadata. \n",
      "\n",
      "  Different \n",
      "methods for embedding documents vs. queries.\n",
      "•\t\n",
      "Vector stores: Store embedded document vectors for efficient similarity search and re-\n",
      "trieval.\n",
      "\n",
      "  •\t\n",
      "Tools: Interfaces that agents use to interact with external systems.\n",
      "\n",
      "  Chapter 2\n",
      "57\n",
      "After discussing chains, agents, memory, and tools, let’s put this all together to get a picture of \n",
      "how LangChain fits all of them together as moving parts.\n",
      "\n",
      "  •\t\n",
      "Text embeddings: Create vector representations of text for semantic search.\n",
      "  Provide interfaces to connect and query language models like \n",
      "GPT-3.\n",
      "  These components can be combined into pipelines also called chains that sequence the following \n",
      "actions:\n",
      "•\t\n",
      "Loading documents\n",
      "•\t\n",
      "Embedding for retrieval\n",
      "•\t\n",
      "Querying LLMs\n",
      "•\t\n",
      "Parsing outputs\n",
      "•\t\n",
      "Writing memory\n",
      "Chains match modules to application goals, while agents leverage chains for goal-directed in-\n",
      "teractions with users.\n",
      "  How does LangChain work?\n",
      "The LangChain framework simplifies building sophisticated LLM applications by providing mod-\n",
      "ular components that facilitate connecting language models with other data and services.\n",
      "  Enable loading files, webpages, videos, etc.\n",
      "•\t\n",
      "Document transformers: Manipulate documents via splitting, combining, filtering, trans-\n",
      "lating, etc. Help adapt data for models.\n",
      "\n",
      "Subtopics:\n",
      "  Can leverage vector \n",
      "stores.\n",
      "\n",
      "  Support async, streaming, and batch requests.\n",
      "\n",
      "  They repeatedly execute actions based on observations, plan optimal logic \n",
      "chains, and persist memory across conversations.\n",
      "\n",
      "  The modules, ranging from simple to advanced, are:\n",
      "•\t\n",
      "LLMs and chat models:\n",
      "  The \n",
      "framework organizes capabilities into modules spanning from basic LLM interaction to complex \n",
      "reasoning and persistence.\n",
      "\n",
      "  •\t\n",
      "Retrievers: General interface to return documents based on a query.\n",
      "----------------------------------------\n",
      "Page 81:\n",
      "Topics:\n",
      "  Document loaders allow ingesting data from various sources into documents containing text and \n",
      "metadata.\n",
      "  LangChain for LLM Apps\n",
      "58\n",
      "•\t\n",
      "Agents: Goal-driven systems that use LLMs to plan actions based on environment ob-\n",
      "servations.\n",
      "\n",
      "  Although LangChain doesn’t supply models itself, it supports integration through LLM wrappers \n",
      "with various language model providers, enabling the app to interact with chat models as well as \n",
      "text embedding model providers.\n",
      "  This provides \n",
      "a flexible API for integrating different language models.\n",
      "\n",
      "  We’ll discuss prompts starting \n",
      "in Chapter 3, Getting Started with LangChain, and prompt engineering is the topic of Chapter 8, \n",
      "Customizing LLMs and Their Output.\n",
      "\n",
      "  Together, the preceding capabilities facilitate the building of robust, efficient, and capable LLM \n",
      "applications with LangChain.\n",
      "  LangChain offers interfaces to connect with and query LLMs like GPT-3 and chat models.\n",
      "  •\t\n",
      "Memory: Persist information across conversations and workflows by reading/writing \n",
      "session data.\n",
      "\n",
      "  We’ll go into some of these options in Chapter \n",
      "3, Getting Started with LangChain.\n",
      "\n",
      "  A core building block of LangChain is the prompt class, which allows users to interact with LLMs \n",
      "by providing concise instructions or examples.\n",
      "  Supported providers include OpenAI, HuggingFace, Azure, and \n",
      "Anthropic.\n",
      "Subtopics:\n",
      "  •\t\n",
      "Toolkits: Initialize groups of tools that share resources like databases.\n",
      "\n",
      "  Hook into pipeline stages for logging, monitoring, streaming, and others.\n",
      "  •\t\n",
      "Callbacks:\n",
      "  This data can then be manipulated via document transformers – splitting, combining, \n",
      "filtering, translating, etc.\n",
      "  Each of them has its own complexity and importance, so it’s im-\n",
      "portant to explain a bit more.\n",
      "\n",
      "  Prompt engineering helps optimize prompts \n",
      "for optimal model performance.\n",
      "  Call-\n",
      "backs enable monitoring chains.\n",
      "\n",
      "  These tools adapt external data for use in LLMs.\n",
      "\n",
      "  Templates give flexibility in terms of input and the available \n",
      "collection of prompts is battle-tested in a range of applications.\n",
      "  Providing a standardized interface means being able to effortlessly swap out models to \n",
      "save money and energy or get better performance.\n",
      "  These \n",
      "interfaces support asynchronous requests, streaming responses, and batch queries.\n",
      "----------------------------------------\n",
      "Page 82:\n",
      "Topics:\n",
      "  Chapter 2\n",
      "59\n",
      "Data loaders include modules for storing data and utilities for interacting with external systems, \n",
      "like web searches or databases, and most importantly data retrieval.\n",
      "  Vector stores come in when working with large documents, where the document needs to be \n",
      "chunked up in order to be passed to the LLM.\n",
      "  While the next chapters will dig into the details of some usage patterns and use cases \n",
      "of LangChain components, the following resources provide invaluable information \n",
      "on LangChain’s components and how they can be assembled into pipelines.\n",
      "\n",
      "  We’ll explore these in \n",
      "Chapter 5, Building a Chatbot like ChatGPT.\n",
      "\n",
      "  Examples are Microsoft Word \n",
      "documents (.docx), HyperText Markup Language (HTML), and other common formats such as \n",
      "PDF, text files, JSON, and CSV.\n",
      "  Other tools will send emails to prospective customers, post funny \n",
      "puns for your followers, or send Slack messages to your coworkers.\n",
      "  For full details on the dozens of available modules, refer to the comprehensive \n",
      "LangChain API reference: https://api.python.langchain.com/. There are also \n",
      "hundreds of code examples demonstrating real-world use cases: https://python.\n",
      "\n",
      "  There are a few other frameworks besides LangChain; however, we’ll see that LangChain is one \n",
      "of the most prominent and feature rich of them.\n",
      "\n",
      "  We’ll look at these in Chapter \n",
      "5, Building a Chatbot like ChatGPT.\n",
      "\n",
      "  These include Alibaba Cloud OpenSearch, \n",
      "AnalyticDB for PostgreSQL, Meta AI’s Annoy library for Approximate Nearest Neighbor (ANN) \n",
      "search, Cassandra, Chroma, Elasticsearch, Facebook AI Similarity Search (Faiss), MongoDB \n",
      "Atlas Vector Search, PGVector as a vector similarity search for Postgres, Pinecone, scikit-learn \n",
      "(SKLearnVectorStore for k-nearest neighbor search), and many more.\n",
      "Subtopics:\n",
      "  This \n",
      "enables semantic search by finding text with the most similar vector representations.\n",
      "  These parts of the document would be stored as \n",
      "embeddings, which means that they are vector representations of the information.\n",
      "  All these \n",
      "tools enhance the LLMs’ knowledge and improve their performance in applications like question \n",
      "answering and summarization.\n",
      "\n",
      "  langchain.com/docs/use_cases/.\n",
      "\n",
      "  Text embedding models create vector representations of text that capture semantic meaning.\n",
      "  Vector stores \n",
      "build on this by indexing embedded document vectors for efficient similarity-based retrieval.\n",
      "\n",
      "  There are numerous integrations for vector storage.\n",
      "----------------------------------------\n",
      "Page 83:\n",
      "Topics:\n",
      "  There are several open-source frameworks for building dynamic LLM applications.\n",
      "  AutoGen is a project recently released by Microsoft that has already garnered some interest. \n",
      "\n",
      "  This graph shows their popularity over time \n",
      "(data source: GitHub star history, https://star-history.com/):\n",
      "Figure 2.11: Comparison of popularity between different frameworks in Python\n",
      "\n",
      "  LlamaIndex focuses on advanced retrieval rather than on the broader aspects of LLM apps.\n",
      "  Similar-\n",
      "ly, Haystack focuses on creating large-scale search systems with components designed specifically \n",
      "for scalable information retrieval using retrievers, readers, and other data handlers combined \n",
      "with semantic indexing via pre-trained models.\n",
      "\n",
      "  LangChain for LLM Apps\n",
      "60\n",
      "Comparing LangChain with other frameworks\n",
      "LLM application frameworks have been developed to provide specialized tooling that can har-\n",
      "ness the power of LLMs effectively to solve complex problems.\n",
      "  It \n",
      "is also the least popular in terms of stars on GitHub.\n",
      "  In this book, we’ll see a lot of the functionality of LangChain and explore its features, which are \n",
      "the reason its popularity is exploding right now.\n",
      "\n",
      "  We can see the number of stars on GitHub over time for each project.\n",
      "  Haystack is the oldest of \n",
      "the compared frameworks, having started in early 2020 (as per the earliest GitHub commits).\n",
      "  A few libraries have emerged that \n",
      "meet the requirements of effectively combining generative AI models with other tools to build \n",
      "LLM applications.\n",
      "\n",
      "  LangChain, LlamaIndex (previously called \n",
      "GPTIndex), and SuperAGI were started in late 2022 or early 2023, and they have all fallen short \n",
      "in popularity in a noticeably brief time compared to LangChain, which has been growing impres-\n",
      "sively.\n",
      "Subtopics:\n",
      "  They all offer \n",
      "value in developing cutting-edge LLM applications.\n",
      "----------------------------------------\n",
      "Page 84:\n",
      "Topics:\n",
      "  However, it’s not as extensive and well supported as LangChain.\n",
      "\n",
      "  AutoGen simplifies the building, orchestrating, and optimizing of complex workflows powered \n",
      "by LLMs.\n",
      "  The LangChain framework simplifies the building of sophisticated \n",
      "applications powered by LLMs that can mitigate these shortcomings.\n",
      "  Summary\n",
      "LLMs produce convincing language but have significant limitations in terms of reasoning, knowl-\n",
      "edge, and access to tools.\n",
      "  There are other LLM app frameworks in languages such as Rust, JavaScript, Ruby, and Java.\n",
      "  Chapter 2\n",
      "61\n",
      "LangChain excels at chaining LLMs together using agents to delegate actions to models.\n",
      "  I haven’t included AutoGPT (and similar tools like AutoLlama), a recursive application that breaks \n",
      "down tasks, because its reasoning capability, based on human and LLM feedback, is very limited \n",
      "compared to LangChain.\n",
      "  Frameworks like LangChain aim to lower barriers by providing guardrails, conventions, and \n",
      "pre-built modules, but foundational knowledge remains important for avoiding pitfalls and \n",
      "maximizing value from LLMs.\n",
      "  AutoGen streamlines \n",
      "agent definition and interaction to automatically compose optimal LLM-based workflows.\n",
      "\n",
      "  Its use cas-\n",
      "es emphasize prompt optimization and context-aware information retrieval/generation; however, \n",
      "with its Pythonic highly modular interface and its huge collection of tools, it is the number-one \n",
      "tool to implement complex business logic.\n",
      "\n",
      "  SuperAGI has similar features to LangChain.\n",
      "  For \n",
      "example, Dust, written in Rust, focuses on the design of LLM apps and their deployment.\n",
      "\n",
      "Subtopics:\n",
      "  As a consequence, it’s often caught in logic loops and regularly repeats \n",
      "steps.\n",
      "  Investing in education pays dividends when delivering capable, \n",
      "responsible applications.\n",
      "\n",
      "  It provides developers with \n",
      "modular, reusable building blocks like chains for composing pipelines and agents for goal-oriented \n",
      "interactions.\n",
      "  I’ve also omitted a few libraries that concentrate on prompt engineering, for example, \n",
      "Promptify.\n",
      "\n",
      "  Its key innovation is enabling customizable conversational agents that automate coor-\n",
      "dination between different LLMs, humans, and tools via automated chat.\n",
      "  These building blocks fit together as LLM apps that come with extended capabilities.\n",
      "\n",
      "  It even comes with a marketplace, a repository for \n",
      "tools and agents.\n",
      "----------------------------------------\n",
      "Page 85:\n",
      "Topics:\n",
      "  What kind of tools are available in LangChain?\n",
      "10.\t \n",
      "  9.\t\n",
      "\n",
      "  In the next chapters, we’ll build on these LangChain fundamentals to create capable, real-world \n",
      "applications.\n",
      "  In other words, LangChain reduces complex \n",
      "orchestration into customizable building blocks.\n",
      "\n",
      "  In the next chapter, we’ll \n",
      "implement our first apps with Langchain!\n",
      "\n",
      "  What are the limitations of LLMs?\n",
      "2.\t \n",
      "  What are LLM applications?\n",
      "4.\t\n",
      "What is LangChain and why should you use it?\n",
      "5.\t\n",
      "What are LangChain’s key features?\n",
      "6.\t\n",
      "\n",
      "  What is a chain in LangChain?\n",
      "7.\t\n",
      "\n",
      "  LangChain for LLM Apps\n",
      "62\n",
      "As we saw in this chapter, chains allow sequencing calls to LLMs, databases, APIs, and more to \n",
      "accomplish multi-step workflows.\n",
      "  What are stochastic parrots?\n",
      "3.\t\n",
      "\n",
      "  What is an agent?\n",
      "8.\t \n",
      "  I’d recommend you go back to the \n",
      "corresponding sections of this chapter if you are unsure about any of them:\n",
      "1.\t\n",
      "\n",
      "  By leveraging LangChain’s capabilities, developers can unlock \n",
      "the full potential of LLMs to power the next generation of AI software.\n",
      "  How does LangChain work?\n",
      "\n",
      "Subtopics:\n",
      "  Memory persists information across executions to maintain \n",
      "state.\n",
      "  Agents leverage chains to take actions based on observations \n",
      "for managing dynamic applications.\n",
      "  We’ll implement conversational agents combining LLMs with knowledge bases and \n",
      "advanced reasoning algorithms.\n",
      "  What is memory and why do we need it?\n",
      "\n",
      "  Questions\n",
      "Please see if you can come up with answers to these questions.\n",
      "  Together, these concepts enable developers to overcome the limitations of individual LLMs \n",
      "by integrating external data, actions, and context.\n",
      "----------------------------------------\n",
      "Page 86:\n",
      "Topics:\n",
      "  Chapter 2\n",
      "63\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 87:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 88:\n",
      "Topics:\n",
      "  Therefore, in \n",
      "this chapter, we’ll give basic setup instructions for all the libraries needed with the most common \n",
      "dependency management tools such as Docker, Conda, pip, and Poetry.\n",
      "  This will give us a bit more context around using LangChain, and we can \n",
      "introduce tips and tricks for using it effectively.\n",
      "\n",
      "  How to set up the dependencies for this book\n",
      "We’ll assume at least a basic familiarity with Python, Jupyter, and environments in this book, but \n",
      "let’s quickly walk through this together.\n",
      "  The main sections are as follows:\n",
      "•\t\n",
      "How to set up the dependencies for this book\n",
      "•\t\n",
      "Model integrations\n",
      "•\t\n",
      "Building an application for customer service\n",
      "\n",
      "  In the end, as a practical example, we’ll go through an example of a real-world application, an \n",
      "LLM app that could help customer service agents, one of the main areas where LLMs could prove \n",
      "to be game-changing.\n",
      "  For each of them, we will show how to get an API key token.\n",
      "\n",
      "  3\n",
      "Getting Started with LangChain\n",
      "In this book, we’ll write a lot of code and test many different integrations and tools.\n",
      "  Next, we’ll go through model integrations that we can use such as OpenAI’s ChatGPT, models on \n",
      "Hugging Face, Jina AI, and others.\n",
      "Subtopics:\n",
      "  You can safely skip this section if you are confident about \n",
      "your setup or if you plan to install libraries separately for each chapter or application.\n",
      "\n",
      "  Further, we’ll introduce, set up, and work with a few providers \n",
      "in turn.\n",
      "  This will ensure that you \n",
      "can run all the practical examples in this book.\n",
      "\n",
      "  We’ll start the chapter by setting up the environment for the book on our computer.\n",
      "\n",
      "----------------------------------------\n",
      "Page 89:\n",
      "Topics:\n",
      "  If you use Docker, Conda, or Poetry, an appropriate Py-\n",
      "thon version should be installed automatically as part of the instructions.\n",
      "  Intuitive interface\n",
      "Robust dependency resolution\n",
      "Built-in virtual environment management\n",
      "Lock files and version control\n",
      "Less common than Pip or Conda\n",
      "Limited non-Python \n",
      "dependency management\n",
      "Conda\n",
      "Manages Python and non-Python dependencies\n",
      "Handles complex dependency trees\n",
      "Supports multiple Python versions\n",
      "Built-in virtual environment management\n",
      "Slower than native package \n",
      "managers\n",
      "Large disk usage\n",
      "Docker\n",
      "Provides fully isolated and reproducible \n",
      "environments\n",
      "Easily shared and distributed\n",
      "Guaranteed consistency across systems\n",
      "Additional platform knowledge \n",
      "required\n",
      "Larger disk usage\n",
      "Slower startup times\n",
      "Table 3.1: Comparison of tools for managing dependencies\n",
      "For developers, Docker, which provides isolation via containers, is a good option.\n",
      "  This table gives \n",
      "an overview of these options for managing dependencies:\n",
      "Tool\n",
      "Pros\n",
      "Cons\n",
      "pip\n",
      "Default Python package manager\n",
      "Simple commands to install packages\n",
      "requirements.txt for tracking dependencies\n",
      "Can’t install non-Python system \n",
      "dependencies\n",
      "No built-in virtual environment \n",
      "management (see venv or other \n",
      "tools)\n",
      "Limited dependency resolution\n",
      "Poetry\n",
      "\n",
      "  Please make sure you have Python version 3.10 or higher installed.\n",
      "  Getting Started with LangChain\n",
      "66\n",
      "\n",
      "  For data scientists, \n",
      "I’d recommend Conda or Poetry. \n",
      "\n",
      "  Environment management tools like Docker, Conda, Pip, and Poetry help create reproducible \n",
      "Python environments for projects.\n",
      "  You should also install \n",
      "Jupyter Notebook or JupyterLab to run the example notebooks interactively.\n",
      "\n",
      "Subtopics:\n",
      "  The downside \n",
      "is that it uses a lot of disk space and is more complex than the other options.\n",
      "  They install dependencies and isolate projects.\n",
      "  You can install it from python.\n",
      "org or your platform’s package manager.\n",
      "----------------------------------------\n",
      "Page 90:\n",
      "Topics:\n",
      "  Chapter 3\n",
      "67\n",
      "Conda handles intricate dependencies efficiently, although it can be excruciatingly slow in large \n",
      "environments.\n",
      "  Let’s go from the simplest to the most complex.\n",
      "  The different installations have been tested \n",
      "at the time of the release of this book; however, things can change, and we will update the GitHub \n",
      "README online to include workarounds for potential problems that could arise.\n",
      "\n",
      "  This includes these files:\n",
      "•\t\n",
      "requirements.txt for pip\n",
      "•\t\n",
      "pyproject.toml for Poetry\n",
      "•\t\n",
      "langchain_ai.yaml for Conda\n",
      "•\t\n",
      "Dockerfile for Docker\n",
      "Depending on whether system dependencies are managed, they can require additional tweaks \n",
      "with more setup, as in the case with pip and poetry.\n",
      "  To use Pip:\n",
      "1.\t\n",
      "If it’s not already included in your Python distribution, install pip following the instruc-\n",
      "tions here: https://pip.pypa.io/.\n",
      "\n",
      "  For all instructions, please make sure you have the book’s repository \n",
      "downloaded (using the GitHub user interface) or cloned on your computer, and you’ve changed \n",
      "into the project’s root directory.\n",
      "\n",
      "  If you encounter issues during the installation process, consult the respective documentation or \n",
      "raise an issue on the GitHub repository of this book.\n",
      "  My preference is Conda because it strikes \n",
      "the right balance for me of complexity versus isolation.\n",
      "\n",
      "Subtopics:\n",
      "  As mentioned, we won’t spend much time on installation but rather breeze through each of \n",
      "the different tools in turn.\n",
      "  You can find a set \n",
      "of instructions and the corresponding configuration files in the book’s repository at https://\n",
      "github.com/benman1/generative_ai_with_langchain.\n",
      "\n",
      "  For each tool, the key steps are installing the tool, using the configuration file from the repository, \n",
      "and activating the environment.\n",
      "  This sets up a reproducible environment to run all the examples \n",
      "in the book (with very few exceptions, which will be noted).\n",
      "\n",
      "  We’ll start with pip!\n",
      "pip\n",
      "pip is the default Python package manager.\n",
      "  All tools allow sharing and replicating dependencies from configuration files.\n",
      "  Poetry resolves dependencies well and managed environments; however, it doesn’t \n",
      "capture system dependencies.\n",
      "\n",
      "----------------------------------------\n",
      "Page 91:\n",
      "Topics:\n",
      "  To use Conda:\n",
      "1.\t\n",
      "Install Miniconda or Anaconda following the instructions from this link: https://docs.\n",
      "continuum.io/anaconda/install/.\n",
      "2.\t\n",
      "Create the environment from langchain_ai.yml:\n",
      "conda env create --file langchain_ai.yaml\n",
      "3.\t\n",
      "Activate the environment:\n",
      "conda activate langchain_ai\n",
      "Docker\n",
      "Docker provides isolated, reproducible environments using containers.\n",
      "  3.\t\n",
      "Install the dependencies from requirements.txt:\n",
      "pip install -r requirements.txt\n",
      "Poetry\n",
      "Poetry is relatively new, but is popular with Python developers and data scientists because of its \n",
      "convenience.\n",
      "  To use Docker:\n",
      "1.\t\n",
      "Install Docker Engine; follow the installation instructions here: https://docs.docker.\n",
      "com/get-docker/.\n",
      "2.\t\n",
      "Build the Docker image from the Dockerfile in this repository:\n",
      "docker build -t langchain_ai \n",
      "3.\t\n",
      "Run the Docker container interactively:\n",
      "docker run -it langchain_ai \n",
      "Let’s move on and see some of the models that you can use with LangChain!\n",
      "\n",
      "  Conda\n",
      "Conda manages Python environments and dependencies.\n",
      "  Getting Started with LangChain\n",
      "68\n",
      "2.\t\n",
      "Use a virtual environment for isolation (for example, venv).\n",
      "\n",
      "  To use Poetry:\n",
      "1.\t\n",
      "Install poetry by following the instructions at https://python-poetry.org/.\n",
      "2.\t\n",
      "\n",
      "Subtopics:\n",
      "  Run poetry install to install the dependencies.\n",
      "\n",
      "  There are many cloud providers of models, where you can use the model through an interface; \n",
      "other sources allow you to download a model to your computer. \n",
      "\n",
      "  It manages dependencies and virtual environments.\n",
      "----------------------------------------\n",
      "Page 92:\n",
      "Topics:\n",
      "  As discussed in Chapter 1, \n",
      "What Is Generative AI?, there are various LLMs by tech giants, like GPT-4 by OpenAI, BERT and \n",
      "PaLM-2 by Google, LLaMA by Meta, and many more.\n",
      "\n",
      "  For LLMs, OpenAI, Hugging Face, Cohere, Anthropic, Azure, Google Cloud Platform’s Vertex AI \n",
      "(PaLM-2), and Jina AI are among the many providers supported in LangChain; however, this \n",
      "list is growing all the time.\n",
      "  Chapter 3\n",
      "69\n",
      "With the help of LangChain, we can interact with all of these – for example, through Application \n",
      "Programming Interface (APIs), or we can call models that we have downloaded on our computer. \n",
      "\n",
      "  Exploring API model integrations\n",
      "Before properly starting with generative AI, we need to set up access to models such as LLMs or \n",
      "text-to-image models so we can integrate them into our applications.\n",
      "  Here’s a screenshot of this page as of the time of writing (October 2023), which includes both \n",
      "cloud providers and interfaces for local models:\n",
      "Figure 3.1: LLM integrations in LangChain\n",
      "\n",
      "Subtopics:\n",
      "  You can check out the full list of supported integrations for LLMs at \n",
      "https://integrations.langchain.com/llms.\n",
      "\n",
      "  Let’s start with models accessed through APIs with cloud providers.\n",
      "\n",
      "----------------------------------------\n",
      "Page 93:\n",
      "Topics:\n",
      "  In Linux and \n",
      "macOS, you can set a system environment variable from the terminal using the export command:\n",
      "export OPENAI_API_KEY=<your token>\n",
      "\n",
      "  LangChain currently doesn’t have out-of-the-box handling \n",
      "of models that are not for text; however, its documentation describe how to work with Replicate, \n",
      "which also provides an interface to Stable Diffusion models.\n",
      "\n",
      "  For each of these providers, to make calls against their API, you’ll first need to create an account \n",
      "and obtain an API key.\n",
      "  To set an API key in an environment, in Python, we can execute the following lines:\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"<your token>\"\n",
      "Here, OPENAI_API_KEY is the environment key that is appropriate for OpenAI.\n",
      "  As for image models, the big developers include OpenAI (DALL-E), Midjourney, Inc. (Midjourney), \n",
      "and Stability AI (Stable Diffusion).\n",
      "  Suffice it to say here \n",
      "that these embeddings are a way to capture and extract information from the input text.\n",
      "  Getting Started with LangChain\n",
      "70\n",
      "LangChain implements three different interfaces – we can use chat models, LLMs, and embedding \n",
      "models.\n",
      "  We’ll focus on text generation in this chapter, and discuss embeddings, vector \n",
      "databases, and neural search in Chapter 5, Building a Chatbot Like ChatGPT.\n",
      "Subtopics:\n",
      "  Embedding models are listed at https://python.langchain.com/\n",
      "docs/integrations/text_embedding.\n",
      "\n",
      "  They are commonly used in chatbot applications where conversations are ex-\n",
      "changed.\n",
      "  Finally, text embedding models are used to convert text inputs into numerical representations \n",
      "called embeddings.\n",
      "  You can find chat models at https://python.langchain.com/docs/integrations/chat.\n",
      "\n",
      "  You can also expose these variables in your system environment from your terminal.\n",
      "  Setting the keys in \n",
      "your environment has the advantage of not needing to include them as parameters in your code \n",
      "every time you use a model or service integration.\n",
      "\n",
      "  Chat \n",
      "models are specifically designed to handle a list of chat messages as input and generate a chat \n",
      "message as output.\n",
      "  However, there are some differences in the types of input and output they handle.\n",
      "  Chat models and LLMs are similar in that they both process text input and produce text \n",
      "output.\n",
      "  They \n",
      "are widely used in natural language processing tasks like sentiment analysis, text classification, \n",
      "and information retrieval.\n",
      "  This is free of charge for all providers and, with some of them, you don’t \n",
      "even have to give them your credit card details.\n",
      "\n",
      "----------------------------------------\n",
      "Page 94:\n",
      "Topics:\n",
      "  Chapter 3\n",
      "71\n",
      "To permanently set the environment variable in Linux or macOS, you would need to add the \n",
      "preceding line to the ~/.bashrc or ~/.bash_profile file, respectively, and then reload the shell \n",
      "using the command source ~/.bashrc or source ~/.bash_profile.\n",
      "\n",
      "  This is on purpose (in fact, I’ve disabled \n",
      "the tracking of this file in Git) since I don’t want to share my keys with other people for security \n",
      "reasons (and because I don’t want to pay for anyone else’s usage).\n",
      "\n",
      "  In Windows, you can set a system environment variable from the command prompt using the \n",
      "set command:\n",
      "set OPENAI_API_KEY=<your token>\n",
      "To permanently set the environment variable in Windows, you can add the preceding line to a \n",
      "batch script.\n",
      "\n",
      "  If you look for this \n",
      "file in the Github repository, you’ll notice that it is missing.\n",
      "Subtopics:\n",
      "  Anytime you want to run an application, you import \n",
      "the function and run it like so:\n",
      "from config import set_environment\n",
      "set_environment()\n",
      "\n",
      "  I then import a \n",
      "function from this module that will load all these keys into the environment.\n",
      "  This function, set_environment(), loads all the \n",
      "keys into the environment as mentioned.\n",
      "  = value\n",
      "You can set all your keys in the config.py file.\n",
      "  = \"... \"\n",
      "# I'm omitting all other keys\n",
      "def set_environment():\n",
      "    variable_dict = globals().items()\n",
      "    for key, value in variable_dict:\n",
      "        if \"API\" in key or \"ID\" in key:\n",
      "            os.environ[key]\n",
      "  My personal choice is to create a config.py file, where all the keys are stored.\n",
      "  My config.py looks like this:\n",
      "import os\n",
      "OPENAI_API_KEY\n",
      "----------------------------------------\n",
      "Page 95:\n",
      "Topics:\n",
      "  The fake LLM is only for testing purposes.\n",
      "  Fake LLM\n",
      "The fake LLM allows you to simulate LLM responses during testing without needing actual API \n",
      "calls.\n",
      "  Overall, it enables fast agent iteration without needing a real LLM.\n",
      "\n",
      "  [\"Action: Python_REPL\\nAction Input: print(2 + 2)\", \"Final \n",
      "Answer: 4\"]\n",
      "llm = FakeListLLM(responses=responses)\n",
      "agent = initialize_agent(\n",
      "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
      ")\n",
      "agent.run(\"whats 2 + 2\")\n",
      "\n",
      "  This is a bit more complex than the previous example but gives a hint of the \n",
      "capabilities we have at our fingertips:\n",
      "from langchain.llms.fake import FakeListLLM\n",
      "from langchain.agents import load_tools\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "tools = load_tools([\"python_repl\"])\n",
      "responses =\n",
      "  The LangChain documentation has an example of \n",
      "tool use with LLMs.\n",
      "  We set up an agent that makes decisions based on the React strategy that we explained in Chapter \n",
      "2, LangChain for LLM Apps (ZERO_SHOT_REACT_DESCRIPTION).\n",
      "  We run the agent with a text: the \n",
      "question what's 2 + 2.\n",
      "\n",
      "  Getting Started with LangChain\n",
      "72\n",
      "Now, let’s go through a few prominent model providers in turn.\n",
      "  This will help \n",
      "to illustrate the general idea of calling language models in LangChain.\n",
      "\n",
      "  Let’s start with a fake LLM that we can use for testing purposes.\n",
      "Subtopics:\n",
      "  Using the FakeLLM avoids hit-\n",
      "ting rate limits during testing.\n",
      "  We’ll give an example of usage \n",
      "for each of them.\n",
      "  This is useful for rapid prototyping and unit testing agents.\n",
      "  You can execute this example in either Python directly or in a notebook.\n",
      "\n",
      "  For example, you could initialize a FakeLLM that returns \"Hello\" as follows:\n",
      "from langchain.llms import FakeLLM\n",
      "fake_llm = FakeLLM(responses=[\"Hello\"])\n",
      "\n",
      "  It also allows you to mock various responses to validate that your \n",
      "agent handles them properly.\n",
      "----------------------------------------\n",
      "Page 96:\n",
      "Topics:\n",
      "  OpenAI\n",
      "As explained in Chapter 1, What Is Generative AI?, OpenAI is an American AI research laboratory \n",
      "that is the current market leader in generative AI models, especially LLMs.\n",
      "  \"\"\"\n",
      "    name = \"Python_REPL\"\n",
      "    description = (\n",
      "        \"A Python shell.\n",
      "  \"\n",
      "    )\n",
      "As you can see in the preceding code block, the names and descriptions of the tools are passed \n",
      "to the LLM, which then decides an action based on the information provided.\n",
      "  We can also observe how the fake LLM output leads to a call to the Python interpreter, which \n",
      "returns 4.\n",
      "  Obviously, if we change the second response to \"Final Answer: 5\", the output \n",
      "of the agent wouldn’t correspond to the question.\n",
      "\n",
      "  Chapter 3\n",
      "73\n",
      "As you can see, we connect a tool, a Python Read-Eval-Print Loop (REPL), that will be called de-\n",
      "pending on the output of the LLM.\n",
      "  One of the first providers anyone will think of is OpenAI.\n",
      "\n",
      "  FakeListLLM will give two responses (\"Action: Python_REPL\\\n",
      "nAction Input: print(2 + 2)\" and \"Final Answer: 4\") that won’t change based on the input.\n",
      "\n",
      "  The output of the Python interpreter is passed to the fake LLM, which ignores the observation \n",
      "and returns 4.\n",
      "  We’ll see, in this chapter, how to \n",
      "interact with OpenAI models with the LangChain and the OpenAI Python client libraries.\n",
      "  Please note that the action must match the name attribute of the tool, PythonREPLTool, \n",
      "which starts like this:\n",
      "class PythonREPLTool(BaseTool):\n",
      "    \"\"\"A tool for running python code in a REPL.\n",
      "  In the next sections, we’ll make our example more meaningful by using an actual LLM rather \n",
      "than a fake one.\n",
      "Subtopics:\n",
      "  \"\n",
      "        \"If you want to see the output of a value, you should print it out \n",
      "\"\n",
      "        \"with `print(...)`.\n",
      "  They offer a range of \n",
      "models with various levels of power suitable for different tasks.\n",
      "  The action can be \n",
      "executing a tool or planning.\n",
      "\n",
      "  \"\n",
      "        \"Input should be a valid python command.\n",
      "  Use this to execute python commands.\n",
      "  OpenAI \n",
      "also offers an Embedding class for text embedding models.\n",
      "\n",
      "----------------------------------------\n",
      "Page 97:\n",
      "Topics:\n",
      "  To create an API key, follow these steps:\n",
      "1.\t\n",
      "You need to create a login at https://platform.openai.com/.\n",
      "2.\t\n",
      "Set up your billing information.\n",
      "\n",
      "  Getting Started with LangChain\n",
      "74\n",
      "\n",
      "  4.\t\n",
      "Click on Create new secret key and give it a name.\n",
      "\n",
      "  We will use OpenAI for our applications but will also try LLMs from other organizations.\n",
      "  Here’s how this should look on the OpenAI platform:\n",
      "Figure 3.2: OpenAI API platform – Create new secret key\n",
      "After clicking Create secret key, you should see the message API key generated.\n",
      "  When using commercial LLMs like GPT-3 and GPT-4 via APIs, each token has an associated cost \n",
      "based on factors like the LLM model and API pricing tiers.\n",
      "  3.\t\n",
      "You can see the API keys under Personal | View API Keys.\n",
      "\n",
      "  We need to obtain an OpenAI API key first.\n",
      "  We can set the key as an environment variable (OPENAI_API_\n",
      "KEY) or pass it as a parameter every time you construct a class for OpenAI calls.\n",
      "\n",
      "Subtopics:\n",
      "  You need to copy \n",
      "the key to your clipboard and keep it.\n",
      "  When you \n",
      "send a prompt to an LLM API, it processes the prompt word by word, breaking down (tokenizing) \n",
      "the text into individual tokens.\n",
      "  Strategies like using \n",
      "smaller models, summarizing outputs, and preprocessing inputs help reduce the tokens required \n",
      "to get useful results.\n",
      "  Being aware of token usage is key for optimizing productivity within budget \n",
      "constraints when leveraging commercial LLMs.\n",
      "\n",
      "  Token usage refers to how many to-\n",
      "kens from the model’s quota have been consumed to generate a response.\n",
      "  The number of tokens directly correlates with the amount of text.\n",
      "\n",
      "----------------------------------------\n",
      "Page 98:\n",
      "Topics:\n",
      "  The company is an American company that develops tools for build-\n",
      "ing machine learning applications.\n",
      "  Hugging Face\n",
      "Hugging Face is a very prominent player in the NLP space and has considerable traction in open-\n",
      "source and hosting solutions.\n",
      "  But for now, let’s move on to the next provider \n",
      "and more examples!\n",
      "\n",
      "  '4 + 4 = 8'\n",
      "The agent comes up with the right solution.\n",
      "  Its employees develop and maintain the Transformers Python \n",
      "library, which is used for NLP tasks, includes implementations of state-of-the-art and popular \n",
      "models like Mistral 7B, BERT, and GPT-2, and is compatible with PyTorch, TensorFlow, and JAX.\n",
      "\n",
      "  Chapter 3\n",
      "75\n",
      "We can use the OpenAI language model class to set up an LLM to interact with.\n",
      "  Let’s create an \n",
      "agent that calculates using this model – I am omitting the imports from the previous example:\n",
      "from langchain.llms import OpenAI\n",
      "llm = OpenAI(temperature=0., model=\"text-davinci-003\")\n",
      "agent = initialize_agent(\n",
      "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
      ")\n",
      "agent.run(\"whats 4 + 4\")\n",
      "We should be seeing this output:\n",
      ">\n",
      "  Entering new  chain...\n",
      " I need to add two numbers\n",
      "Action: Python_REPL\n",
      "Action Input: print(4 + 4)\n",
      "Observation: 8\n",
      "Thought: I now know the final answer\n",
      "Final Answer: 4 + 4 = 8\n",
      "> Finished chain.\n",
      "\n",
      "Subtopics:\n",
      "  During the course of this book, we’ll try to come \n",
      "up with solutions to more complex problems.\n",
      "  It’s a simple problem, but I still find it fascinating to \n",
      "be able to put my question in natural language.\n",
      "----------------------------------------\n",
      "Page 99:\n",
      "Topics:\n",
      "  The HuggingFaceHub integration, for example, provides access to different models for tasks like \n",
      "text generation and text classification.\n",
      "  In addition to their products, Hugging Face has been involved in initiatives such as the BigScience \n",
      "Research Workshop, where they released an open LLM called BLOOM with 176 billion parame-\n",
      "ters.\n",
      "  Additionally, you can make the token available \n",
      "in your environment as HUGGINGFACEHUB_API_TOKEN.\n",
      "\n",
      "  These tools allow users to load and use models, embeddings, and datasets from Hugging Face. \n",
      "\n",
      "  We get the response \"japan\".\n",
      "\n",
      "  Hugging Face has \n",
      "also formed partnerships with companies like Graphcore and Amazon Web Services to optimize \n",
      "their offerings and make them available to a broader customer base.\n",
      "\n",
      "  Let’s see an example, where we use an open-source model developed by Google, the Flan-T5-XXL \n",
      "model:\n",
      "from langchain.llms import HuggingFaceHub\n",
      "llm = HuggingFaceHub(\n",
      "    model_kwargs={\"temperature\":\n",
      "  0.5, \"max_length\": 64},\n",
      "    repo_id=\"google/flan-t5-xxl\"\n",
      ")\n",
      "prompt = \"In which country is Tokyo?\"\n",
      "completion = llm(prompt)\n",
      "print(completion)\n",
      "\n",
      "  Hugging Face offer various other libraries within their ecosystem, including Datasets for dataset \n",
      "processing, Evaluate for model evaluation, Simulate for simulation, and Gradio for machine \n",
      "learning demos.\n",
      "\n",
      "  To use Hugging Face as a provider for your models, you can create an account and API keys at \n",
      "https://huggingface.co/settings/profile.\n",
      "  They have received significant funding, including a $40 million Series B round and a recent \n",
      "Series C funding round led by Coatue and Sequoia at a $2 billion valuation.\n",
      "  Getting Started with LangChain\n",
      "76\n",
      "Hugging Face also provides the Hugging Face Hub, a platform for hosting Git-based code re-\n",
      "positories, machine learning models, datasets, and web applications, which provides over 120k \n",
      "models, 20k datasets, and 50k demo apps (spaces) for machine learning.\n",
      "Subtopics:\n",
      "  It is an online platform \n",
      "where people can collaborate and facilitate machine learning development.\n",
      "\n",
      "  The HuggingFaceEmbeddings integration allows users to \n",
      "work with sentence-transformer models.\n",
      "\n",
      "----------------------------------------\n",
      "Page 100:\n",
      "Topics:\n",
      "  GCP provides access to LLMs like LaMDA, T5, and PaLM. \n",
      "\n",
      "  You have different options for this:\n",
      "•\t\n",
      "Using gcloud config set project my-project \n",
      "•\t\n",
      "Passing a constructor argument when initializing the LLM\n",
      "•\t\n",
      "Using aiplatform.init()\n",
      "•\t\n",
      "Setting a GCP environment variable\n",
      "I found all these options work fine.\n",
      "  For models with GCP, you need to have the gcloud command-line interface (CLI) installed.\n",
      "  The NL API’s improved v2 \n",
      "classification model is enhanced with over 1,000 labels and supports 11 languages with improved \n",
      "accuracy (it is unclear, however, which model is used under the hood).\n",
      "\n",
      "  Google has also updated the Google Cloud Natural Language (NL) API with a new LLM-based \n",
      "model for Content Classification.\n",
      "  The GCP environment variable works well with the config.py file that I men-\n",
      "tioned earlier.\n",
      "  Google Cloud Platform\n",
      "There are many models and functions available through Google Cloud Platform (GCP) and Vertex \n",
      "AI, GCP’s machine learning platform.\n",
      "  To enable Vertex AI, install the Google Vertex \n",
      "AI SDK with the pip install google-cloud-aiplatform command.\n",
      "  If you’ve followed the in-\n",
      "structions on GitHub as indicated in the previous section, you should already have this installed.\n",
      "\n",
      "  Chapter 3\n",
      "77\n",
      "The LLM takes a text input, a question in this case, and returns a completion.\n",
      "  You can find more details about these options in the Vertex \n",
      "documentation.\n",
      "  You \n",
      "can find the instructions here: https://cloud.google.com/sdk/docs/install.\n",
      "\n",
      "Subtopics:\n",
      "  The model has a \n",
      "lot of knowledge and can come up with answers to knowledge questions.\n",
      "\n",
      "  Please \n",
      "make sure you set the project ID before you move on.\n",
      "\n",
      "  If you haven’t enabled it, you should get a helpful error message pointing you to the right website, \n",
      "where you click Enable.\n",
      "\n",
      "  I found the gcloud command very convenient though, so I went with this.\n",
      "  Then we have to set up the Google Cloud project ID.\n",
      "  This updated version offers an expansive pre-trained classifi-\n",
      "cation taxonomy to help with ad targeting and content-based filtering.\n",
      "  You can then authenticate and print a key token with this command from the terminal:\n",
      "gcloud auth application-default login\n",
      "You also need to enable Vertex AI for your project.\n",
      "----------------------------------------\n",
      "Page 101:\n",
      "Topics:\n",
      "  Getting Started with LangChain\n",
      "78\n",
      "Let’s run a model!\n",
      "\n",
      "  •\t\n",
      "codechat-bison is a chatbot that is fine-tuned to help with code-related questions.\n",
      "  •\t\n",
      "code-bison generates code from natural language descriptions, with a max input of 4,096 \n",
      "tokens and an output of 2,048 tokens.\n",
      "\n",
      "  It has \n",
      "an input limit of 4,096 tokens and an output limit of 2,048 tokens.\n",
      "\n",
      "  •\t\n",
      "chat-bison is optimized for multi-turn conversation with a max input of 4,096 tokens, \n",
      "an output of 1,024 tokens, and up to 2,500 turns.\n",
      "\n",
      "  \"\"\"\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
      "llm = VertexAI()\n",
      "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
      "question = \"What NFL team won the Super Bowl in the year Justin Beiber was \n",
      "born?\"\n",
      "llm_chain.run(question)\n",
      "We should see this response:\n",
      "\n",
      "  [[Question: What NFL team won the Super Bowl in the year Justin Beiber was \n",
      "born?\n",
      "\n",
      "  Answer: Let's think step by step.[0m\n",
      "\n",
      "  The Super Bowl in 1994 was won by \n",
      "the San Francisco 49ers.\n",
      "\n",
      "  [1m> Finished chain.[0m\n",
      "Justin Beiber was born on March 1, 1994.\n",
      "  from langchain.llms import VertexAI\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "template = \"\"\"Question: {question}\n",
      "Answer: Let's think step by step.\n",
      "  [1m> Entering new chain...[0m\n",
      "Prompt after formatting:\n",
      "\n",
      "  Vertex AI offers a range of models tailored for tasks like following instructions, conversation, and \n",
      "code generation/assistance:\n",
      "•\t\n",
      "text-bison is fine-tuned to follow natural language instructions, with a max input of 8,192 \n",
      "tokens and an output of 1,024.\n",
      "\n",
      "Subtopics:\n",
      "  It’s quite impressive that it produces \n",
      "the right response even given a misspelling of the name.\n",
      "  The step-by-step prompt instruction is \n",
      "key to the correct answer.\n",
      "\n",
      "  I’ve set verbose to True to see the model’s reasoning process.\n",
      "----------------------------------------\n",
      "Page 102:\n",
      "Topics:\n",
      "  It has a max input length of 2,048 tokens and \n",
      "an output of 64 tokens.\n",
      "\n",
      "  answer.append(\"FizzBuzz\")\n",
      "    elif i % 3 == 0:\n",
      "        answer.append(\"Fizz\")\n",
      "    \n",
      "  For more detailed and up-to-date information about models, including when models have been \n",
      "updated, you can check out the documentation at https://cloud.google.com/vertex-ai/docs/\n",
      "generative-ai/learn/overview.\n",
      "\n",
      "  elif i % 5 == 0:\n",
      "        answer.append(\"Buzz\")\n",
      "    else:\n",
      "        answer.append(str(i))\n",
      "return answer\n",
      "```\n",
      "\n",
      "  Let’s see if the code-bison model can solve FizzBuzz, a common in-\n",
      "terview question for entry-level software developer positions:\n",
      "question = \"\"\"\n",
      "Given an integer n, return a string array answer (1-indexed) where:\n",
      "answer[i] == \"FizzBuzz\" if i is divisible by 3 and 5.\n",
      "answer[i] == \"Fizz\" if i is divisible by 3.\n",
      "answer[i] == \"Buzz\" if i is divisible by 5.\n",
      "answer[i] == i (as a string) if none of the above conditions are true.\n",
      "\n",
      "  []\n",
      "for i in range(1, n + 1):\n",
      "    if i % 3 == 0\n",
      "  Chapter 3\n",
      "79\n",
      "•\t\n",
      "code-gecko suggests code completions.\n",
      "  and i % 5 == 0:\n",
      "        \n",
      "Subtopics:\n",
      "  We are getting this response:\n",
      "```python\n",
      "answer =\n",
      "  \"\"\"\n",
      "llm = VertexAI(model_name=\"code-bison\")\n",
      "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
      "print(llm_chain.run(question))\n",
      "\n",
      "  These models also have different input/output limits and training data and are often updated. \n",
      "\n",
      "  We can also generate code.\n",
      "----------------------------------------\n",
      "Page 103:\n",
      "Topics:\n",
      "  Recently, Jina AI launched Finetuner, a tool that enables the fine-tuning \n",
      "of any deep neural network to specific use cases and requirements.\n",
      "\n",
      "  Notable investors in Jina AI include GGV \n",
      "Capital and Canaan Partners.\n",
      "\n",
      "  Here, we are setting up a Visual Question Answering API with the recommended model:\n",
      "Figure 3.3: Visual Question Answering API in Jina AI\n",
      "\n",
      "  Jina AI\n",
      "Jina AI, founded in February 2020 by Han Xiao and Xuanbin He, is a German AI company based \n",
      "in Berlin that specializes in providing cloud-native neural search solutions with models for text, \n",
      "image, audio, and video.\n",
      "  Getting Started with LangChain\n",
      "80\n",
      "Would you hire code-bison for your team?\n",
      "\n",
      "  On the platform, we can set up APIs for different use cases such as image caption, text embedding, \n",
      "image embedding, visual question answering, visual reasoning, image upscale, or Chinese text \n",
      "embedding.\n",
      "\n",
      "  The company raised $37.5 million in funding through three rounds, with their most recent fund-\n",
      "ing coming from a Series A round in November 2021.\n",
      "Subtopics:\n",
      "  Their open-source neural search ecosystem enables businesses and de-\n",
      "velopers to easily build scalable and highly available neural search solutions, allowing for efficient \n",
      "information retrieval.\n",
      "  You can set up a login at https://chat.jina.ai/api.\n",
      "\n",
      "----------------------------------------\n",
      "Page 104:\n",
      "Topics:\n",
      "  This is cool, but unfortunately, these APIs are not available yet through LangChain.\n",
      "  Chapter 3\n",
      "81\n",
      "We get examples for client calls in Python and cURL, and a demo, where we can ask a question. \n",
      "\n",
      "  Let’s ask for some food recommendations:\n",
      "from langchain.schema import SystemMessage\n",
      "chat = JinaChat(temperature=0.)\n",
      "\n",
      "  [\n",
      "    HumanMessage(\n",
      "        content=\"Translate this sentence from English to French: I love \n",
      "generative AI!\"\n",
      "    )\n",
      "]\n",
      "chat(messages)\n",
      "\n",
      "  Let’s translate from English to French here:\n",
      "from langchain.chat_models import JinaChat\n",
      "from langchain.schema import HumanMessage\n",
      "chat = JinaChat(temperature=0.)\n",
      "\n",
      "  Let’s set up another chatbot, this time powered by Jina AI.\n",
      "  We can generate the API token, which \n",
      "we can set as JINACHAT_API_KEY, at https://chat.jina.ai/api.\n",
      "\n",
      "  We should be seeing :\n",
      "AIMessage(content=\"J'adore l'IA générative !\", additional_kwargs={}, \n",
      "example=False).\n",
      "\n",
      "  We can im-\n",
      "plement such calls ourselves by subclassing the LLM class in LangChain as a custom LLM interface.\n",
      "\n",
      "Subtopics:\n",
      "  messages =\n",
      "  \"\n",
      "        ),\n",
      "\n",
      "  We are starting the conversation with a system \n",
      "message clarifying the purpose of the chatbot.\n",
      "\n",
      "  We can set different temperatures, where a low temperature makes the responses more predict-\n",
      "able.\n",
      "  In this case, it makes only a minor difference.\n",
      "  chat(\n",
      "    [\n",
      "        SystemMessage(\n",
      "            content=\"You help a user find a nutritious and tasty food to \n",
      "eat in one word.\n",
      "----------------------------------------\n",
      "Page 105:\n",
      "Topics:\n",
      "  It’s important to understand the difference in LangChain between LLMs and chat models.\n",
      "  \"\n",
      "        )\n",
      "    ]\n",
      ")\n",
      "I get this response in Jupyter – your answer could vary:\n",
      "AIMessage(content='A tasty and nutritious option could be a vegetable \n",
      "pasta dish.\n",
      "  Getting Started with LangChain\n",
      "82\n",
      "        HumanMessage(\n",
      "            content=\"I like pasta with cheese, but I need to eat more \n",
      "vegetables, what should I eat?\n",
      "  Replicate\n",
      "Established in 2019, Replicate Inc. is a San Francisco-based start-up that presents a streamlined \n",
      "process to AI developers, where they can implement and publish AI models with minimal code \n",
      "input through the utilization of cloud technology.\n",
      "  This shared interface allows for interchange-\n",
      "ability between diverse types of models in applications and between chat and LLM models.\n",
      "\n",
      "  The firm, deriving its most recent \n",
      "funding from a Series A funding round of which the invested total was $12.5 million, was spear-\n",
      "headed by Andreessen Horowitz, and involved the participation of Y Combinator, Sequoia, and \n",
      "various independent investors.\n",
      "\n",
      "  Try adding broccoli, spinach, bell peppers, \n",
      "and zucchini to your pasta with some grated parmesan cheese on top.\n",
      "  This \n",
      "way, you get to enjoy your pasta with cheese while incorporating some \n",
      "veggies into your meal.', additional_kwargs={}, example=False)\n",
      "It ignored the one-word instruction, but I liked reading the ideas.\n",
      "Subtopics:\n",
      "  With other chatbots, I got Ratatouille as a suggestion.\n",
      "\n",
      "  I think I should try this for my \n",
      "son.\n",
      "  The platform works with private as well as \n",
      "public models and enables model inference and fine-tuning.\n",
      "  Both LLMs and chat models implement the base language model interface, which includes meth-\n",
      "ods such as predict() and predict_messages().\n",
      "  Depending on your taste, you can choose a sauce that \n",
      "complements the vegetables.\n",
      "  LLMs \n",
      "are text completion models that take a string prompt as input and output a string completion.\n",
      "  They take \n",
      "a list of chat messages as input, labeled with the speaker, and return a chat message as output.\n",
      "\n",
      "  As \n",
      "mentioned, chat models are like LLMs but are specifically designed for conversations.\n",
      "----------------------------------------\n",
      "Page 106:\n",
      "Topics:\n",
      "  image_url = text2image(\"a book cover for a book about creating generative \n",
      "ai applications in Python\")\n",
      "Replicate has lots of models available on their platform: https://replicate.com/\n",
      "explore.\n",
      "\n",
      "  You can authenticate with your GitHub credentials at https://replicate.com/. If you then click \n",
      "on your user icon at the top left, you’ll find the API tokens – just copy the API key and make it \n",
      "available in your environment as REPLICATE_API_TOKEN.\n",
      "  Here is a simple example for creating an image:\n",
      "from langchain.llms import Replicate\n",
      "text2image = Replicate(\n",
      "    model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6\n",
      "dce16bb082a930b0c49861f96d1e5bf\",\n",
      "    input={\"image_dimensions\": \"512x512\"},\n",
      ")\n",
      "\n",
      "  Consequently, they \n",
      "created Cog, an open-source tool that packs machine learning models into a standard produc-\n",
      "tion-ready container that can run on any current operating system and automatically generate an \n",
      "API.\n",
      "  Chapter 3\n",
      "83\n",
      "Ben Firshman, who drove open-source product efforts at Docker, and Andreas Jansson, a former \n",
      "machine learning engineer at Spotify, co-founded Replicate Inc. with the mutual aspiration to \n",
      "eliminate the technical barriers that were hindering the mass acceptance of AI.\n",
      "  These containers can also be deployed on clusters of GPUs through the Replicate platform.\n",
      "Subtopics:\n",
      "  As \n",
      "a result, developers can concentrate on other essential tasks, thereby enhancing their productivity.\n",
      "\n",
      "  To run bigger jobs, you need to set up \n",
      "your credit card (under billing).\n",
      "\n",
      "----------------------------------------\n",
      "Page 107:\n",
      "Topics:\n",
      "  Sadly, as \n",
      "you’ll see, I faced issues with Azure and Anthropic, two major providers.\n",
      "  By authenticating either through GitHub or Microsoft credentials, we can create an account on \n",
      "Azure at https://azure.microsoft.com/.\n",
      "We can then create new API keys under Cognitive Services | Azure OpenAI.\n",
      "  Getting Started with LangChain\n",
      "84\n",
      "\n",
      "  I think it’s a nice image – is that an AI chip that creates art?\n",
      "Others\n",
      "There are a lot more providers, and we’ll encounter quite a few throughout the book.\n",
      "  Azure\n",
      "Azure, the cloud computing platform run by Microsoft, integrates with OpenAI to provide powerful \n",
      "language models like GPT-3, Codex, and Embeddings.\n",
      "  Your mileage might vary – if you are \n",
      "already using Microsoft services, this process could be pain-free for you.\n",
      "\n",
      "  After going through account val-\n",
      "idation a few times, getting denied, and trying to contact Microsoft customer service, I gave up. \n",
      "\n",
      "  I got this image:\n",
      "Figure 3.4: A book cover for a book about generative AI with Python – Stable Diffusion\n",
      "\n",
      "Subtopics:\n",
      "  It offers access, management, and devel-\n",
      "opment of applications and services through its global data centers for use cases such as writing \n",
      "assistance, summarization, code generation, and semantic search.\n",
      "  It provides capabilities like \n",
      "software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS).\n",
      "\n",
      "  For this reason, I don’t have a practical example with Azure.\n",
      "  There are a few more \n",
      "steps involved, and personally, I found this process frustrating.\n",
      "  Let’s still have a quick \n",
      "look at them!\n",
      "\n",
      "----------------------------------------\n",
      "Page 108:\n",
      "Topics:\n",
      "  Anthropic\n",
      "Anthropic is an AI start-up and public-benefit corporation based in the United States.\n",
      "  As of July 2023, Anthropic has raised $1.5 billion in funding.\n",
      "  Please note that we don’t need an API token for local models!\n",
      "\n",
      "  They have also \n",
      "worked on projects like Claude, an AI chatbot like OpenAI’s ChatGPT, and have researched the \n",
      "interpretability of machine learning systems, specifically the Transformer architecture.\n",
      "\n",
      "  These will let you run on machines with a lot of memory and different hardware including \n",
      "Tensor Processing Units (TPUs) or GPUs.\n",
      "\n",
      "  The use cases presented in this section should run even on old hardware, \n",
      "like an old MacBook; however, if you choose a big model, it can take an exceptionally long time \n",
      "to run or may crash the Jupyter notebook.\n",
      "  The company specializes in developing general AI systems and language models with a focus on \n",
      "responsible AI usage.\n",
      "  Next, let’s see how to run models locally.\n",
      "\n",
      "  You need to apply for access to \n",
      "use Claude and set the ANTHROPIC_API_KEY environment variable.\n",
      "\n",
      "  In rough terms, if quantized (roughly, compressed; we’ll discuss quantization in Chapter 8, Cus-\n",
      "tomizing LLMs and Their Output), 1 billion parameters correspond to 1 GB of RAM (please note \n",
      "that not all models will come quantized).\n",
      "\n",
      "  It was found-\n",
      "ed in 2021 by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei. \n",
      "\n",
      "  Unfortunately, Claude is not available to the general public (yet).\n",
      "  Exploring local models\n",
      "We can also run local models from LangChain.\n",
      "  Chapter 3\n",
      "85\n",
      "After setting up, the models should be accessible through the AzureOpenAI() class interface in \n",
      "LangChain.\n",
      "\n",
      "  We’ll have a look here at Hugging Face’s transformers, llama.cpp, and GPT4All.\n",
      "  One of the main bottlenecks is memory requirement. \n",
      "\n",
      "  You can also run these models on hosted resources or services such as Kubernetes or Google Co-\n",
      "lab.\n",
      "  Let’s preface this with a note of caution: an LLM is big, which means that it’ll take up a lot of disk \n",
      "space or system memory.\n",
      "Subtopics:\n",
      "  Let’s start \n",
      "by showing how we can run a model with the transformers library by Hugging Face.\n",
      "\n",
      "  These tools pro-\n",
      "vide huge power and are full of great functionality too broad to cover in this chapter.\n",
      "  The advantages of running models locally are \n",
      "complete control over the model and not sharing any data over the internet.\n",
      "\n",
      "----------------------------------------\n",
      "Page 109:\n",
      "Topics:\n",
      "  This model is quite small (355 million param-\n",
      "eters) but relatively performant and instruction-tuned for conversations.\n",
      "  Getting Started with LangChain\n",
      "86\n",
      "Hugging Face Transformers\n",
      "I’ll quickly show the general recipe for setting up and running a pipeline:\n",
      "from transformers import pipeline\n",
      "import torch\n",
      "generate_text = pipeline(\n",
      "    model=\"aisquared/dlite-v1-355m\",\n",
      "    torch_dtype=torch.bfloat16,\n",
      "    trust_remote_code=True,\n",
      "    device_map=\"auto\",\n",
      "    framework=\"pt\"\n",
      ")\n",
      "generate_text(\"In this chapter, we'll discuss first steps with generative \n",
      "AI in Python.\")\n",
      "Running the preceding code will download everything that’s needed for the model such as the \n",
      "tokenizer and model weights from Hugging Face.\n",
      "  To plug this pipeline into a LangChain agent or chain, we can use it the same way that we’ve seen \n",
      "in the other examples in this chapter:\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "template = \"\"\"Question: {question}\n",
      "Answer: Let's think step by step.\n",
      "Subtopics:\n",
      "  If you don’t have all libraries installed, make sure you execute \n",
      "this command:\n",
      "pip install transformers accelerate torch\n",
      "\n",
      "  I haven’t included accelerate in the main requirements, but I’ve included the \n",
      "transformers library.\n",
      "  We can then run a text \n",
      "completion to give us some inspiration for this chapter.\n",
      "\n",
      "  \"\"\"\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
      "llm_chain = LLMChain(prompt=prompt, llm=generate_text)\n",
      "\n",
      "----------------------------------------\n",
      "Page 110:\n",
      "Topics:\n",
      "  Chapter 3\n",
      "87\n",
      "question = \"What is electroencephalography?\"\n",
      "print(llm_chain.run(question))\n",
      "\n",
      "  -j4 # runs make in subdir with 4 processes\n",
      "\n",
      "  llama.cpp is a C++ port of Facebook’s LLaMA, LLaMA 2, and other derivative models with a \n",
      "similar architecture.\n",
      "  One \n",
      "of the main use cases of llama.cpp is to run models efficiently on the CPU; however, there are \n",
      "also some options for GPU.\n",
      "\n",
      "  This is included by default \n",
      "in several Linux distributions such as Ubuntu.\n",
      "  On macOS, you can install it with brew like this:\n",
      "brew install md5sha1sum\n",
      "We need to download the llama.cpp repository from GitHub.\n",
      "  You can do this online by choosing \n",
      "one of the download options on GitHub, or you can use a git command from the terminal like this:\n",
      "git clone https://github.com/ggerganov/llama.cpp.git\n",
      "Then we need to install the Python requirements, which we can do with the pip package installer \n",
      "– let’s also switch to the llama.cpp project root directory for convenience:\n",
      "cd llama.cpp\n",
      "pip install -r requirements.txt\n",
      "You might want to create a Python environment before you install the requirements, but this is \n",
      "up to you.\n",
      "  llama.cpp\n",
      "Written and maintained by Georgi Gerganov, llama.cpp is a C++ toolkit that executes models \n",
      "based on architectures based on or like LLaMA, one of the first large open-source models, which \n",
      "was released by Meta, and which spawned the development of many other models in turn.\n",
      "  We can parallelize the build with 4 processes:\n",
      "make -C .\n",
      "  In this example, we also see the use of a PromptTemplate that gives specific instructions for the task.\n",
      "\n",
      "Subtopics:\n",
      "  In my case, I received an error message at the end that a few libraries were missing, so \n",
      "I had to execute this command:\n",
      "pip install 'blosc2==2.0.0' cython FuzzyTM\n",
      " \n",
      "  Please note that you need to have an md5 checksum tool installed.\n",
      "  Now we need to compile llama.cpp.\n",
      "  Let’s have a look at this next.\n",
      "\n",
      "----------------------------------------\n",
      "Page 111:\n",
      "Topics:\n",
      "  There are also many other models with more permissive licensing such as Falcon or Mistral, Vi-\n",
      "cuna, OpenLLaMA, or Alpaca.\n",
      "  Getting Started with LangChain\n",
      "88\n",
      "To get the Llama model weights, you need to sign up with the T&Cs and wait for a registration \n",
      "email from Meta.\n",
      "  There are tools such as the llama model downloader in the pyllama project, but \n",
      "please be advised that they might not conform to the license stipulations by Meta.\n",
      "\n",
      "  We have \n",
      "to convert the model to llama.cpp format, which is called ggml, using the convert script:\n",
      "python3 convert.py models/3B/ --ctx 2048. \n",
      "\n",
      "  You can move the two files \n",
      "into the models/3B directory.\n",
      "\n",
      "  You can download models in much bigger sizes such as 13B, 30B, and 65B; however, a note of \n",
      "caution is in order here: these models are big both in terms of memory and disk space.\n",
      "  Once we have chosen a model that \n",
      "we want to run, we can integrate it into an agent or a chain, for example, as follows:\n",
      "llm = LlamaCpp(\n",
      "    model_path=\"./ggml-model-q4_0.bin\",\n",
      "    verbose=True\n",
      ")\n",
      "\n",
      "  Let’s assume you download the model weights and the tokenizer \n",
      "model for the OpenLLaMA 3B model using the link on the llama.cpp GitHub page.\n",
      "  The model \n",
      "file should be about 6.8 Gigabyes big, the tokenizer is much smaller.\n",
      "Subtopics:\n",
      "  GPT4All Is a fantastic tool that not only includes running but also serving and customizing models.\n",
      "\n",
      "  Compared \n",
      "to llama.cpp, however, it’s much more convenient to use and much easier to install.\n",
      "  This tool is closely related to llama.cpp, and it’s based on an interface with llama.cpp.\n",
      "  The setup \n",
      "instructions for this book already include the gpt4all library, which is needed.\n",
      "\n",
      "  ./models/3B/ggml-model-f16.gguf ./models/3B/ggml-model-q4_0.bin \n",
      "q4_0\n",
      "This last file is much smaller than the previous files and will take up much less space in memory \n",
      "as well, which means that you can run it on smaller machines.\n",
      "  Quantization \n",
      "refers to reducing the number of bits that are used to store weight:\n",
      "./quantize\n",
      "  Then we can optionally quantize the models to save memory when doing inference.\n",
      "  GPT4All\n",
      "\n",
      "----------------------------------------\n",
      "Page 112:\n",
      "Topics:\n",
      "  This should serve as a first introduction to integrations with local models.\n",
      "  This model requires 3.83 GB of harddisk to store and 8 GB of \n",
      "RAM to run.\n",
      "  the model, which is one of the best \n",
      "chat model available through GPT4All, pre-trained by the French startup Mistral AI, and fine-\n",
      "tuned by the OpenOrca AI initiative.\n",
      "  Building an application for customer service\n",
      "Customer service agents are responsible for answering customer inquiries, resolving issues, and \n",
      "addressing complaints.\n",
      "  In the next section, \n",
      "we’ll discuss building a text classification application in LangChain to assist customer service \n",
      "agents.\n",
      "  Executing this should first download (if not downloaded yet)\n",
      "  Chapter 3\n",
      "89\n",
      "As for model support, GPT4All supports a large array of Transformer architectures:\n",
      "•\t\n",
      "GPT-J\n",
      "•\t\n",
      "LLaMA (via llama.cpp)\n",
      "•\t\n",
      "Mosaic ML’s MPT architecture\n",
      "•\t\n",
      "Replit\n",
      "•\t\n",
      "Falcon\n",
      "•\t\n",
      "BigCode’s StarCoder\n",
      "You can find a list of all available models on the project website, where you can also see their \n",
      "results in important benchmarks: https://gpt4all.io/.\n",
      "Here’s a quick example of text generation with GPT4All:\n",
      "from langchain.llms import GPT4All\n",
      "model = GPT4All(model=\"mistral-7b-openorca.\n",
      "Subtopics:\n",
      "  Their work is crucial for maintaining customer satisfaction and loyalty, \n",
      "which directly affects a company’s reputation and financial success.\n",
      "\n",
      "  The goal is to categorize customer emails based on intent, extract sentiment, and generate \n",
      "summaries to help agents understand and respond faster.\n",
      "\n",
      "  Then we should hopefully see some convincing arguments for running LLMs locally.\n",
      "\n",
      "  Q4_0.gguf\", n_ctx=512, n_\n",
      "threads=8)\n",
      "response = model(\n",
      "    \"We can run large language models locally for all kinds of \n",
      "applications, \"\n",
      ")\n",
      "\n",
      "----------------------------------------\n",
      "Page 113:\n",
      "Topics:\n",
      "  A prompt for an LLM for sentiment analysis could be something like this:\n",
      "Given this text, what is the sentiment conveyed?\n",
      "  This was analyzed by Zengzhi Wang and others in their April 2023 study, Is \n",
      "ChatGPT a Good Sentiment Analyzer?\n",
      "  •\t\n",
      "Answer suggestions: This provides agents with suggested responses to common inquiries, \n",
      "ensuring that accurate and consistent messaging is provided.\n",
      "\n",
      "  We could ask any LLM to give us an open-domain (any category) classification or choose between \n",
      "multiple categories.\n",
      "  LangChain provides the flexibility to leverage different models.\n",
      "  We will work on other projects for ques-\n",
      "tion-answering in Chapter 5, Building a Chatbot Like ChatGPT.\n",
      "\n",
      "  LangChain comes with many \n",
      "integrations that can enable us to tackle a wide range of text problems.\n",
      "  •\t\n",
      "Intent classification: Similar to summarization, this helps predict the customer’s purpose \n",
      "and allows for faster problem-solving.\n",
      "\n",
      "  Getting Started with LangChain\n",
      "90\n",
      "Generative AI can assist customer service agents in several ways:\n",
      "•\t\n",
      "Sentiment classification: This helps identify customer emotions and allows agents to \n",
      "personalize their responses.\n",
      "•\t\n",
      "Summarization:\n",
      "  We can access all kinds of models for open-domain classification and sentiment and smaller \n",
      "transformer models through Hugging Face for focused tasks.\n",
      "Subtopics:\n",
      "  This enables agents to understand the key points of lengthy customer \n",
      "messages and save time.\n",
      "\n",
      "  We’ll build a prototype that uses \n",
      "sentiment analysis to classify email sentiment, summarization to condense lengthy text, and \n",
      "intent classification to categorize the issue.\n",
      "\n",
      "  In particular, because of their large training size, LLMs are enormously pow-\n",
      "erful models, especially when given few-shot prompts, for sentiment analysis that don’t need any \n",
      "additional training.\n",
      "  A Preliminary Study.\n",
      "\n",
      "  Customer service is crucial for maintaining \n",
      "customer satisfaction and loyalty.\n",
      "  We have a choice between \n",
      "many different integrations to perform these tasks.\n",
      "\n",
      "  Is it positive, neutral, \n",
      "or negative?\n",
      "\n",
      "  Combined, these can enable more accurate, timely responses.\n",
      "\n",
      "  Given a document such as an email, we want to classify it into different categories related to \n",
      "intent, extract the sentiment, and provide a summary.\n",
      "  These approaches combined can help customer service agents respond more accurately and in \n",
      "a timely manner, improving customer satisfaction.\n",
      "  Generative AI can help agents in several ways – sentiment \n",
      "analysis to gauge emotion, summarization to identify key points, and intent classification to \n",
      "determine purpose.\n",
      "----------------------------------------\n",
      "Page 114:\n",
      "Topics:\n",
      "  We can list the 5 most downloaded models on Hugging Face Hub for text classification through \n",
      "the Hugging Face API:\n",
      "from huggingface_hub import list_models\n",
      "def list_most_popular(task: str):\n",
      "\n",
      "  Hugging Face contains thousands of models, many fine-tuned for particular domains.\n",
      "  Many Hugging Face models are supported for these tasks, including:\n",
      "•\t\n",
      "Document question-answering\n",
      "•\t\n",
      "Summarization\n",
      "•\t\n",
      "Text classification\n",
      "•\t\n",
      "Text question-answering\n",
      "•\t\n",
      "Translation\n",
      "We can execute these models either locally by running a pipeline in transformer, remotely on the \n",
      "Hugging Face Hub server (HuggingFaceHub), or as a tool through the load_huggingface_tool() \n",
      "loader.\n",
      "\n",
      "  For example, NLP Cloud’s model list includes \n",
      "spaCy and many others: https://docs.nlpcloud.com/#models-list.\n",
      "\n",
      "  Finally, \n",
      "text classification could also be a case for embeddings, which we’ll discuss in Chapter 5, Building \n",
      "a Chatbot Like ChatGPT.\n",
      "\n",
      "  For example, \n",
      "ProsusAI/finbert is a BERT model that was trained on a dataset called Financial PhraseBank and \n",
      "can analyze the sentiment of financial text.\n",
      "  Chapter 3\n",
      "91\n",
      "Text: {sentence}\n",
      "Sentiment:\n",
      "LLMs can also be highly effective at summarization, much better than any previous models. \n",
      "\n",
      "Subtopics:\n",
      "  We could also use any local model.\n",
      "  I’ve decided to try and manage as much as I can with smaller models that I can find on Hugging \n",
      "Face for this exercise.\n",
      "\n",
      "  For text classifi-\n",
      "cation, the models tend to be much smaller, so this would be less of a drag on resources.\n",
      "  The downside can be that these model calls are slower than more traditional machine learning \n",
      "models and more expensive.\n",
      "\n",
      "  Cohere and other providers have text classification \n",
      "and sentiment analysis as part of their capabilities.\n",
      "  If we want to try out more traditional or smaller models, we can rely on libraries such as spaCy \n",
      "or access them through specialized providers.\n",
      "----------------------------------------\n",
      "Page 115:\n",
      "Topics:\n",
      "  Getting Started with LangChain\n",
      "92\n",
      "    for rank, model in enumerate(\n",
      "        list_models(filter=task, sort=\"downloads\", direction=-1)\n",
      "):\n",
      "        if rank == 5:\n",
      "            break\n",
      "        print(f\"{model.id}, {model.downloads}\\n\")\n",
      "list_most_popular(\"text-classification\")\n",
      "\n",
      "  Let’s see what our sentiment \n",
      "model has to say:\n",
      "from transformers import pipeline\n",
      "customer_email = \"\"\"\n",
      "I am writing to pour my heart out about the recent unfortunate experience \n",
      "I had with one of your coffee machines that arrived broken.\n",
      "  Let’s see the list:\n",
      "Model\n",
      "Downloads\n",
      "distilbert-base-uncased-finetuned-sst-2-english\n",
      "40,672,289\n",
      "cardiffnlp/twitter-roberta-base-sentiment\n",
      "9,292,338\n",
      "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n",
      "7,907,049\n",
      "cardiffnlp/twitter-roberta-base-irony\n",
      "7,023,579\n",
      "SamLowe/roberta-base-go_emotions\n",
      "6,706,653\n",
      "Table 3.2: The most popular text classification models on Hugging Face Hub\n",
      "Generally, we should see that these models are about small ranges of categories such as sentiment, \n",
      "emotions, irony, or well-formedness.\n",
      "  You can find the full email on GitHub.\n",
      "Subtopics:\n",
      "  I anxiously \n",
      "unwrapped the box containing my highly anticipated coffee machine. \n",
      "\n",
      "  Let’s use a sentiment model with a customer email, which \n",
      "should be a common use case in customer service.\n",
      "\n",
      "  I’ve asked GPT-3.5 to put together a rambling customer email complaining about a coffee machine \n",
      "– I’ve shortened it a bit here.\n",
      "  However, what I discovered within broke not only my spirit but also any \n",
      "semblance of confidence I had placed in your brand.\n",
      "\n",
      "----------------------------------------\n",
      "Page 116:\n",
      "Topics:\n",
      "  These are the labels:\n",
      "•\t\n",
      "0 – negative\n",
      "•\t\n",
      "1 – neutral\n",
      "•\t\n",
      "2 – positive\n",
      "Please make sure you have all the dependencies installed according to instructions in order to \n",
      "execute this.\n",
      "  Apart from emotion sentiment analysis, this model can \n",
      "also perform other tasks such as emotion recognition (anger, joy, sadness, or optimism), emoji \n",
      "prediction, irony detection, hate speech detection, offensive language identification, and stance \n",
      "detection (favor, neutral, or against).\n",
      "\n",
      "  For comparison, if the email says “I am so angry and sad, I want to kill myself,” we should get a \n",
      "score of close to 0.98 for the same label.\n",
      "  The sentiment model we are using here, Twitter-roBERTa-base, was trained on tweets, so it \n",
      "might not be the most adequate use case.\n",
      "  I am getting this result:\n",
      "[{'label': 'LABEL_0', 'score': 0.5822020173072815}]\n",
      "Not a happy camper.\n",
      "\n",
      "  This heartbreaking display of negligence shattered my dreams \n",
      "of indulging in daily coffee perfection, leaving me emotionally distraught \n",
      "and inconsolable\n",
      "\"\"\"\n",
      "sentiment_model = pipeline(\n",
      "    task=\"sentiment-analysis\",\n",
      "    model=\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
      ")\n",
      "print(sentiment_model(customer_email))\n",
      "\n",
      "  Chapter 3\n",
      "93\n",
      "Its once elegant exterior was marred by the scars of travel, resembling a \n",
      "war-torn soldier who had fought valiantly on the fields of some espresso \n",
      "battlefield.\n",
      "Subtopics:\n",
      "  We could try out other models or train better models \n",
      "once we have established metrics to work against.\n",
      "\n",
      "  For the sentiment analysis, we’ll get a rating and a numeric score that expresses confidence in \n",
      "the label.\n",
      "  Let’s move on!\n",
      "\n",
      "----------------------------------------\n",
      "Page 117:\n",
      "Topics:\n",
      "  Getting Started with LangChain\n",
      "94\n",
      "Here are the 5 most popular models for summarization as well (downloads at the time of writing, \n",
      "October 2023):\n",
      "Model\n",
      "Downloads\n",
      "facebook/bart-large-cnn\n",
      "4,637,417\n",
      "t5-small\n",
      "2,492,451\n",
      "t5-base\n",
      "1,887,661\n",
      "sshleifer/distilbart-cnn-12-6\n",
      "715,809\n",
      "t5-large\n",
      "332,854\n",
      "Table 3.3: The most popular summarization models on Hugging Face Hub\n",
      "All these models have a small footprint, which is nice, but to apply them in earnest, we should \n",
      "make sure they are reliable enough.\n",
      "\n",
      "  \"This heartbreaking display of negligence \n",
      "shattered my dreams of indulging in daily coffee perfection, leaving me \n",
      "emotionally distraught and inconsolable,\" the customer writes.\n",
      "  Let’s execute the summarization model remotely on a server.\n",
      "  Please note that you need to have \n",
      "your HUGGINGFACEHUB_API_TOKEN set for this to work:\n",
      "from langchain import HuggingFaceHub\n",
      "summarizer = HuggingFaceHub(\n",
      "    repo_id=\"facebook/bart-large-cnn\",\n",
      "    model_kwargs={\"temperature\":0, \"max_length\":180}\n",
      ")\n",
      "def summarize(llm, text) -> str:\n",
      "    return llm(f\"Summarize this: {text}!\")\n",
      "summarize(summarizer, customer_email)\n",
      "After executing this, I see this summary:\n",
      "A customer's coffee machine arrived ominously broken, evoking a profound \n",
      "sense of disbelief and despair.\n",
      "Subtopics:\n",
      "  \"I hope \n",
      "this email finds you amidst an aura of understanding, despite the tangled \n",
      "mess of emotions swirling within me as I write to you,\" he adds.\n",
      "\n",
      "----------------------------------------\n",
      "Page 118:\n",
      "Topics:\n",
      "  We’ll \n",
      "look at summarization in much more detail in Chapter 4, Building Capable Assistants.\n",
      "  We could try other models or just go for an LLM with a prompt asking to summarize.\n",
      "  Let’s ask Vertex \n",
      "AI:\n",
      "from langchain.llms import VertexAI\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "template = \"\"\"Given this text, decide what is the issue the customer is \n",
      "concerned about.\n",
      "  Before you execute the following code, make sure you have authenticated with GCP \n",
      "and you’ve set your GCP project according to the instructions mentioned in the \n",
      "section about Vertex AI.\n",
      "\n",
      "  Chapter 3\n",
      "95\n",
      "This summary is just passable, but not very convincing.\n",
      "Subtopics:\n",
      "  It could be quite useful to know what kind of issue the customer is writing about.\n",
      "  Valid categories are these:\n",
      "\n",
      "  Let’s move on.\n",
      "\n",
      "  * product issues\n",
      "* delivery problems\n",
      "* missing or late orders\n",
      "* wrong product\n",
      "* cancellation request\n",
      "* refund or exchange\n",
      "* bad support experience\n",
      "*\n",
      "  We get product issues back, which is correct for the long email example that I am using here.\n",
      "\n",
      "  There is still a lot of rambling in the sum-\n",
      "mary.\n",
      "  llm = VertexAI()\n",
      "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
      "print(llm_chain.run(customer_email))\n",
      "\n",
      "  no clear reason to be upset\n",
      "Text: {email}\n",
      "Category:\n",
      "\"\"\"\n",
      "prompt = PromptTemplate(template=template, input_variables=[\"email\"])\n",
      "\n",
      "----------------------------------------\n",
      "Page 119:\n",
      "Topics:\n",
      "  Getting Started with LangChain\n",
      "96\n",
      "\n",
      "  With thoughtful implementation, such AI \n",
      "automation can complement human agents – handling frequent questions to allow focusing \n",
      "on complex problems.\n",
      "  For each of them, we explained where to get the API token, and demonstrated how \n",
      "to call a model.\n",
      "\n",
      "  List at least 4 cloud providers of LLMs apart from OpenAI!\n",
      "3.\t\n",
      "\n",
      "  Let’s wrap up!\n",
      "\n",
      "  In Chapter 4, Building Capable Assistants and Chapter 5, Building a Chatbot Like ChatGPT, we’ll dive \n",
      "more into use cases such as question answering in chatbots through augmentation with tools \n",
      "and retrieval.\n",
      "\n",
      "  What are Jina AI and Hugging Face?\n",
      "4.\t\n",
      "\n",
      "  I’d recommend you go \n",
      "back to the corresponding sections of this chapter if you are unsure about any of them:\n",
      "1.\t\n",
      "\n",
      "  Finally, we developed an LLM app for text categorization (intent classification) and sentiment \n",
      "analysis in a use case for customer service.\n",
      "  This showcases LangChain’s ease in orchestrating \n",
      "multiple models to create useful applications.\n",
      "  How do you generate images with LangChain?\n",
      "5.\t\n",
      "How do you run a model locally on your own machine rather than through a service?\n",
      "6.\t\n",
      "\n",
      "  By chaining together various functionalities in \n",
      "LangChain, we can help reduce response times in customer service and make sure answers are \n",
      "accurate and to the point.\n",
      "\n",
      "  Summary\n",
      "In this chapter, we walked through four distinct ways of installing LangChain and other libraries \n",
      "needed in this book as an environment.\n",
      "  Overall, this demonstrates generative AI’s potential to enhance customer \n",
      "service workflows.\n",
      "\n",
      "  How can we help customer service agents in their work through generative AI?\n",
      "\n",
      "  How do you perform text classification in LangChain?\n",
      "7.\t\n",
      "\n",
      "  How do you install LangChain?\n",
      "2.\t\n",
      "\n",
      "Subtopics:\n",
      "  Questions\n",
      "Please look to see whether you can provide answers to these questions.\n",
      "  I hope it was exciting to see how quickly we can throw a few models and tools together in Lang-\n",
      "Chain to get something that looks actually useful.\n",
      "  We could easily expose this in a graphical interface for customer service agents to see and interact \n",
      "with.\n",
      "  Then, we introduced several providers of models for text \n",
      "and images.\n",
      "  This is something we will do in the next chapter.\n",
      "\n",
      "----------------------------------------\n",
      "Page 120:\n",
      "Topics:\n",
      "  Chapter 3\n",
      "97\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 121:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 122:\n",
      "Topics:\n",
      "  4\n",
      "Building Capable Assistants\n",
      "As LLMs continue to advance, a key challenge is transforming their impressive fluency into reliably \n",
      "capable assistants.\n",
      "  In short, this chapter covers:\n",
      "•\t\n",
      "Mitigating hallucinations through fact-checking\n",
      "•\t\n",
      "Summarizing information\n",
      "•\t\n",
      "Extracting information from documents\n",
      "•\t\n",
      "Answering questions with tools\n",
      "•\t\n",
      "Exploring reasoning strategies\n",
      "Let’s get started with addressing hallucinations through automatic fact-checking!\n",
      "\n",
      "Subtopics:\n",
      "  Finally, we will further extend this appli-\n",
      "cation through the application of reasoning strategies.\n",
      "\n",
      "  By verifying claims against the available evidence, we can reduce the spread of \n",
      "misinformation.\n",
      "  We’ll implement an application that showcases how connecting external data and \n",
      "services can augment LLMs’ limited world knowledge.\n",
      "  We will begin by addressing the critical weakness of hallucinated content through automatic \n",
      "fact-checking.\n",
      "  We’ll have sample applications \n",
      "that demonstrate these techniques in this chapter.\n",
      "\n",
      "  We will continue by discussing a key strength of LLMs with important appli-\n",
      "cations – summarization, which we’ll go into with the integration of prompts at different levels \n",
      "of sophistication, and the map reduce approach for very long documents.\n",
      "  This chapter explores methods for instilling greater intelligence, productivity, \n",
      "and trustworthiness in LLMs.\n",
      "  We will then move on \n",
      "to information extraction from documents with function calls, which leads to the topic of tool \n",
      "integrations.\n",
      "  The unifying theme across these approaches is enhancing LLMs \n",
      "through prompts, tools, and structured reasoning techniques.\n",
      "----------------------------------------\n",
      "Page 123:\n",
      "Topics:\n",
      "  Fact-checking involves three main stages:\n",
      "1.\t\n",
      "Claim detection: Identify parts needing verification\n",
      "2.\t\n",
      "Evidence retrieval:\n",
      "  Building Capable Assistants\n",
      "100\n",
      "Mitigating hallucinations through fact-checking\n",
      "As discussed in previous chapters, hallucination in LLMs refers to the generated text being un-\n",
      "faithful or nonsensical compared to the input.\n",
      "  We can see the general idea of these three stages illustrated in the following diagram (source – \n",
      "https://github.com/Cartus/Automated-Fact-Checking-Resources by Zhijiang Guo):\n",
      "Figure 4.1: Automatic fact-checking pipeline in three stages\n",
      "\n",
      "  This poses threats to society, including distrust in science, polarization, \n",
      "and democratic processes.\n",
      "\n",
      "  Find sources supporting or refuting the claim\n",
      "3.\t\n",
      "Verdict prediction: Assess claim veracity based on evidence\n",
      "Alternative terms for the last two stages are justification production and verdict prediction.\n",
      "\n",
      "Subtopics:\n",
      "  This allows for catching incorrect or unverified \n",
      "statements.\n",
      "\n",
      "  It contrasts with faithfulness, where outputs stay \n",
      "consistent with the source.\n",
      "  Hallucinations can spread misinformation like disinformation, rumors, \n",
      "and deceptive content.\n",
      "  Fact-checking ini-\n",
      "tiatives provide training and resources to journalists and independent checkers, allowing expert \n",
      "verification at scale.\n",
      "  One technique to address hallucinations is automatic fact-checking – verifying claims made by \n",
      "LLMs against evidence from external sources.\n",
      "  Journalism and archival studies have researched misinformation extensively.\n",
      "  Addressing false claims is crucial to preserving information integrity and \n",
      "combatting detrimental societal impacts.\n",
      "\n",
      "----------------------------------------\n",
      "Page 124:\n",
      "Topics:\n",
      "  In LangChain, we have a chain available for fact-checking with prompt chaining, where a mod-\n",
      "el actively questions the assumptions that went into a statement.\n",
      "  Alternatively, we can integrate external tools to search knowledge bases, Wikipedia, textbooks, \n",
      "and other corpora.\n",
      "  Chapter 4\n",
      "101\n",
      "Pre-trained LLMs contain extensive world knowledge that can be prompted for facts.\n",
      "  For exam-\n",
      "ple, to answer the question “Where is Microsoft’s headquarters located?”, the question would be \n",
      "rewritten as “Microsoft’s headquarters is in [MASK]” and fed into a language model for the answer.\n",
      "\n",
      "  Starting with the \n",
      "24-layer BERT-Large in 2018, language models have been pre-trained on large knowledge bases \n",
      "such as Wikipedia; therefore, they would be able to answer knowledge questions from Wikipedia \n",
      "or – since their training set increasingly includes other sources – the internet, textbooks, arXiv, \n",
      "and GitHub.\n",
      "\n",
      "  In this self-checking chain, \n",
      "LLMCheckerChain, the model is prompted sequentially – first, to make the assumptions explicit, \n",
      "which looks like this:\n",
      "Here's a statement: {statement}\\nMake a bullet point list of the \n",
      "assumptions you made when producing the above statement.\\n\n",
      "Please note that this is a string template, where the elements in curly brackets will be replaced \n",
      "by variables.\n",
      "  Additionally, \n",
      "external tools can search knowledge bases, Wikipedia, textbooks, and corpora for evidence.\n",
      "Subtopics:\n",
      "  Automatic fact-checking provides a way to make LLMs more reliable by checking that their re-\n",
      "sponses align with real-world evidence.\n",
      "  Next, these assumptions are fed back to the model in order to check them one by \n",
      "one with a prompt like this:\n",
      "Here is a bullet point list of assertions:\n",
      "    {assertions}\n",
      "    For each assertion, determine whether it is true or false.\n",
      "  By \n",
      "grounding claims in data, fact-checking makes LLMs more reliable.\n",
      "\n",
      "  The key idea is verifying hallucinated claims by grounding them in factual \n",
      "data sources.\n",
      "\n",
      "  If it is \n",
      "false, explain why.\\n\\n\n",
      "Finally, the model is tasked to make a final judgment:\n",
      "\n",
      "  Pre-trained LLMs contain extensive world knowledge from their training data.\n",
      "  We can prompt them with masking and other techniques to retrieve facts for evidence.\n",
      "  In light of the above facts, how would you answer the question \n",
      "'{question}'\n",
      "\n",
      "  In the next sections, we’ll demonstrate this approach.\n",
      "\n",
      "----------------------------------------\n",
      "Page 125:\n",
      "Topics:\n",
      "  When I was trying this out, I got results such as the blue \n",
      "whale, the North American beaver, and the extinct Giant Moa in response to my question \"What \n",
      "type of mammal lays the biggest eggs?\".\n",
      "  The eggs of the American \n",
      "echidna (spiny anteater) can grow as large as 10 cm in length, and \n",
      "dunnarts (mouse-sized marsupials found in Australia) can have eggs that \n",
      "exceed 5 cm in length.\n",
      "\n",
      "  Monotreme eggs can grow to 10 cm in \n",
      "length.\n",
      "\n",
      "  • Monotremes can be found in Australia and New Guinea\n",
      "• The largest eggs in the mammalian world are laid by monotremes\n",
      "•\n",
      "  The following is the right answer:\n",
      "Monotremes, a type of mammal found in Australia and parts of New Guinea, \n",
      "lay the largest eggs in the mammalian world.\n",
      "  The American echidna lays eggs that can grow to 10 cm in length\n",
      "• Dunnarts lay eggs that can exceed 5 cm in length\n",
      "• Monotremes can be found in Australia and New Guinea – True\n",
      "• The largest eggs in the mammalian world are laid by monotremes – True\n",
      "• The American echidna lays eggs that can grow to 10 cm in length – False, \n",
      "the American echidna lays eggs that are usually between 1 to 4 cm in \n",
      "length.\n",
      "\n",
      "  The largest eggs in the mammalian world are laid by monotremes, which can \n",
      "be found in Australia and New Guinea.\n",
      "  Dunnarts lay eggs that can exceed 5 cm in length – False, dunnarts lay \n",
      "eggs that are typically between 2 to 3 cm in length.\n",
      "\n",
      "  Building Capable Assistants\n",
      "102\n",
      "LLMCheckerChain does this all by itself, as this example shows:\n",
      "from langchain.chains import LLMCheckerChain\n",
      "from langchain.llms import OpenAI\n",
      "llm = OpenAI(temperature=0.7)\n",
      "text = \"What type of mammal lays the biggest eggs?\"\n",
      "checker_chain\n",
      "Subtopics:\n",
      "  > Finished chain.\n",
      "\n",
      "  The model can return different results to this question, some of which are wrong, and some of \n",
      "which it would correctly identify as false.\n",
      "  •\n",
      "  = LLMCheckerChain.from_llm(llm, verbose=True)\n",
      "checker_chain.run(text)\n",
      "\n",
      "----------------------------------------\n",
      "Page 126:\n",
      "Topics:\n",
      "  Summarizing information\n",
      "In today’s fast-paced business and research landscape, keeping up with the ever-increasing vol-\n",
      "ume of information can be a daunting task.\n",
      "  Off-the-shelf search engines \n",
      "like Google and Bing can also retrieve both topically and evidentially relevant content to capture \n",
      "the veracity of a statement accurately.\n",
      "  We will \n",
      "explore techniques for summarization using LangChain at increasing levels of sophistication.\n",
      "\n",
      "  Simply instruct the LLM on \n",
      "the desired length and provide a text:\n",
      "from langchain import OpenAI\n",
      "prompt = \"\"\"\n",
      "Summarize this text in one sentence:\n",
      "{text}\n",
      "\"\"\"\n",
      "llm = OpenAI()\n",
      "summary = llm(prompt.format(text=text))\n",
      "\n",
      "  Chapter 4\n",
      "103\n",
      "So, while this technique does not guarantee correct answers, it can put a stop to some incorrect \n",
      "results.\n",
      "Subtopics:\n",
      "  For engineers and researchers in fields like computer \n",
      "science and artificial intelligence, staying updated with the latest developments is crucial.\n",
      "  As engineers, we are driven by the desire to build and \n",
      "innovate and avoid repetitive tasks by automating them through the creation of pipelines and \n",
      "processes.\n",
      "  We’ll apply this approach to return results based on web \n",
      "searches and other applications of this chapter.\n",
      "\n",
      "  This approach, often mistaken for laziness, allows engineers to focus on more complex \n",
      "challenges and utilize their skills more efficiently.\n",
      "\n",
      "  Tools designed for searching domain \n",
      "datasets can assist fact-checkers in finding evidence effectively.\n",
      "  How-\n",
      "ever, reading and comprehending numerous papers can be time-consuming and labor-intensive. \n",
      "\n",
      "  In the next section, we’ll discuss automating the process of summarizing texts and longer doc-\n",
      "uments such as research papers.\n",
      "\n",
      "  This is where automation comes into play.\n",
      "  LLMs excel at condensing text through their strong language understanding abilities.\n",
      "  Fact-checking approaches involve decomposing claims into smaller checkable queries, \n",
      "which can be formulated as question-answering tasks.\n",
      "  Basic prompting\n",
      "For summarizing a couple of sentences, basic prompting works well.\n",
      "----------------------------------------\n",
      "Page 127:\n",
      "Topics:\n",
      "  We can also use the LangChain decorator syntax, which is implemented in the LangChain \n",
      "decorators library, which you should have installed together with all the other dependencies if \n",
      "you followed the instructions in Chapter 3, Getting Started with LangChain.\n",
      "LangChain Decorators provides a more Pythonic interface for defining and executing prompts \n",
      "compared to base LangChain, making it easier to leverage the power of LLMs.\n",
      "  Here’s a decorator example for summarization:\n",
      "from langchain_decorators import llm_prompt\n",
      "@llm_prompt\n",
      "def summarize(text:str, length=\"short\") -> str:\n",
      "    \"\"\"\n",
      "    Summarize this text in {length} length:\n",
      "    {text}\n",
      "    \"\"\"\n",
      "    return\n",
      "summary = summarize(text=\"let me tell you a boring story from when I was \n",
      "young...\")\n",
      "\n",
      "  This is similar to what we saw in Chapter 3, Getting Started with LangChain.\n",
      "  Building Capable Assistants\n",
      "104\n",
      "\n",
      "  By providing this intuitive interface, LangChain Decorators unlock \n",
      "the power of LLMs for developers.\n",
      "\n",
      "Subtopics:\n",
      "  Prompt \n",
      "templates allow variable length limits and modular prompt design.\n",
      "\n",
      "  The output, the value of the summary variable, I am getting is The speaker is about to share \n",
      "a story from their youth.\n",
      "  This abstraction enables prompting in a \n",
      "natural Python style while handling the complexity behind the scenes, making it easy to focus \n",
      "on creating effective prompts.\n",
      "  Pa-\n",
      "rameters are cleanly passed in and outputs are parsed.\n",
      "  Function decora-\n",
      "tors translate prompt documentation into executable code, enabling multiline definitions and \n",
      "natural code flow.\n",
      "\n",
      "  You can try more meaningful and longer examples for summari-\n",
      "zation yourself.\n",
      "\n",
      "  The @llm_prompt decorator translates the docstring into a prompt and handles executing it.\n",
      "  text is a string variable \n",
      "that can be any text that we want to summarize.\n",
      "\n",
      "  Prompt templates\n",
      "For dynamic inputs, prompt templates enable inserting text into predefined prompts.\n",
      "----------------------------------------\n",
      "Page 128:\n",
      "Topics:\n",
      "  Chain of density\n",
      "Researchers at Salesforce (Adams and colleagues, 2023; From Sparse to Dense: GPT-4 Summariza-\n",
      "tion with Chain of Density Prompting) have developed a prompt-guided technique called Chain of \n",
      "Density (CoD) to incrementally increase the information density of GPT-4 generated summaries \n",
      "while controlling length.\n",
      "\n",
      "  \"\n",
      ")\n",
      "runnable = prompt | llm | StrOutputParser()\n",
      "summary = runnable.invoke({\"text\": text})\n",
      "LCEL provides a declarative way to compose chains that is more intuitive and productive than \n",
      "directly writing code.\n",
      "  This is the prompt to use with CoD:\n",
      "template = \"\"\"Article: { text }\n",
      "You will generate increasingly concise, entity-dense summaries of the \n",
      "above article.\n",
      "\n",
      "  In this case, runnable is a chain, where the prompt template, the LLM, and the output parser are \n",
      "piped into one another.\n",
      "\n",
      "  Step 1.\n",
      "  Chapter 4\n",
      "105\n",
      "We can implement this in LangChain Expression Language (LCEL):\n",
      "from langchain import PromptTemplate, OpenAI\n",
      "from langchain.schema import StrOutputParser\n",
      "llm = OpenAI()\n",
      "prompt = PromptTemplate.from_template(\n",
      "    \"Summarize this text: {text}?\n",
      "  Repeat the following 2 steps 5 times.\n",
      "\n",
      "  Identify 1-3 informative entities (\";\" delimited) from the article \n",
      "which are missing from the previously generated summary.\n",
      "\n",
      "  A missing entity is:\n",
      "- relevant to the main story,\n",
      "- specific yet concise (5 words or fewer),\n",
      "- novel (not in the previous summary),\n",
      "- faithful (present in the article),\n",
      "- anywhere (can be located anywhere in the article).\n",
      "\n",
      "  Key benefits of LCEL include built-in support for asynchronous processing, \n",
      "batching, streaming, fallbacks, parallelism, and seamless integration with LangSmith tracing.\n",
      "\n",
      "  Step 2.\n",
      "Subtopics:\n",
      "  Write a new, denser summary of identical length which covers every \n",
      "entity and detail from the previous summary plus the missing entities.\n",
      "\n",
      "----------------------------------------\n",
      "Page 129:\n",
      "Topics:\n",
      "  This method and analysis sheds light on controlling \n",
      "information density in AI text generation.\n",
      "\n",
      "  It then iteratively identifies 1–3 missing \n",
      "entities and fuses them into a rewrite of the previous summary in the same number of words.\n",
      "\n",
      "  The JSON should be a list (length 5) of dictionaries whose \n",
      "keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
      "\n",
      "  Remember, use the exact same number of words for each summary.\n",
      "Answer in JSON.\n",
      "  The authors conduct both human preference studies and \n",
      "GPT-4 scoring to evaluate the impact on overall quality across the density spectrum.\n",
      "\n",
      "  Use overly verbose language and fillers (e.g., \"this article \n",
      "discusses\") to reach ~80 words.\n",
      "- Make every word count: rewrite the previous summary to improve flow and \n",
      "make space for additional entities.\n",
      "- Make space with fusion, compression, and removal of uninformative \n",
      "phrases like \"the article discusses\".\n",
      "-\n",
      "  Building Capable Assistants\n",
      "106\n",
      "Guidelines:\n",
      "- The first summary should be long (4-5 sentences, ~80 words) yet highly \n",
      "non-specific, containing little information beyond the entities marked \n",
      "as missing.\n",
      "  Through five iterative steps, summaries become highly condensed with more entities per token \n",
      "packed in through creative rewriting.\n",
      "  The CoD prompt instructs highly powered LLMs such as GPT-4 to produce an initial sparse, ver-\n",
      "bose summary of an article containing only a few entities.\n",
      "Subtopics:\n",
      "  The summaries should become highly dense and concise yet self-contained, \n",
      "i.e., easily understood without the article.\n",
      "- Missing entities can appear anywhere in the new summary.\n",
      "\n",
      "  The results reveal a trade-off between informativeness gained through density and declining \n",
      "coherence from excessive compression.\n",
      "  This repeated rewriting under length constraint forces increasing abstraction, fusion of details, \n",
      "and compression to make room for additional entities in each step.\n",
      "  Please try this out for yourself!\n",
      "\n",
      "  The authors measure statis-\n",
      "tics like entity density and source sentence alignment to characterize the densification effects.\n",
      "\n",
      "  \"\"\"\n",
      "Please note that you can easily adapt this to any kind of content and provide a different set of \n",
      "guidelines to suit other applications.\n",
      "\n",
      "  Optimal density balances concision and clarity, with too \n",
      "many entities overwhelming expression.\n",
      "  If space cannot be made, \n",
      "add fewer new entities.\n",
      "\n",
      "  - Never drop entities from the previous summary.\n",
      "----------------------------------------\n",
      "Page 130:\n",
      "Topics:\n",
      "  This is illustrated in the figure here:\n",
      "Figure 4.2: Map reduce chain in LangChain\n",
      "\n",
      "  2.\t\n",
      "Collapse (optional):\n",
      "  Optional collapsing, which may also involve utilizing LLMs, makes sure the data fits within se-\n",
      "quence length limits.\n",
      "  To summarize long documents, we can first split the document into smaller parts (chunks) that \n",
      "are suitable for the token context length of the LLM, and then a map-reduce chain can summarize \n",
      "these chunks independently before recombining.\n",
      "  1.\t\n",
      "Map: Each document is passed through a summarization chain (LLM chain).\n",
      "\n",
      "  3.\t\n",
      "Reduce: The collapsed document goes through a final LLM chain to produce the output.\n",
      "\n",
      "  Chapter 4\n",
      "107\n",
      "Map-Reduce pipelines\n",
      "LangChain supports a map reduce approach for processing documents using LLMs, which al-\n",
      "lows for efficient processing and analysis of documents.\n",
      "Subtopics:\n",
      "  A chain can be applied to each document \n",
      "individually and then we combine the outputs into a single document.\n",
      "\n",
      "  This compression step can be performed recursively if needed.\n",
      "\n",
      "  So, the map step applies a chain to each document in parallel.\n",
      "  The reduce step aggregates the \n",
      "mapped outputs and generates the final result.\n",
      "\n",
      "  The summarized documents are combined into a single document.\n",
      "\n",
      "  This scales summarization to any length of text \n",
      "while controlling chunk size.\n",
      "\n",
      "  The key steps are:\n",
      "\n",
      "----------------------------------------\n",
      "Page 131:\n",
      "Topics:\n",
      "  Building Capable Assistants\n",
      "108\n",
      "This approach’s implications are that it allows the parallel processing of documents and enables \n",
      "the use of LLMs for reasoning, generating, or analyzing individual documents and combining \n",
      "their outputs.\n",
      "\n",
      "  The variable pdf_file_path is a string with the path of a PDF file.\n",
      "  In the text summarization application developed for \n",
      "this chapter on GitHub, we can see how to pass other prompts.\n",
      "  Here’s a simple example of loading a PDF document and summarizing it:\n",
      "from langchain.chains.summarize import load_summarize_chain\n",
      "from langchain import OpenAI\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "pdf_file_path = \"<pdf_file_path>\"\n",
      "pdf_loader = PyPDFLoader(pdf_file_path)\n",
      "docs = pdf_loader.load_and_split()\n",
      "llm = OpenAI()\n",
      "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
      "chain.run(docs)\n",
      "\n",
      "  Please replace the file path \n",
      "with the path to a PDF document.\n",
      "\n",
      "  Don't try to make up an answer.\\\n",
      "nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: {question}\\\n",
      "n=========\\nContent: {text}\n",
      "In the preceding prompt, we could formulate a concrete question, but equally, we could give the \n",
      "LLM a more abstract instruction to extract assumptions and implications. \n",
      "\n",
      "Subtopics:\n",
      "  On LangChainHub, we can see \n",
      "the question-answering-with-sources prompt, which takes a reduce/combine prompt like this:\n",
      "Given the following extracted parts of a long document and a question, \n",
      "create a final answer with references (\\\"SOURCES\\\").\n",
      "  The default prompt for both the map and reduce steps is this:\n",
      "Write a concise summary of the following:\n",
      "{text}\n",
      "CONCISE SUMMARY:\n",
      "We can specify any prompt for each step.\n",
      "  \\nIf you don't know \n",
      "the answer, just say that you don't know.\n",
      "----------------------------------------\n",
      "Page 132:\n",
      "Topics:\n",
      "  Chapter 4\n",
      "109\n",
      "The text would be the summaries from the map steps.\n",
      "  A few practical tips are:\n",
      "•\t\n",
      "Start with simpler approaches and move to map-reduce if needed\n",
      "•\t\n",
      "Tune chunk size to balance context limits and parallelism\n",
      "•\t\n",
      "Customize map and reduce prompts for the best results\n",
      "•\t\n",
      "Compress or recursively reduce chunks to fit context limits\n",
      "\n",
      "  The tool on GitHub will summarize the core assertions, implications, and mechanics of a paper \n",
      "in a more concise and simplified manner.\n",
      "  This \n",
      "can be built out into an automation tool that can quickly summarize the content of long texts \n",
      "in a more digestible format, as you should be able to tell from the summarize package in the \n",
      "book’s GitHub repository, which shows how to focus on different perspectives and structures of \n",
      "the response (adapted from David Shapiro).\n",
      "\n",
      "  For any serious usage of generative AI, we need to understand the capabilities, pricing options, \n",
      "and use cases for different language models.\n",
      "  For example, OpenAI exposes powerful language models suitable \n",
      "for solving complex problems with NLP and offers flexible pricing options based on the size and \n",
      "number of tokens used.\n",
      "\n",
      "  All cloud providers provide different models that \n",
      "cater to various NLP needs.\n",
      "  Thoughtful prompt engineering with LangChain provides powerful summarization \n",
      "capabilities using LLMs.\n",
      "Subtopics:\n",
      "  Once we start making a lot of calls, especially in the map step, if we use a cloud provider, we’ll \n",
      "see tokens and, therefore, costs increase.\n",
      "  An instruction like that would help against \n",
      "hallucinations.\n",
      "  Other examples of instructions could be translating the document into a different \n",
      "language or rephrasing in a certain style.\n",
      "\n",
      "  By changing the prompt, we can ask any question to be answered from these documents.\n",
      "  Overall, \n",
      "the approach aims to benefit researchers by providing a more efficient and accessible way to stay \n",
      "updated on the latest research.\n",
      "\n",
      "  Monitoring token usage\n",
      "When using LLMs, especially in long loops such as with map operations, it’s important to track \n",
      "the token usage and understand how much money you are spending.\n",
      "\n",
      "  It can also answer specific questions about the paper, \n",
      "making it a valuable resource for literature reviews and accelerating scientific research.\n",
      "  It’s time to give this some visibility!\n",
      "\n",
      "----------------------------------------\n",
      "Page 133:\n",
      "Topics:\n",
      "  OpenAI provides DALL·E, Whisper, and API services for various applications, such as image gener-\n",
      "ation, speech transcription, translation, and access to language models.\n",
      "  We can track the token usage in OpenAI models by hooking into the OpenAI callback:\n",
      "from langchain import OpenAI, PromptTemplate\n",
      "from langchain.callbacks import get_openai_callback\n",
      "llm_chain = PromptTemplate.from_template(\"Tell me a joke about {topic}!\") \n",
      "| OpenAI()\n",
      "with get_openai_callback() as cb:\n",
      "    response = llm_chain.invoke(dict(topic=\"light bulbs\"))\n",
      "    \n",
      "  print(f\"Total Tokens: {cb.total_tokens}\")\n",
      "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
      "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
      "    \n",
      "  Building Capable Assistants\n",
      "110\n",
      "For example, ChatGPT models, like GPT-3.5-Turbo, specialize in dialogue applications such as \n",
      "chatbots and virtual assistants.\n",
      "  OpenAI offers three tiers of resolution, allowing users to choose the level \n",
      "of detail they need.\n",
      "  The cost of using Whisper is based on a per-minute rate.\n",
      "\n",
      "  Whisper is an AI tool that can transcribe speech into text and translate multiple languages into \n",
      "English.\n",
      "  Total Tokens: 36\n",
      "Prompt Tokens: 8\n",
      "\n",
      "  The pricing of models depends on the model’s capabilities and \n",
      "ranges from low-cost options like Ada to more expensive options like Davinci.\n",
      "\n",
      "  Different models within the InstructGPT family, designed for single-turn instruction following, \n",
      "such as Ada and Davinci, offer varying levels of speed and power.\n",
      "  Ada is the fastest model, suit-\n",
      "able for applications where speed is crucial, while Davinci is the most powerful model, capable \n",
      "of handling complex instructions.\n",
      "Subtopics:\n",
      "  They excel at generating responses with accuracy and fluency. \n",
      "\n",
      "  We should see an output with the costs and tokens.\n",
      "  print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
      "\n",
      "  I am getting this output when I run this:\n",
      "Q: How many light bulbs does it take to change people's minds?\n",
      "\n",
      "  print(response)\n",
      "    \n",
      "  DALL·E is an AI-powered \n",
      "image generation model that can be seamlessly integrated into apps for generating and editing \n",
      "novel images and art.\n",
      "  Higher resolutions offer more complexity and detail, while lower resolutions \n",
      "provide a more abstract representation.\n",
      "  It helps capture conversations, facilitates communication, and improves understanding \n",
      "across languages.\n",
      "  The price per image varies based on the resolution.\n",
      "\n",
      "  A: Depends on how stubborn they are!\n",
      "\n",
      "----------------------------------------\n",
      "Page 134:\n",
      "Topics:\n",
      "  Chapter 4\n",
      "111\n",
      "Completion Tokens: 28\n",
      "Total Cost (USD): $0.00072\n",
      "You can change the parameters of the model and the prompt, and you should see costs and tokens \n",
      "changing as a consequence.\n",
      "\n",
      "  ', generation_info={'finish_reason': 'stop', \n",
      "'logprobs': None})], [Generation(text='\\n\\nFootwear Factory.', generation_\n",
      "info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_\n",
      "usage': {'prompt_tokens': 36, 'total_tokens': 55, 'completion_tokens': \n",
      "19}, 'model_name': 'text-davinci-003'})\n",
      "\n",
      "  There are two other ways of getting the token usage.\n",
      "  As an alternative to the OpenAI callback, \n",
      "the generate() method of the llm class returns a response of type LLMResult instead of a string. \n",
      "\n",
      "  This includes token usages and finish reason, for example (from the LangChain docs):\n",
      "input_list = [\n",
      "    {\"product\": \"socks\"},\n",
      "    {\"product\": \"computer\"},\n",
      "    {\"product\": \"shoes\"}\n",
      "]\n",
      "llm_chain.generate(input_list)\n",
      "\n",
      "  Finally, the chat completions response format in the OpenAI API includes a usage object with \n",
      "token information; for example, it could look like this (excerpt):\n",
      "  {\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 17,\n",
      "    \"prompt_tokens\": 57,\n",
      "    \"total_tokens\": 74\n",
      "  }\n",
      "}\n",
      "\n",
      "  The result looks like this:\n",
      "    LLMResult(generations=[[Generation(text='\\n\\nSocktastic!', generation_\n",
      "info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\\n",
      "nTechCore Solutions.\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 135:\n",
      "Topics:\n",
      "  Extracting information from documents\n",
      "In June 2023, OpenAI announced updates to OpenAI’s API, including new capabilities for function \n",
      "calling, which enhanced functionality.\n",
      "  In Chapter 9, Generative AI in Production.\n",
      "  This feature aims to enhance the connection between GPT models and external tools and APIs, \n",
      "providing a reliable way to retrieve structured data from the models.\n",
      "\n",
      "  It also allows for converting natural language queries into API calls or \n",
      "database queries and extracting structured data from text.\n",
      "\n",
      "  we’ll look at LangSmith and \n",
      "similar tools that provide additional observability of LLMs in action, including their token usage.\n",
      "\n",
      "  Developers can now describe functions to the gpt-4-0613 and gpt-3.5-turbo-0613 models and \n",
      "have the models intelligently generate a JSON object containing arguments to call those functions. \n",
      "\n",
      "  Next, we’ll look at how to extract certain pieces of information from documents using OpenAI \n",
      "functions with LangChain.\n",
      "\n",
      "  By describing functions in a schema, developers can tune LLMs to return structured \n",
      "outputs adhering to that schema – for example, extracting entities from text by outputting them \n",
      "in a predefined JSON format.\n",
      "\n",
      "  For information extraction, we can obtain specific entities and their properties from a \n",
      "text and their properties from a document in an extraction chain with OpenAI chat models.\n",
      "  By using the OpenAI functions \n",
      "parameter and specifying a schema, it ensures that the model outputs the desired entities and \n",
      "properties with their appropriate types.\n",
      "\n",
      "  OpenAI’s addition of function calling builds on instruc-\n",
      "tion tuning.\n",
      "  The mechanics of the update involve using new API parameters, namely functions, in the /v1/\n",
      "chat/completions endpoint.\n",
      "  In LangChain, we can use these function calls in OpenAI for information extraction or for calling \n",
      "plugins.\n",
      "  Building Capable Assistants\n",
      "112\n",
      "This can be extremely helpful for understanding how much money you are spending on distinct \n",
      "parts of your application.\n",
      "  Function calling enables developers to create chatbots that can answer questions using external \n",
      "tools or OpenAI plugins.\n",
      "Subtopics:\n",
      "  The functions parameter is defined through a name, description, \n",
      "parameters, and the function to call itself.\n",
      "  It also enables specifying which properties \n",
      "are required and which are optional.\n",
      "\n",
      "  Developers can describe functions to the model using \n",
      "JSON schema and specify the desired function to be called.\n",
      "\n",
      "  The implications of this approach are that it allows for precise extraction of entities by defining \n",
      "a schema with the desired properties and their types.\n",
      "  For \n",
      "example, this can help identify the people mentioned in the text.\n",
      "----------------------------------------\n",
      "Page 136:\n",
      "Topics:\n",
      "  Chapter 4\n",
      "113\n",
      "The default format for the schema is a dictionary, but we can also define properties and their types \n",
      "in Pydantic, a popular parsing library, providing control and flexibility in the extraction process.\n",
      "\n",
      "  Here’s an example of a desired schema for information in a Curriculum Vitae (CV):\n",
      "from typing import Optional\n",
      "from pydantic import BaseModel\n",
      "class Experience(BaseModel):\n",
      "    start_date: Optional[str]\n",
      "    end_date: Optional[str]\n",
      "    description: Optional[str]\n",
      "class Study(Experience):\n",
      "    degree: Optional[str]\n",
      "    university: Optional[str]\n",
      "    country: Optional[str]\n",
      "    grade: Optional[str]\n",
      "class WorkExperience(Experience):\n",
      "    company: str\n",
      "    job_title: str\n",
      "class Resume(BaseModel):\n",
      "    first_name: str\n",
      "    last_name: str\n",
      "    linkedin_url: Optional[str]\n",
      "    email_address: Optional[str]\n",
      "    nationality: Optional[str]\n",
      "    skill: Optional[str]\n",
      "    study: Optional[Study]\n",
      "    work_experience: Optional[WorkExperience]\n",
      "    hobby: Optional[str]\n",
      "We can use this for information extraction from a CV.\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 137:\n",
      "Topics:\n",
      "  In its most \n",
      "simple terms, we can try this code snippet:\n",
      "from langchain.chains import create_extraction_chain_pydantic\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "\n",
      "  This adds two extra lines to the beginning of the code:\n",
      "from config import setup_environment\n",
      "setup_environment()\n",
      "\n",
      "  Utilizing the create_extraction_chain_pydantic() function in LangChain, we can provide \n",
      "our schema as input, and an output will be an instantiated object that adheres to it.\n",
      "  Building Capable Assistants\n",
      "114\n",
      "Please note that you should set up your environment according to the instructions in Chapter 3, \n",
      "Getting Started with LangChain.\n",
      "  Here’s an example CV from https://github.com/xitanggg/open-resume:\n",
      "Figure 4.3: Extract of an example CV\n",
      "\n",
      "Subtopics:\n",
      "  This is my advice – you can take it or leave it.\n",
      "\n",
      "  We are going to try to parse the information from this resume.\n",
      "\n",
      "  I’ve found it most convenient to import my config module here \n",
      "and execute setup_environment().\n",
      "----------------------------------------\n",
      "Page 138:\n",
      "Topics:\n",
      "  [Resume(first_name='John', last_name='Doe', linkedin_url='linkedin.com/\n",
      "in/john-doe', email_address='hello@openresume.com', nationality=None, \n",
      "skill='React', study=None, work_experience=WorkExperience(start_date='May \n",
      "2023', end_date='Present', description='Lead a cross-functional team of \n",
      "5 engineers in developing a search bar, which enables thousands of daily \n",
      "active users to search content across the entire platform.\n",
      "  OpenAI injects these function calls into the system message in a certain syntax, which their \n",
      "models have been optimized for.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0613\")\n",
      "chain = create_extraction_chain_pydantic(pydantic_schema=Resume, llm=llm)\n",
      "chain.run(docs)\n",
      "Please note that the pdf_file_path variable should be the relative or absolute path to a pdf file. \n",
      "\n",
      "  We’ll look at \n",
      "this now, and we’ll build this into an interactive web app with Streamlit.\n",
      "\n",
      "  For a complete example, please refer to the GitHub repository. \n",
      "\n",
      "  ', company='ABC Company', job_title='Software Engineer'), \n",
      "hobby=None)]\n",
      "\n",
      "  LangChain natively has the functionality to inject function calls as prompts.\n",
      "  Chapter 4\n",
      "115\n",
      "pdf_file_path = \"<pdf_file_path>\"\n",
      "pdf_loader = PyPDFLoader(pdf_file_path)\n",
      "docs = pdf_loader.load_and_split()\n",
      "# please note that function calling is not enabled for all models!\n",
      "\n",
      "  This leads to tool \n",
      "integrations, where LLM agents can execute these function calls to connect LLMs with live data, \n",
      "services, and runtime environments.\n",
      "  This result is far from perfect – only one work experience gets parsed out.\n",
      "  Create stunning \n",
      "home page product demo animations that drives up sign up rate by 20%. \n",
      "\n",
      "  Write clean code that is modular and easy to maintain while ensuring 100% \n",
      "test coverage.\n",
      "  This means we can \n",
      "use models from providers other than OpenAI for function calls within LLM apps.\n",
      "Subtopics:\n",
      "  This implies that functions count against the context limit and \n",
      "are correspondingly billed as input tokens.\n",
      "\n",
      "  But it’s a good start given \n",
      "the little effort we’ve put in so far.\n",
      "  We could add more functionality, for example, to guess personality or leadership capability.\n",
      "\n",
      "  In the next section, we’ll discuss how tools can augment \n",
      "context by retrieving external knowledge sources to enhance understanding.\n",
      "\n",
      "  We should get an output like this:\n",
      "\n",
      "  Instruction tuning and function calling allow models to produce callable code.\n",
      "----------------------------------------\n",
      "Page 139:\n",
      "Topics:\n",
      "  Tools leverage contextual dialogue representation to search pertinent data sources related to the \n",
      "user’s query.\n",
      "  Building Capable Assistants\n",
      "116\n",
      "Answering questions with tools\n",
      "LLMs are trained on general corpus data and may not be as effective for tasks that require do-\n",
      "main-specific knowledge.\n",
      "  One important aspect of tools is their capability to work within specific domains or process spe-\n",
      "cific inputs.\n",
      "  For example, for a question about a historical event, tools could retrieve Wikipedia \n",
      "articles to augment context.\n",
      "\n",
      "  The LLM combined with such a mathematical tool performs calculations \n",
      "and provides accurate answers.\n",
      "\n",
      "  For example, a tool could be developed to \n",
      "enable an LLM to perform advanced retrieval searches, query a database for specific information, \n",
      "automate email writing, or even handle phone calls.\n",
      "\n",
      "  For example, an LLM lacks inherent mathematical capabilities.\n",
      "  On their own, LLMs can’t interact with the environment and access \n",
      "external data sources; however, LangChain provides a platform for creating tools that access \n",
      "real-time information and perform tasks such as weather forecasting, making reservations, sug-\n",
      "gesting recipes, and managing tasks.\n",
      "  Let’s set up an agent with a few tools:\n",
      "from langchain.agents import (\n",
      "    AgentExecutor, AgentType, initialize_agent, load_tools\n",
      ")\n",
      "\n",
      "  Information retrieval with tools\n",
      "We have quite a few tools available in LangChain, and – if that’s not enough – it’s not hard to roll \n",
      "out our own tools.\n",
      "Subtopics:\n",
      "  By grounding responses in real-time data, tools reduce hallucinated or incorrect replies.\n",
      "  Contex-\n",
      "tual tool use complements chatbots’ core language capabilities to make responses more useful, \n",
      "correct, and aligned with real-world knowledge.\n",
      "  However, a mathe-\n",
      "matical tool like a calculator can accept mathematical expressions or equations as an input and \n",
      "calculate the outcome.\n",
      "  Let’s see this in action!\n",
      "\n",
      "  Tools provide creative solutions to problems and \n",
      "open up new possibilities for LLMs in various domains.\n",
      "  Tools within the framework of agents and chains allow for \n",
      "the development of applications powered by LLMs that are data-aware and agentic and open \n",
      "up a wide range of approaches to solving problems with LLMs, expanding their use cases, and \n",
      "making them more versatile and powerful.\n",
      "\n",
      "----------------------------------------\n",
      "Page 140:\n",
      "Topics:\n",
      "  The Zero-Shot agent is a general-purpose action agent, which we’ll \n",
      "discuss in the next section.\n",
      "\n",
      "  Chapter 4\n",
      "117\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "def load_agent() ->\n",
      "  llm = ChatOpenAI(temperature=0, streaming=True)\n",
      "    # DuckDuckGoSearchRun, wolfram alpha, arxiv search, wikipedia\n",
      "    # TODO: try wolfram-alpha!\n",
      "    tools = load_tools(\n",
      "        tool_names=[\"ddg-search\", \"wolfram-alpha\", \"arxiv\", \"wikipedia\"],\n",
      "        llm=llm\n",
      "    )\n",
      "    return initialize_agent(\n",
      "        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
      "verbose=True\n",
      "    )\n",
      "\n",
      "  Currently, only the OpenAI, ChatOpenAI, \n",
      "and ChatAnthropic implementations support streaming.\n",
      "\n",
      "  These tools here are plugged into the agent:\n",
      "•\t\n",
      "DuckDuckGo: A search engine that focuses on privacy; an added advantage is that it \n",
      "doesn’t require developer signup\n",
      "•\t\n",
      "Wolfram Alpha: An integration that combines natural language understanding with math \n",
      "capabilities, for questions like “What is 2x+5 = -3x + 7?”\n",
      "•\t\n",
      "arXiv: Search in academic pre-print publications; this is useful for research-oriented \n",
      "questions\n",
      "•\t\n",
      "Wikipedia:\n",
      "  This function returns AgentExecutor, which is a chain; therefore, if we wanted, we could inte-\n",
      "grate it into a larger chain.\n",
      "Subtopics:\n",
      "  For any question about entities of significant notoriety\n",
      "\n",
      "  This \n",
      "makes for a better user experience since it means that the text response will be updated as it comes \n",
      "in, rather than once all the text has been completed.\n",
      "  All the tools mentioned have their specific purpose that’s part of the description, which is passed \n",
      "to the language model.\n",
      "  AgentExecutor:\n",
      "    \n",
      "  Please notice the streaming parameter in the ChatOpenAI constructor, which is set to True.\n",
      "----------------------------------------\n",
      "Page 141:\n",
      "Topics:\n",
      "  For this application, we’ll need the Streamlit, unstructured, and docx libraries, among others. \n",
      "\n",
      "  Building Capable Assistants\n",
      "118\n",
      "Please note that to use Wolfram Alpha, you have to set up an account and set the WOLFRAM_\n",
      "ALPHA_APPID environment variable with the developer token you create at https://products.\n",
      "wolframalpha.com/api.\n",
      "  Let’s write the code for this using the load_agent() function we’ve just defined:\n",
      "import streamlit as st\n",
      "from langchain.callbacks import StreamlitCallbackHandler\n",
      "chain = load_agent()\n",
      "st_callback = StreamlitCallbackHandler(st.container())\n",
      "\n",
      "  Streamlit provides an ideal framework for this goal.\n",
      "  Building a visual interface\n",
      "After developing an intelligent agent with LangChain, the natural next step is deploying it in an \n",
      "easy-to-use application.\n",
      "  We can start the app locally from the \n",
      "terminal like this:\n",
      "PYTHONPATH=. streamlit run question_answering/app.py\n",
      "We can open our app in the browser.\n",
      "  So let’s make our agent available as a Streamlit app!\n",
      "\n",
      "  These are in the environment that we set up in Chapter 3, Getting Started with LangChain.\n",
      "\n",
      "  There are a lot of other search tools integrated into LangChain apart from DuckDuckGo that let you \n",
      "utilize Google or Bing search engines or work with meta-search engines.\n",
      "  As an open-source \n",
      "platform optimized for ML workflows, Streamlit makes it simple to wrap our agent in an inter-\n",
      "active web application.\n",
      "Subtopics:\n",
      "  Please notice that we are using the callback handler in the call to the chain, which means that \n",
      "we’ll see responses as they come back from the model.\n",
      "  There’s an Open-Meteo \n",
      "integration for weather information; however, this information is also available through search.\n",
      "\n",
      "  Please note that the website can sometimes be a bit slow, and it might \n",
      "take patience to register.\n",
      "\n",
      "  response = chain.run(prompt, callbacks=[st_callback])\n",
      "        \n",
      "  if prompt := st.chat_input():\n",
      "    st.chat_message(\"user\").write(prompt)\n",
      "    with st.chat_message(\"assistant\"):\n",
      "        st_callback = StreamlitCallbackHandler(st.container())\n",
      "        \n",
      "  st.write(response)\n",
      "\n",
      "  Here’s a screenshot that illustrates what the app looks like:\n",
      "\n",
      "----------------------------------------\n",
      "Page 142:\n",
      "Topics:\n",
      "  2.\t\n",
      "Create a Hugging Face account at https://huggingface.co/.\n",
      "3.\t\n",
      "Go to Spaces and click Create new Space.\n",
      "  Chapter 4\n",
      "119\n",
      "Figure 4.4: Question-answering app in Streamlit\n",
      "The search works quite well although, depending on the tools used, it might still come up with \n",
      "the wrong results.\n",
      "  For Streamlit Community Cloud, do this:\n",
      "1.\t\n",
      "Create a GitHub repository.\n",
      "\n",
      "  In the form, set the fill in a name, \n",
      "type of space as Streamlit, and choose the new repo.\n",
      "\n",
      "  Alternatively, you \n",
      "can deploy this on Streamlit Community Cloud or on Hugging Face Spaces.\n",
      "\n",
      "  As for Hugging Face Spaces, it works like this:\n",
      "1.\t\n",
      "Create a GitHub repo.\n",
      "\n",
      "  3.\t\n",
      "Click Deploy!.\n",
      "\n",
      "  2.\t\n",
      "Go to Streamlit Community Cloud, click on New app, and select the new repo.\n",
      "\n",
      "Subtopics:\n",
      "  For the question about the mammal with the largest egg, using DuckDuckGo, \n",
      "it comes back with a result that discusses eggs in birds and mammals and sometimes concludes \n",
      "that the ostrich is the mammal with the largest egg, although platypus also comes back sometimes.\n",
      "\n",
      "  Deployment of Streamlit applications can be local or on a server.\n",
      "----------------------------------------\n",
      "Page 143:\n",
      "Topics:\n",
      "  Streamlit automatically handles ele-\n",
      "ments like input fields, buttons, and interactive widgets.\n",
      "\n",
      "  You can see that with a powerful framework for automation and problem-solving at your behest, \n",
      "you can compress work that can take hundreds of hours into minutes.\n",
      "  Entering new AgentExecutor chain...\n",
      "\n",
      "  Action: duckduckgo_search\n",
      "Action Input: \"mammal that lays the biggest eggs\"\n",
      "Observation: Posnov / Getty Images.\n",
      "  •\t\n",
      "Streamlit’s optimized performance for running models and data workflows \n",
      "ensures responsiveness even with large models.\n",
      "  •\t\n",
      "The result is an elegant web interface that lets users interact naturally with \n",
      "our LLM-powered agent.\n",
      "  Building Capable Assistants\n",
      "120\n",
      "Here’s the log output (shortened) for the correct reasoning:\n",
      ">\n",
      "  •\t\n",
      "Easy sharing and deployment options including open-source GitHub repos, \n",
      "personal Streamlit sharing links, and Streamlit Community Cloud.\n",
      "  •\t\n",
      "Streamlit apps run Python code in real time, enabling seamless connection \n",
      "to the agent’s backend API with no added latency.\n",
      "  Streamlit handles the complexity behind the scenes.\n",
      "\n",
      "  Our LangChain workflows \n",
      "integrate fluidly.\n",
      "\n",
      "Subtopics:\n",
      "  The western long-beaked echidna ...\n",
      "Final Answer: The platypus is the mammal that lays the biggest eggs.\n",
      "\n",
      "  The interface can \n",
      "be customized to match the domain.\n",
      "\n",
      "  You can play around with \n",
      "different research questions to see how the tools are used.\n",
      "  •\t\n",
      "Seamlessly integrate the agent’s capabilities into an app tailored for a specific \n",
      "use case, such as customer support or research assistance.\n",
      "  > Finished chain.\n",
      "\n",
      "  This al-\n",
      "lows instantly publishing and distributing the app.\n",
      "\n",
      "  Building a Streamlit app offers several key advantages:\n",
      "•\t\n",
      "Quickly create an intuitive graphical interface around our chatbot without \n",
      "having to build a complex frontend.\n",
      "  Our chatbot can scale grace-\n",
      "fully.\n",
      "\n",
      "  The actual implementation in the re-\n",
      "pository for the book allows you to try out different tools and has an option for self-verification.\n",
      "\n",
      "  I'm not sure, but I think I can find the answer by searching online.\n",
      "\n",
      "----------------------------------------\n",
      "Page 144:\n",
      "Topics:\n",
      "  Chapter 4\n",
      "121\n",
      "While our LLM app can provide answers to simple questions, its reasoning abilities are still limited. \n",
      "\n",
      "  An illustration of augmenting LLMs through tools and reasoning is shown here (source – https://\n",
      "github.com/billxbf/ReWOO, implementation for the paper Decoupling Reasoning from Observations \n",
      "for Efficient Augmented Language Models Resources, by Binfeng Xu and others, May 2023):\n",
      "\n",
      "  Hybrid systems that combine neural pattern completion with deliberate symbolic ma-\n",
      "nipulation can master skills including these:\n",
      "•\t\n",
      "Multi-step deductive reasoning to draw conclusions from a chain of facts\n",
      "•\t\n",
      "Mathematical reasoning like solving equations through a series of transformations\n",
      "•\t\n",
      "Planning tactics to break down a problem into an optimized sequence of actions\n",
      "By integrating tools together with explicit reasoning steps instead of pure pattern completion, \n",
      "our agent can tackle problems requiring abstraction and imagination, and can arrive at a com-\n",
      "plex understanding of the world enabling them to hold more meaningful conversations about \n",
      "complex concepts.\n",
      "\n",
      "Subtopics:\n",
      "  In the following section, we’ll implement more advanced types of agents.\n",
      "\n",
      "  Implementing more advanced reasoning strategies would make our research assistant far more \n",
      "capable.\n",
      "  Exploring reasoning strategies\n",
      "LLMs excel at pattern recognition in data but struggle with the symbolic reasoning required for \n",
      "complex multi-step problems.\n",
      "\n",
      "----------------------------------------\n",
      "Page 145:\n",
      "Topics:\n",
      "  The agent class uses the output of the LLMChain to decide which action to take.\n",
      "\n",
      "  In LangChain, this consists of three parts:\n",
      "•\t\n",
      "Tools\n",
      "•\t\n",
      "An LLMChain\n",
      "•\t\n",
      "The agent itself\n",
      "There are two key agent architectures:\n",
      "\n",
      "  Building Capable Assistants\n",
      "122\n",
      "Figure 4.5: Tool-augmented LLM paradigm\n",
      "The tools are the available resources that the agent can use, such as search engines or databases. \n",
      "\n",
      "Subtopics:\n",
      "  The LLMChain is responsible for generating text prompts and parsing the output to determine \n",
      "the next action.\n",
      "  While tool-augmented language models combine LLMs with external resources like search en-\n",
      "gines and databases to enhance reasoning capabilities, this can be further enhanced with agents.\n",
      "\n",
      "----------------------------------------\n",
      "Page 146:\n",
      "Topics:\n",
      "  •\t\n",
      "Plan-and-execute agents plan completely upfront before taking any action.\n",
      "\n",
      "  An alternative is plan-and-execute agents that first create a complete plan and then gather evi-\n",
      "dence to execute it.\n",
      "  P and E are combined and fed to the Solver LLM to generate the final output\n",
      "Plan-and-execute separates planning from execution.\n",
      "  We can see the reasoning with the observation pattern in the following diagram (source – https://\n",
      "arxiv.org/abs/2305.18323; Binfeng Xu and others, May 2023):\n",
      "Figure 4.6: Reasoning with observation\n",
      "\n",
      "  Chapter 4\n",
      "123\n",
      "•\t\n",
      "Action agents reason iteratively based on observations after each action.\n",
      "\n",
      "Subtopics:\n",
      "  Observations from tools are incorporated to inform the \n",
      "next reasoning step.\n",
      "  This approach is used in action agents.\n",
      "\n",
      "  The Planner LLM produces a list of plans (P).\n",
      "  The agent gathers evidence (E) \n",
      "using tools.\n",
      "  The trade-off is that plan-and-execute requires more upfront \n",
      "planning.\n",
      "\n",
      "  In observation-dependent reasoning, the agent iteratively provides context and examples to an \n",
      "LLM to generate thoughts and actions.\n",
      "  Smaller specialized models can be used \n",
      "for the Planner and Solver roles.\n",
      "----------------------------------------\n",
      "Page 147:\n",
      "Topics:\n",
      "  This strategy (in LangChain, called \n",
      "the plan-and-execute agent) is illustrated in the diagram here (source – https://arxiv.org/\n",
      "abs/2305.18323; Binfeng Xu and others, May 2023):\n",
      "Figure 4.7: Decoupling reasoning from observations\n",
      "\n",
      "  Building Capable Assistants\n",
      "124\n",
      "Observation-dependent reasoning involves making judgments, predictions, or choices based on \n",
      "the current state of knowledge or the evidence fetched through observation.\n",
      "  A user’s task is first combined with the context \n",
      "and examples and given to the LLM to initiate reasoning.\n",
      "  In each iteration, the \n",
      "agent provides context and examples to the LLM.\n",
      "  In LangChain, this is an action agent (also, Zero-Shot agent, \n",
      "ZERO_SHOT_REACT_DESCRIPTION), which is the default setting when you create an agent.\n",
      "\n",
      "  The observation is added to the prompt to \n",
      "initiate the next call to the LLM.\n",
      "  The LLM generates a thought and an \n",
      "action and then waits for an observation from tools.\n",
      "Subtopics:\n",
      "  As mentioned, plans can also be made ahead of any actions.\n",
      "----------------------------------------\n",
      "Page 148:\n",
      "Topics:\n",
      "  For plan-\n",
      "and-solve, we’ll define a planner and an executor, which we’ll use to create a PlanAndExecute \n",
      "agent executor:\n",
      "from typing import Literal\n",
      "from langchain.agents import initialize_agent, load_tools, AgentType\n",
      "from langchain.chains.base import Chain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain_experimental.plan_and_execute import (\n",
      "    load_chat_planner, load_agent_executor, PlanAndExecute\n",
      ")\n",
      "\n",
      "  This opens the possibility of using \n",
      "smaller, specialized models for Planner and Solver, and using fewer tokens for each of the calls.\n",
      "\n",
      "  2.\t\n",
      "For each step, determine the proper tools to accomplish the step and execute.\n",
      "\n",
      "  The Planner and the Solver can be distinct language models.\n",
      "  First, let’s add a strategy variable to the load_agent() function.\n",
      "  For zero-shot-react, the logic stays the same.\n",
      "  ReasoningStrategies = \"zero-shot-react\"\n",
      ") ->\n",
      "  We \n",
      "can write a pseudo algorithm like this:\n",
      "1.\t\n",
      "Plan out all the steps (Planner).\n",
      "\n",
      "  It can take two values, either \n",
      "plan-and-solve or zero-shot-react.\n",
      "  Chapter 4\n",
      "125\n",
      "The Planner (an LLM), which can be fine-tuned for planning and tool usage, produces a list of \n",
      "plans (P) and calls a worker (in LangChain, the agent) to gather evidence (E) by using tools.\n",
      "Subtopics:\n",
      "  P \n",
      "and E are combined with the task and then fed into the Solver (an LLM) for the final answer.\n",
      "  We can implement plan-and-solve in our research app; let’s do it!\n",
      "\n",
      "  ReasoningStrategies = Literal[\"zero-shot-react\", \"plan-and-solve\"]\n",
      "def load_agent(\n",
      "        tool_names: list[str],\n",
      "        strategy:\n",
      "  llm = ChatOpenAI(temperature=0, streaming=True)\n",
      "    tools = load_tools(\n",
      "        tool_names=tool_names,\n",
      "        llm=llm\n",
      "    )\n",
      "\n",
      "  Chain:\n",
      "    \n",
      "----------------------------------------\n",
      "Page 149:\n",
      "Topics:\n",
      "  Let’s define a new variable that’s set through a radio button in Streamlit.\n",
      "  We can execute this agent with Streamlit.\n",
      "  We should run the following command in our terminal:\n",
      "PYTHONPATH=. streamlit run question_answering/app.py\n",
      "\n",
      "  Please refer to the version on GitHub (within the question_answering package) for the full ver-\n",
      "sion.\n",
      "  We’ll pass this variable \n",
      "over to the load_agent() function:\n",
      "strategy = st.radio(\n",
      "    \"Reasoning strategy\",\n",
      "    (\"plan-and-solve\", \"zero-shot-react\")\n",
      ")\n",
      "\n",
      "  Building Capable Assistants\n",
      "126\n",
      "    if strategy == \"plan-and-solve\":\n",
      "        planner = load_chat_planner(llm)\n",
      "        \n",
      "  executor = load_agent_executor(llm, tools, verbose=True)\n",
      "        return PlanAndExecute(planner=planner, executor=executor, \n",
      "verbose=True)\n",
      "    return initialize_agent(\n",
      "        tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
      "verbose=True\n",
      "    )\n",
      "\n",
      "Subtopics:\n",
      "  [\"ddg-search\", \"wolfram-alpha\", \"wikipedia\"])\n",
      "Finally, still in the app, the agent is loaded like this:\n",
      "agent_chain = load_agent(tool_names=tool_names, strategy=strategy)\n",
      "\n",
      "  We can handle these by setting \n",
      "handle_parsing_errors in the initialize_agent() method.\n",
      "\n",
      "  For example, we might come across output parsing errors.\n",
      "  [\n",
      "        \"google-search\", \"ddg-search\", \"wolfram-alpha\", \"arxiv\",\n",
      "        \"wikipedia\", \"python_repl\", \"pal-math\", \"llm-math\"\n",
      "    ],\n",
      "    \n",
      "  You might have noticed that the load_agent() method takes a list of strings, tool_names.\n",
      "  This \n",
      "can be chosen in the user interface (UI) as well:\n",
      "tool_names = st.multiselect(\n",
      "    'Which tools do you want to use?',\n",
      "    \n",
      "----------------------------------------\n",
      "Page 150:\n",
      "Topics:\n",
      "  Implementing plan-and-execute in our research app\n",
      "Please have a look at the app in your browser and see the different steps for the question “What \n",
      "is a plan-and-solve agent in the context of LLM?”.\n",
      "\n",
      "  3.\t\n",
      "Understand the concept of a solve agent in the context of LLMs: A solve agent is an \n",
      "LLM that works as an agent.\n",
      "  If we open our browser on the indicated \n",
      "URL (by default, http://localhost:8501/), we should see the UI here:\n",
      "Figure 4.8:\n",
      "  Chapter 4\n",
      "127\n",
      "We should see how Streamlit starts up our application.\n",
      "  The steps look as follows – please note that the result might not be 100% accurate but this is what \n",
      "the agent comes up with:\n",
      "1.\t\n",
      "Define LLMs: LLMs are AI models that are trained on vast amounts of text data and can \n",
      "generate human-like text based on the input they receive.\n",
      "\n",
      "  2.\t\n",
      "Understand the concept of a plan in the context of LLMs:\n",
      "Subtopics:\n",
      "  In the context of large language \n",
      "models, a plan refers to a structured outline or set of steps that the model generates to \n",
      "solve a problem or answer a question.\n",
      "\n",
      "  It is responsible for generating plans to solve problems or \n",
      "answer questions.\n",
      "\n",
      "----------------------------------------\n",
      "Page 151:\n",
      "Topics:\n",
      "  Action:\n",
      "{\n",
      "\"action\": \"Wikipedia\",\n",
      "\"action_input\": \"large language models\"\n",
      "}\n",
      "We didn’t discuss another aspect of question answering, which is the prompting strategy used in \n",
      "these steps.\n",
      "  5.\t\n",
      "Given the above steps, respond to the user’s original question: In the context of large \n",
      "language models, a plan is a structured outline or set of steps generated by a solve agent \n",
      "to solve a problem or answer a question.\n",
      "  You can find a very advanced example of augmented information retrieval with LangChain in the \n",
      "BlockAGI project, which is inspired by BabyAGI and AutoGPT, at https://github.com/blockpipe/\n",
      "BlockAGI.\n",
      "\n",
      "  •\t\n",
      "Zero-shot CoT prompting elicits reasoning steps without examples by simply instructing \n",
      "the LLM to “think step by step.”\n",
      "•\t\n",
      "CoT prompting aims to aid understanding of reasoning processes through examples.\n",
      "\n",
      "  We’ll go into detail about prompting in Chapter 8, Customizing LLMs and Their Output, \n",
      "where we talk about prompting techniques, but very quickly, here’s an overview:\n",
      "•\t\n",
      "Few-shot chain-of-thought (CoT) prompting demonstrates step-by-step reasoning to \n",
      "guide the LLM through a thought process.\n",
      "\n",
      "  Building Capable Assistants\n",
      "128\n",
      "4.\t\n",
      "Recognize the importance of plans and solve agents in LLMs: Plans and solve agents \n",
      "help organize the model’s thinking process and provide a structured approach to prob-\n",
      "lem-solving or question-answering tasks.\n",
      "\n",
      "  Accordingly, the first step is to perform a look-up of LLMs:\n",
      "\n",
      "Subtopics:\n",
      "  A solve agent is a component of a large language \n",
      "model that is responsible for generating these plans.\n",
      "\n",
      "  However, \n",
      "they help improve the quality of generated reasoning steps, increase accuracy in problem-solving \n",
      "tasks, and enhance LLMs’ ability to handle various types of reasoning problems.\n",
      "\n",
      "  Additionally, while in plan-and-solve, complex tasks are broken down into subtask plans that are \n",
      "executed sequentially, this can be extended with more detailed instructions to improve reasoning \n",
      "quality, like emphasizing key variables and common sense.\n",
      "\n",
      "  This concludes our introduction to reasoning strategies.\n",
      "  All strategies have their problems, which \n",
      "can manifest as calculation errors, missing-step errors, and semantic misunderstandings.\n",
      "----------------------------------------\n",
      "Page 152:\n",
      "Topics:\n",
      "  We’ve implemented an app with \n",
      "Streamlit that can help answer research questions by relying on external tools such as search \n",
      "engines or Wikipedia.\n",
      "  The approaches unlock more capable, reliable AI assistants.\n",
      "  We’ve implemented a remarkably simple version of a CV parser as an \n",
      "example of this functionality that indicates how this could be applied.\n",
      "  We implemented a few simple approaches that help to \n",
      "make LLM outputs more accurate.\n",
      "  Unlike Retrieval augmented generation (RAG), which we’ll discuss in \n",
      "the next chapter and which uses vector search for semantic similarity, tools provide contextual \n",
      "augmentation by directly querying databases, APIs, and other structured external sources.\n",
      "  We implemented a plan-and-solve and a zero-shot \n",
      "agent in a Streamlit app.\n",
      "\n",
      "  For example, we’ll \n",
      "discuss reasoning with agents in much more detail in Chapter 6, Developing Software with Gener-\n",
      "ative AI, and Chapter 7, LLMs for Data Science, and provide an overview of prompting techniques \n",
      "in Chapter 8, Customizing LLMs and Their Output.\n",
      "\n",
      "  With \n",
      "LangChain, we can implement different agents that call tools.\n",
      "  Chapter 4\n",
      "129\n",
      "Summary\n",
      "In this chapter, we first talked about the problem of hallucinations and automatic fact-checking, \n",
      "and how to make LLMs more reliable.\n",
      "  Tools and function calling \n",
      "are not unique to OpenAI, however.\n",
      "Subtopics:\n",
      "  The OpenAI API implements functions, which we can use, among other things, for information \n",
      "extraction in documents.\n",
      "  Therefore, I dedicated a subsection to token usage.\n",
      "\n",
      "  We then looked at and implemented prompting strategies to \n",
      "break down and summarize documents.\n",
      "  Finally, we looked at different strategies employed by the agents to make decisions.\n",
      "  This can be immensely helpful for digesting large re-\n",
      "search articles or analyses.\n",
      "  While this chapter introduced many promising directions for developing capable and trustworthy \n",
      "LLMs, subsequent chapters will expand on the techniques developed here.\n",
      "  The main \n",
      "distinction is the point of decision-making.\n",
      "  Once we get into making a lot of chained calls to LLMs, this can mean \n",
      "we incur a lot of costs.\n",
      "  The \n",
      "factual information retrieved by tools supplements the chatbot’s internal context.\n",
      "\n",
      "  The evolution of instruction tuning, function calling, and tool \n",
      "usage enables models to move beyond freeform text generation into robustly automating tasks by \n",
      "interacting with real systems.\n",
      "----------------------------------------\n",
      "Page 153:\n",
      "Topics:\n",
      "  6.\t\n",
      "\n",
      "  How is instruction tuning related to function calling and tool usage?\n",
      "7.\t\n",
      "Give some examples of tools that are available in LangChain.\n",
      "\n",
      "  9.\t\n",
      "\n",
      "  8.\t Please define two agent paradigms.\n",
      "\n",
      "  I’d recommend you go back to the corresponding sections of this chapter if you are unsure about \n",
      "any of them:\n",
      "1.\t\n",
      "\n",
      "  How can we summarize documents with LLMs?\n",
      "2.\t \n",
      "  What is the chain of density?\n",
      "3.\t\n",
      "What are LangChain decorators and what’s the LangChain Expression Language?\n",
      "4.\t\n",
      "What is map-reduce in LangChain?\n",
      "5.\t\n",
      "\n",
      "  How does automated fact-checking work?\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "  What is Streamlit and why do we want to use it?\n",
      "10.\t \n",
      "  Building Capable Assistants\n",
      "130\n",
      "Questions\n",
      "Please have a look to see if you can come up with the answers to these questions from memory. \n",
      "\n",
      "Subtopics:\n",
      "  How can we count the tokens we are using (and why should we)?\n",
      "\n",
      "----------------------------------------\n",
      "Page 154:\n",
      "Topics:\n",
      "  In a nutshell, the main topics are:\n",
      "•\t\n",
      "What is a chatbot?\n",
      "•\t\n",
      "Understanding retrieval and vectors\n",
      "\n",
      "  Building on these \n",
      "core techniques, we will demonstrate practical RAG implementations using popular libraries like \n",
      "Milvus and Pinecone.\n",
      "  In this chapter, we explore how to overcome these limitations through \n",
      "Retrieval-Augmented Generation (RAG).\n",
      "  LangChain allows \n",
      "you to pass any text through a moderation chain to check whether it contains harmful content.\n",
      "\n",
      "  5\n",
      "Building a Chatbot like ChatGPT\n",
      "Chatbots powered by LLMs have demonstrated impressive fluency in conversational tasks like \n",
      "customer service.\n",
      "  Throughout the chapter, we’ll work on a chatbot implementation with an interface in Streamlit \n",
      "that you can find in the chat_with_retrieval directory in the GitHub repository for the book \n",
      "(https://github.com/benman1/generative_ai_with_langchain).\n",
      "\n",
      "  The key steps involve encoding corpora into vector embeddings to enable rapid semantic search \n",
      "and integrating retrieval results into the chatbot’s prompt.\n",
      "\n",
      "Subtopics:\n",
      "  This is achieved by \n",
      "retrieving relevant passages from corpora to condition the language model’s generation process. \n",
      "\n",
      "  We will also provide foundations for representing documents as vectors, indexing methods for \n",
      "efficient similarity lookups, and vector databases for managing embeddings.\n",
      "  By walking through end-to-end examples, we will showcase how RAG can \n",
      "significantly improve chatbots’ reasoning and factual correctness.\n",
      "  RAG enhances chatbots by grounding their responses in \n",
      "external evidence sources, leading to more accurate and informative answers.\n",
      "  Finally, we discuss another \n",
      "important topic from the reputational and legal perspective: moderation.\n",
      "  However, their lack of world knowledge limits their usefulness for domain-spe-\n",
      "cific question answering.\n",
      "----------------------------------------\n",
      "Page 155:\n",
      "Topics:\n",
      "  Despite limitations, it established a philosophical \n",
      "foundation for AI.\n",
      "  Early chatbots, like ELIZA (1966) and PARRY (1972), used pattern matching.\n",
      "  Building a Chatbot like ChatGPT\n",
      "132\n",
      "•\t\n",
      "Loading and retrieving in LangChain\n",
      "•\t\n",
      "Implementing a chatbot\n",
      "•\t\n",
      "Moderating responses\n",
      "We’ll begin the chapter by introducing chatbots and the state-of-the-art technology behind them.\n",
      "\n",
      "  The goal is no longer merely imitation but developing useful AI alongside a deeper \n",
      "comprehension of the inner workings of adaptive learning systems.\n",
      "\n",
      "  Still, the Turing test influenced the conversation on \n",
      "AI capabilities.\n",
      "\n",
      "  Today’s benchmarks thus focus more on testing specific task performance to probe the limits of \n",
      "LLMs like GPT-4.\n",
      "  The advent of LLMs like GPT-3 enabled more human-like chatbot systems such as ChatGPT (2022). \n",
      "\n",
      "  IBM Watson (2011) answered complex questions to beat Jeopardy! champions.\n",
      "  Chatbots analyze user input, understand the intent behind it, and generate appropriate responses. \n",
      "\n",
      "  Recent advances, \n",
      "like LLMs, allow more natural conversations, as seen in systems like ChatGPT (2022).\n",
      "  The Turing test, proposed in 1950, established a landmark for assessing intelligence by a comput-\n",
      "er’s ability to impersonate human conversation.\n",
      "  Siri \n",
      "(2011), as a voice-based assistant, pioneered integrating chatbots into everyday devices.\n",
      "  Systems \n",
      "like Google Duplex (2018) book appointments via phone conversations.\n",
      "\n",
      "  Philosophers like John Searle argued symbolic manipulation alone \n",
      "did not equate to human-level intelligence.\n",
      "  Chatbots are AI programs that simulate conversational interactions with users via text or voice. \n",
      "\n",
      "  However, early systems like ELIZA passed the test using scripted responses \n",
      "without true understanding, calling into question the test’s validity as an evaluation of AI.\n",
      "Subtopics:\n",
      "  However, \n",
      "challenges remain in achieving human-level discourse.\n",
      "\n",
      "  The \n",
      "test also faced criticism for relying on deceit and for limitations in its format that constrained the \n",
      "complexity of questioning.\n",
      "  Yet their abilities remain tightly constrained.\n",
      "  They can be designed to work with text-based messaging platforms or voice-based applications.\n",
      "\n",
      "  Understanding these boundaries is crucial for safe, beneficial \n",
      "applications.\n",
      "  What is a chatbot?\n",
      "\n",
      "  While ChatGPT displays remarkable coherence, its lack of grounding can result \n",
      "in plausible but incorrect responses.\n",
      "  True human discourse requires complex reasoning, \n",
      "pragmatics, common sense, and broad contextual knowledge.\n",
      "\n",
      "  Recent chatbots with more advanced natural language processing can better simulate conversa-\n",
      "tional depth.\n",
      "----------------------------------------\n",
      "Page 156:\n",
      "Topics:\n",
      "  •\t\n",
      "Education: In educational settings, virtual assistants are being explored as virtual tutors, \n",
      "helping students learn and assess their knowledge, answer questions, and deliver per-\n",
      "sonalized learning experiences.\n",
      "•\t\n",
      "HR and recruitment: Chatbots can assist in the recruitment process by screening candi-\n",
      "dates, scheduling interviews, and providing information about job openings.\n",
      "\n",
      "  Chapter 5\n",
      "133\n",
      "Some use cases for chatbots in customer service include providing 24/7 support, handling fre-\n",
      "quently asked questions, assisting with product recommendations, processing orders and pay-\n",
      "ments, and resolving simple customer issues.\n",
      "\n",
      "  •\t\n",
      "Virtual assistants: Chatbots can act as personal assistants, helping users with tasks like \n",
      "setting reminders, sending messages, or making phone calls.\n",
      "•\t\n",
      "Language learning: Chatbots can assist in language learning by providing interactive \n",
      "conversations and language practice.\n",
      "\n",
      "  •\t\n",
      "Mental health support: Chatbots can offer emotional support, provide resources, and \n",
      "engage in therapeutic conversations for mental health purposes.\n",
      "\n",
      "  Some more use cases of chatbots include:\n",
      "•\t\n",
      "Appointment scheduling: Chatbots can help users schedule appointments, book reser-\n",
      "vations, and manage their calendars.\n",
      "\n",
      "  •\t\n",
      "Information retrieval: Chatbots can provide users with specific information, such as \n",
      "weather updates, news articles, or stock prices.\n",
      "\n",
      "  •\t\n",
      "Entertainment: Chatbots can engage users in interactive games, quizzes, and storytelling \n",
      "experiences.\n",
      "•\t\n",
      "Law: Chatbots can be used to provide basic legal information, answer common legal \n",
      "questions, assist with legal research, and help users navigate legal processes.\n",
      "Subtopics:\n",
      "  They can \n",
      "also help with document preparation, such as drafting contracts or creating legal forms.\n",
      "\n",
      "  Chat technology in any field can make information more accessible and \n",
      "provide initial support to individuals seeking assistance.\n",
      "  With responsible development, chatbots hold promise for \n",
      "intuitive interfaces in customer service and other domains, even if human-level language mastery \n",
      "remains elusive.\n",
      "  They can improve clinical decision-making by providing \n",
      "relevant information and recommendations to healthcare professionals.\n",
      "\n",
      "  But their inability to reason or analyze \n",
      "limits roles requiring true intelligence.\n",
      "  These are just a few examples, and the use cases of chatbots continue to expand across various \n",
      "industries and domains.\n",
      "  •\t\n",
      "Medicine: Chatbots can assist with symptom checking, provide basic medical advice, and \n",
      "offer mental health support.\n",
      "  Ongoing research aims to develop safe, useful chatbot capabilities.\n",
      "\n",
      "----------------------------------------\n",
      "Page 157:\n",
      "Topics:\n",
      "  However, mastering context and reasoning remains an AI \n",
      "challenge to create proactive yet controllable assistants.\n",
      "  Overall, RAG and RALMs overcome the limits of language models’ memory by grounding respons-\n",
      "es in external information.\n",
      "  RALMs \n",
      "augment this by first retrieving relevant context from external corpora using semantic search \n",
      "algorithms.\n",
      "  Retrieval-Augmented Language Models (RALMs) specifically refer to retrieval-augmented lan-\n",
      "guage models that integrate retrieval into the training and inference process.\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "134\n",
      "There is an important distinction between chatbots that merely respond to explicit user prompts \n",
      "versus those with the more advanced ability to proactively initiate conversation and provide \n",
      "information without direct prompting.\n",
      "Subtopics:\n",
      "  This cycle repeats, with RALMs formulating queries dynamically, retrieving \n",
      "information on demand during generation.\n",
      "  Traditional language models generate text autoregressively based only on the prompt.\n",
      "  As we’ll explore more later, efficient storage and indexing of vector \n",
      "embeddings is crucial for enabling real-time semantic search over large document collections.\n",
      "\n",
      "  Understanding retrieval and vectors\n",
      "Retrieval-augmented generation (RAG) is a technique that enhances text generation by re-\n",
      "trieving and incorporating external knowledge.\n",
      "  However, proactive chatbots aim to anticipate \n",
      "needs and preferences based on prior interactions and contextual cues, taking the conversational \n",
      "initiative to address potential user questions preemptively.\n",
      "\n",
      "  Semantic search typically involves indexing documents into vector embeddings, \n",
      "allowing fast similarity lookups via approximate nearest neighbor search.\n",
      "\n",
      "  While responsive intentional chatbots can effectively fulfill precise user directions, proactive \n",
      "abilities hold the promise of more natural, efficient human-AI interaction by building loyalty \n",
      "and trust through anticipatory service.\n",
      "  Their capabilities continue advancing through optimizations in indexing methods, \n",
      "reasoning about retrieval timing, and fusing internal and external contexts.\n",
      "\n",
      "  This grounds the output in factual information \n",
      "rather than relying solely on the knowledge that is encoded in the language model’s parameters. \n",
      "\n",
      "  Active RALMs interleave retrieval and text creation, \n",
      "regenerating uncertain parts by fetching clarifying knowledge.\n",
      "\n",
      "  Intentional chatbots are designed to directly understand \n",
      "and fulfill specific user requests and intentions.\n",
      "  Current research is advancing chatbot \n",
      "abilities on both fronts, with the goal of balancing proactive dialog with responsiveness to user \n",
      "intent in fluid, purposeful conversation.\n",
      "\n",
      "  By incorporating outside knowledge, RALMs generate text that is more useful, nuanced, and fac-\n",
      "tually correct.\n",
      "  The retrieved evidence then conditions the language model to produce more accurate, contex-\n",
      "tually relevant text.\n",
      "----------------------------------------\n",
      "Page 158:\n",
      "Topics:\n",
      "  If the space \n",
      "is 3-dimensional, these could be vectors such as [0.5, 0.2, -0.1] for cat and [0.8, -0.3, \n",
      "0.6] for dog.\n",
      "  Through retrieval of relevant data, RAG helps in reducing hallucination \n",
      "responses from LLMs.\n",
      "  For example, an LLM used in a healthcare application could retrieve rele-\n",
      "vant medical information from external sources such as medical literature or databases during \n",
      "inference.\n",
      "  The distance between two embeddings indicates the \n",
      "semantic similarity between the corresponding concepts (the original content).\n",
      "\n",
      "  Chapter 5\n",
      "135\n",
      "By grounding LLMs with use-case-specific information through RAG, the quality and accuracy of \n",
      "responses are improved.\n",
      "  As for the OpenAI language embedding models, the embedding \n",
      "is a vector of 1,536 floating point numbers that represent the text.\n",
      "Subtopics:\n",
      "  We’ll start with the fundamentals of embeddings now.\n",
      "  Embeddings\n",
      "An embedding is a numerical representation of content in a way that machines can process and \n",
      "understand.\n",
      "  These vectors encode information about the relationships of these \n",
      "concepts with other words.\n",
      "  This retrieved data can then be incorporated into the context to enhance the generated \n",
      "responses and ensure they are accurate and aligned with domain-specific knowledge.\n",
      "\n",
      "  Once you understand \n",
      "embeddings, you’ll be able to build everything from search engines to chatbots.\n",
      "\n",
      "  They can represent words or sentences as numerical vectors (lists \n",
      "of float numbers).\n",
      "  As an example, let’s say we have the words cat and dog – these could be represented \n",
      "numerically in a space together with all other words in the vocabulary.\n",
      "  It \n",
      "is commonly used in applications such as recommendation systems, image and text search, and \n",
      "anomaly detection.\n",
      "  Roughly speaking, we would expect the concepts of cat \n",
      "and dog to be closer (more similar) to the concept of animal than to the concept of \n",
      "computer or embedding.\n",
      "\n",
      "  Since we are talking about vector storage, we need to discuss vector search, which is a technique \n",
      "used to search and retrieve vectors (or embeddings) based on their similarity to a query vector.\n",
      "  An embedding takes a piece of content, such as a word, sentence, or image, and maps \n",
      "it into a multi-dimensional vector space.\n",
      "  The essence of the process is to convert an object such as an image or some text into \n",
      "a vector that encapsulates its semantic content while discarding irrelevant details as much as \n",
      "possible.\n",
      "  These numbers \n",
      "are derived from a sophisticated language model that captures semantic content.\n",
      "\n",
      "  Embeddings are representations of data objects generated by machine learning mod-\n",
      "els to represent.\n",
      "----------------------------------------\n",
      "Page 159:\n",
      "Topics:\n",
      "  When a pre-trained CNN then runs over a new image, it can \n",
      "output an embedding vector.\n",
      "\n",
      "  This approach, which in the scikit-learn library is implemented as CountVectorizer, was \n",
      "popular until word2vec came about.\n",
      "  Building a Chatbot like ChatGPT\n",
      "136\n",
      "Embeddings can be created using different methods.\n",
      "  Nowadays, often, convolutional neural networks (CNNs) are pre-trained on large \n",
      "datasets (like ImageNet) to learn a good representation of the image’s properties.\n",
      "  The general \n",
      "idea of embeddings is illustrated in the following figure (source: “Analogies Explained: Towards \n",
      "Understanding Word Embeddings” by Carl Allen and Timothy Hospedales, 2019; https://arxiv.\n",
      "org/abs/1901.09813):\n",
      "Figure 5.1:\n",
      "  For texts, one simple method is the bag-of-\n",
      "words approach, where each word is represented by a count of how many times it appears in a \n",
      "text.\n",
      "Subtopics:\n",
      "  Word2vec word embeddings in a 3D space\n",
      "As for images, embeddings could come from feature extraction stages such as edge detection, \n",
      "texture analysis, and color composition.\n",
      "  Word2vec, which – roughly speaking – learns embeddings \n",
      "by predicting the words in a sentence based on other surrounding words ignoring the word order \n",
      "in a linear model.\n",
      "\n",
      "  These features can be extracted over different window \n",
      "sizes to make the representations both scale-invariant and shift-invariant (scale-space repre-\n",
      "sentations).\n",
      "  We can perform simple vector arithmetic with these vectors, for example, the vector for king \n",
      "minus man plus the vector for woman gives us a vector that comes close to queen.\n",
      "  Since convo-\n",
      "lutional layers apply a series of filters (or kernels) on the input image to produce a feature map, \n",
      "conceptually this is like scale-space.\n",
      "----------------------------------------\n",
      "Page 160:\n",
      "Topics:\n",
      "  In LangChain, you can obtain an embedding by using the embed_query() method from any em-\n",
      "bedding class, for example, from the the OpenAIEmbeddings class.\n",
      "  Chapter 5\n",
      "137\n",
      "Today, for most domains including texts and images, embeddings usually come from trans-\n",
      "former-based models, which consider the context and order of the words in a sentence and the \n",
      "paragraph.\n",
      "  I am assuming you have \n",
      "set the API key as an environment variable, as recommended in Chapter 3, Getting Started with \n",
      "LangChain.\n",
      "\n",
      "  query_result = embeddings.embed_query(text) \n",
      "print(query_result)\n",
      "print(len(query_result))\n",
      "\n",
      "Subtopics:\n",
      "  doc_vectors = embeddings.embed_documents(words)\n",
      "\n",
      "  Here is an example:\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings \n",
      "words =\n",
      "  All these models are trained on large \n",
      "datasets to establish the concepts and their relationships.\n",
      "\n",
      "  By representing data objects as numerical vectors, \n",
      "we can perform mathematical operations on them and measure their similarity or use them as \n",
      "input for other machine learning models.\n",
      "  For example, we could be performing a simple sentiment classifier by checking if embeddings of \n",
      "product reviews are closer to the concept of positive or negative.\n",
      "\n",
      "  You can also obtain embeddings for multiple document inputs using the embed_documents() \n",
      "method.\n",
      "  The length of the embedding \n",
      "(the number of dimensions) can be obtained using the len() function.\n",
      "  These embeddings can be used in various tasks.\n",
      "  [\"cat\", \"dog\", \"computer\", \"animal\"]\n",
      "embeddings = OpenAIEmbeddings()\n",
      "\n",
      "  By calculating distances between embeddings, we can \n",
      "perform tasks like search and similarity scoring, or classify objects, for example by topic or category. \n",
      "\n",
      "  Here is an example code snippet:\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings \n",
      "embeddings = OpenAIEmbeddings() \n",
      "text = \"This is a sample query.\" \n",
      "\n",
      "  The result is stored in the query_result variable.\n",
      "  This code passes a single string input to the embed_query method and retrieves the corresponding \n",
      "text embedding.\n",
      "  Based on the model architecture, most importantly the number of parameters, these \n",
      "models can capture extraordinarily complex relationships.\n",
      "----------------------------------------\n",
      "Page 161:\n",
      "Topics:\n",
      "  The distance plot should look like this:\n",
      "Figure 5.2:\n",
      "  DataFrame(\n",
      "    data=dists,\n",
      "    index=words,\n",
      "    columns=words\n",
      ")\n",
      "df.style.background_gradient(cmap='coolwarm')\n",
      "\n",
      "  Although these questions \n",
      "can be important in certain applications, let’s bear in mind that this is a simple example.\n",
      "\n",
      "  We can also do arithmetic between these embeddings; for example, we can calculate distances \n",
      "between them:\n",
      "from scipy.spatial.distance import pdist, squareform\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "X = np.array(doc_vectors)\n",
      "dists = squareform(pdist(X))\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "138\n",
      "In this case, the embed_documents() method is used to retrieve embeddings for multiple text \n",
      "inputs.\n",
      "  This gives us the Euclidean distances between our words as a square matrix.\n",
      "Subtopics:\n",
      "  Euclidean distances between embeddings of the words cat, dog, computer, and \n",
      "animal\n",
      "We can confirm: a cat and a dog are indeed closer to an animal than to a computer.\n",
      "  The result is stored in the doc_vectors variable.\n",
      "  Let’s plot them:\n",
      "import pandas as pd\n",
      "df = pd.\n",
      "  We could have retrieved embeddings for \n",
      "long documents – instead, we’ve retrieved the vectors only for each single word.\n",
      "\n",
      "  There could \n",
      "be many questions here, for example, if a dog is more an animal than a cat, or why a dog and a \n",
      "cat are only a little more distant from a computer than from an animal.\n",
      "----------------------------------------\n",
      "Page 162:\n",
      "Topics:\n",
      "  Vector search is commonly used in various applications such as recommen-\n",
      "dation systems, image and text search, and similarity-based retrieval.\n",
      "  Vector storage\n",
      "As mentioned, in vector search, each data point is represented as a vector in a high-dimensional \n",
      "space.\n",
      "  Vector storage refers to the mechanism used to store vector embeddings and is also relevant to \n",
      "how those vector embeddings can be retrieved.\n",
      "  Chapter 5\n",
      "139\n",
      "In these examples, we’ve used OpenAI embeddings – in the examples further on, we’ll use embed-\n",
      "dings from models served by Hugging Face.\n",
      "  Additionally, LangChain provides a FakeEmbeddings class that can be used to test your pipeline \n",
      "without making actual calls to the embedding providers.\n",
      "\n",
      "  There are a few integrations and tools in LangChain \n",
      "that can help with this process, some of which we’ll encounter later on in this chapter.\n",
      "\n",
      "  The distance \n",
      "between vectors can be computed using distance metrics like cosine similarity or Euclidean dis-\n",
      "tance.\n",
      "  Vector search refers to the process of searching for similar vectors among other stored \n",
      "vectors, for example, in a vector database, based on their similarity to a given query \n",
      "vector.\n",
      "Subtopics:\n",
      "  The vectors capture the features or characteristics of the data points.\n",
      "  In vector search, every data object in a dataset is assigned a vector embedding.\n",
      "  To perform a vector search, the query vector (representing the search query) is compared \n",
      "to every vector in the collection.\n",
      "  In the context of this chapter, we’ll use them for retrieval of related information (semantic search). \n",
      "\n",
      "  The goal of \n",
      "vector search is to efficiently and accurately retrieve vectors that are most similar \n",
      "to the query vector, typically using similarity measures such as the dot product or \n",
      "cosine similarity.\n",
      "\n",
      "  These embeddings \n",
      "are arrays of numbers that can be used as coordinates in a high-dimensional space.\n",
      "  The distance between the query vector and each vector in the \n",
      "collection is calculated, and objects with smaller distances are considered more similar.\n",
      "\n",
      "  Vector storage can be a standalone solution that \n",
      "is specifically designed to store and retrieve vector embeddings efficiently.\n",
      "  However, we still need to talk about the integration of these embeddings into apps and broader \n",
      "systems, and this is where vector storage comes in.\n",
      "\n",
      "  To perform vector search efficiently, vector storage mechanisms are used such as vector databases.\n",
      "\n",
      "  On the other hand, \n",
      "vector databases are purpose-built to manage vector embeddings and provide several advantages \n",
      "over using standalone vector indices like Faiss.\n",
      "\n",
      "  The goal is to find \n",
      "the most similar vectors to a given query vector.\n",
      "\n",
      "----------------------------------------\n",
      "Page 163:\n",
      "Topics:\n",
      "  Understanding these fundamentals should make it intuitive to work with RAG.\n",
      "\n",
      "  Vector indexing\n",
      "Indexing in the context of vector embeddings is a method of organizing data to optimize its re-\n",
      "trieval and/or storage.\n",
      "  Some \n",
      "of them include:\n",
      "•\t\n",
      "Product quantization (PQ): PQ is a technique that divides the vector space into smaller \n",
      "subspaces and quantizes each subspace separately.\n",
      "  3.\t\n",
      "Vector databases like Milvus or Pinecone are designed to store, manage, and retrieve large \n",
      "sets of vectors.\n",
      "  There are three levels to this:\n",
      "1.\t\n",
      "Indexing organizes vectors to optimize retrieval, structuring them so that vectors can be \n",
      "retrieved quickly.\n",
      "  Building a Chatbot like ChatGPT\n",
      "140\n",
      "Let’s dive into a few of these concepts a bit more.\n",
      "  2.\t\n",
      "Vector libraries provide functions for vector operations like dot product and vector in-\n",
      "dexing.\n",
      "\n",
      "  A typical algorithm applied in this context is k-dimensional trees (k-d trees), but many others, \n",
      "like ball trees, Annoy, and Faiss, are often implemented, especially for high-dimensional vectors, \n",
      "which traditional methods can struggle with.\n",
      "\n",
      "Subtopics:\n",
      "  Let’s look at these in turn to understand the fundamentals of working \n",
      "with embeddings.\n",
      "  There are different algorithms like k-d trees or Annoy for this.\n",
      "\n",
      "  There are several other types of algorithms commonly used for similarity search indexing.\n",
      "  This reduces the dimensionality of \n",
      "the vectors and allows for efficient storage and search.\n",
      "  In \n",
      "k-d trees, a binary tree structure is built up that partitions the data points based on their \n",
      "feature values.\n",
      "  In ball trees, a tree structure that partitions the data points into \n",
      "nested hyperspheres.\n",
      "  For vector embeddings, indexing aims to structure the \n",
      "vectors – roughly speaking – so that similar vectors are stored next to each other, enabling fast \n",
      "proximity or similarity searches.\n",
      "\n",
      "  These components work together for the creation, manipulation, storage, and efficient retrieval \n",
      "of vector embeddings.\n",
      "  They use indexing mechanisms to facilitate efficient similarity searches \n",
      "on these vectors.\n",
      "\n",
      "  It is efficient for low-dimensional data but becomes less effective as the \n",
      "dimensionality increases.\n",
      "  It is suitable for high-dimensional data but can be slower than k-d \n",
      "trees for low-dimensional data.\n",
      "\n",
      "  Examples of PQ are k-d trees and ball trees.\n",
      "  PQ is known for its fast search \n",
      "speed but may sacrifice some accuracy.\n",
      "  It’s similar to the concept in traditional database systems, where indexing \n",
      "allows quicker access to data records.\n",
      "----------------------------------------\n",
      "Page 164:\n",
      "Topics:\n",
      "  •\t\n",
      "Apart from HNSW and KNN, there are other graph-based methods, like Graph Neural \n",
      "Networks (GNNs) and Graph Convolutional Networks (GCNs), that leverage graph \n",
      "structures for similarity search.\n",
      "\n",
      "  algorithm is a popular LSH algorithm that uses random \n",
      "projection trees to index vectors.\n",
      "  Chapter 5\n",
      "141\n",
      "•\t\n",
      "Locality sensitive hashing (LSH): This is a hashing-based method that maps similar \n",
      "data points to the same hash buckets.\n",
      "  These libraries use the Approximate Near-\n",
      "est Neighbor (ANN) algorithm to efficiently search through vectors and find the most similar \n",
      "ones.\n",
      "  They typically offer different implementations of the ANN algorithm, such as clustering or \n",
      "tree-based methods, and allow users to perform vector similarity search for various applications.\n",
      "\n",
      "  Vector libraries\n",
      "Vector libraries, like Facebook (Meta) Faiss or Spotify Annoy, provide functionality for working \n",
      "with vector data.\n",
      "Subtopics:\n",
      "  These indexing algorithms have different trade-offs in terms of search speed, accuracy, and mem-\n",
      "ory usage.\n",
      "  The Annoy (Approximate \n",
      "Nearest Neighbors\n",
      "  Annoy is simple to use and provides fast approximate \n",
      "nearest neighbor search.\n",
      "\n",
      "  •\t\n",
      "Hierarchical navigable small world (HNSW): HNSW is a graph-based indexing algorithm \n",
      "that constructs a hierarchical graph structure to organize the vectors.\n",
      "  It constructs a binary tree structure where each node \n",
      "represents a random hyperplane.\n",
      "  HNSW is known for its high search accuracy and scalability.\n",
      "\n",
      "  In the context of vector search, a vector library is specifically designed to store \n",
      "and perform similarity search on vector embeddings.\n",
      "  It uses a combination \n",
      "of randomization and greedy search to build a navigable network, allowing for efficient \n",
      "nearest-neighbor search.\n",
      "  Oh Yeah)\n",
      "  It is efficient for high-dimensional data but may \n",
      "have a higher probability of false positives and false negatives.\n",
      "  The choice of algorithm depends on the specific requirements of the application and \n",
      "the characteristics of the vector data.\n",
      "\n",
      "----------------------------------------\n",
      "Page 165:\n",
      "Topics:\n",
      "  It offers various \n",
      "indexing algorithms, including PQ, LSH, and HNSW.\n",
      "  •\t\n",
      "Annoy is a C++ library for approximate nearest neighbor search in high-dimensional spaces \n",
      "maintained and developed by Spotify implementing the Annoy algorithm.\n",
      "  Building a Chatbot like ChatGPT\n",
      "142\n",
      "Here’s a quick overview of some open-source libraries for vector storage that shows their popu-\n",
      "larity in terms of GitHub stars over time (source: star-history.com):\n",
      "Figure 5.3:\n",
      "  Faiss is widely used for large-scale \n",
      "vector search tasks and supports both CPU and GPU acceleration.\n",
      "\n",
      "  •\t\n",
      "nmslib (Non-Metric Space Library) is an open-source library that provides efficient sim-\n",
      "ilarity search in non-metric spaces.\n",
      "  Annoy comes second.\n",
      "  •\t\n",
      "hnswlib is a C++ library for approximate nearest-neighbor search using the HNSW algo-\n",
      "rithm.\n",
      "  Star history for several popular open-source vector libraries\n",
      "You can see that Faiss has been starred a lot by GitHub users.\n",
      "  It supports various indexing algorithms like HNSW, \n",
      "SW-graph, and SPTAG.\n",
      "\n",
      "  Let’s quickly go through these:\n",
      "•\t\n",
      "Faiss (Facebook AI Similarity Search) is a library developed by Meta (previously Facebook) \n",
      "that provides efficient similarity search and clustering of dense vectors.\n",
      "Subtopics:\n",
      "  Others have \n",
      "not found the same popularity yet.\n",
      "\n",
      "  It is designed \n",
      "to be efficient and scalable, making it suitable for large-scale vector data.\n",
      "  It works with \n",
      "a forest of random projection trees.\n",
      "\n",
      "  It provides fast and memory-efficient indexing and search capabilities for high-di-\n",
      "mensional vector data.\n",
      "\n",
      "----------------------------------------\n",
      "Page 166:\n",
      "Topics:\n",
      "  It offers additional features such as data management, metadata storage and filtering, \n",
      "and scalability.\n",
      "  •\t\n",
      "Natural Language Processing (NLP): Vector databases are widely used in NLP tasks such \n",
      "as sentiment analysis, text classification, and semantic search.\n",
      "  Chapter 5\n",
      "143\n",
      "•\t\n",
      "SPTAG by Microsoft implements a distributed ANN.\n",
      "  Both nmslib and hnswlib are maintained by Leo Boytsov, who works as a senior research scientist \n",
      "at Amazon, and Yury Malkov.\n",
      "  Other use cases for vector databases are continually expanding as the technology evolves; however, \n",
      "some common use cases for vector databases include:\n",
      "•\t\n",
      "Anomaly detection: Vector databases can be used to detect anomalies in large datasets \n",
      "by comparing the vector embeddings of data points.\n",
      "Subtopics:\n",
      "  You can see an overview at https://\n",
      "github.com/erikbern/ann-benchmarks\n",
      "Vector databases\n",
      "A vector database is designed to handle vector embeddings, making it easier to search and query \n",
      "data objects.\n",
      "  While vector storage focuses solely on storing and retrieving vector embeddings, \n",
      "a vector database provides a more comprehensive solution for managing and querying vector \n",
      "data.\n",
      "  Vector databases can be used to create personalized recommendation \n",
      "systems by finding similar vectors based on user preferences or behavior.\n",
      "\n",
      "  This can be valuable in fraud de-\n",
      "tection, network security, or monitoring systems where identifying unusual patterns or \n",
      "behaviors is crucial.\n",
      "\n",
      "  There are a lot more libraries.\n",
      "  The primary application is similarity search (also semantic search), where we can \n",
      "efficiently search through large volumes of text, images, or videos, identifying objects matching \n",
      "the query based on the vector representation.\n",
      "  Vector databases can be particularly useful for applications that involve copious amounts \n",
      "of data and require flexible and efficient search capabilities across several types of vectorized data, \n",
      "such as text, images, audio, video, and more.\n",
      "\n",
      "  By representing text as \n",
      "vector embeddings, it becomes easier to compare and analyze textual data.\n",
      "\n",
      "  This is particularly useful in applications such as \n",
      "document search, reverse image search, and recommendation systems.\n",
      "\n",
      "  Vector databases can be used to store and serve machine learning models and their corresponding \n",
      "embeddings.\n",
      "  •\t\n",
      "Personalization:\n",
      "  It comes with a k-d tree and rela-\n",
      "tive neighborhood graph (SPTAG-KDT), as well as a balanced k-means tree and relative \n",
      "neighborhood graph (SPTAG-BKT).\n",
      "\n",
      "----------------------------------------\n",
      "Page 167:\n",
      "Topics:\n",
      "  The popular Postgres has an extension for efficient vector \n",
      "search: pg_embedding.\n",
      "  •\t\n",
      "Enable advanced search capabilities: With vector databases, it becomes possible to build \n",
      "powerful search engines that can search for similar vectors or embeddings.\n",
      "  •\t\n",
      "Specialized for specific tasks: Vector databases are designed to perform a specific task, \n",
      "such as finding close embeddings.\n",
      "  For example, MongoDB, Cockroach, Neo4J, and Influx \n",
      "are all examples of successful companies that introduced innovative database technologies and \n",
      "achieved substantial market share.\n",
      "  The characteristics of vector databases include:\n",
      "•\t\n",
      "Efficient retrieval of similar vectors: Vector databases excel at finding close embeddings \n",
      "or similar points in a high-dimensional space.\n",
      "  •\t\n",
      "Support for high-dimensional spaces: Vector databases can handle vectors with thou-\n",
      "sands of dimensions, allowing for complex representations of data.\n",
      "  Firstly, artificial intelligence (AI) and data management have become crucial for businesses, \n",
      "leading to a high demand for advanced database solutions.\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "144\n",
      "These databases are popular because they are optimized for scalability and representing and \n",
      "retrieving data in high-dimensional vector spaces.\n",
      "  HNSW provides a faster and more efficient alternative to the pgvector \n",
      "extension with IVFFlat indexing.\n",
      "\n",
      "Subtopics:\n",
      "  They are not general-purpose databases and are tailored \n",
      "to handle substantial amounts of vector data efficiently.\n",
      "\n",
      "  This is crucial for tasks \n",
      "like natural language processing or image recognition.\n",
      "\n",
      "  These market creators often dominate the industry, attracting significant in-\n",
      "vestments from venture capitalists (VCs).\n",
      "  Overall, vector databases offer a specialized and efficient solution for handling large-dimensional \n",
      "vector data, enabling tasks like similarity search and advanced search capabilities.\n",
      "\n",
      "  In the database market, there is a history of new types of databases emerging and creating new \n",
      "market categories.\n",
      "  This makes them ideal for tasks like reverse \n",
      "image search or similarity-based recommendations.\n",
      "\n",
      "  This opens \n",
      "possibilities for applications like content recommendation systems or semantic search.\n",
      "\n",
      "  Traditional databases are not designed to \n",
      "efficiently handle large-dimensional vectors, such as those used to represent images or text em-\n",
      "beddings.\n",
      "\n",
      "  The market for open-source software and databases is currently thriving due to several factors. \n",
      "\n",
      "----------------------------------------\n",
      "Page 168:\n",
      "Topics:\n",
      "  Chapter 5\n",
      "145\n",
      "Some examples of vector databases are listed in Table 5.1.\n",
      "  SaaS \n",
      "2022\n",
      "Apache-2.0\n",
      "HNSW\n",
      "Chroma \n",
      "Inc\n",
      "Qdrant\n",
      "Managed/\n",
      "self-hosted vector \n",
      "search engine and \n",
      "database with \n",
      "extended filtering \n",
      "support\n",
      "(Partly \n",
      "open) \n",
      "\n",
      "  SaaS \n",
      "2021\n",
      "Apache 2.0\n",
      "HNSW\n",
      "Qdrant \n",
      "Solutions \n",
      "GmbH\n",
      "Milvus\n",
      "Vector database \n",
      "built for scalable \n",
      "similarity search\n",
      "(Partly \n",
      "open) \n",
      "\n",
      "  SaaS\n",
      "2019\n",
      "BSD\n",
      "IVF, \n",
      "HNSW, \n",
      "PQ, \n",
      "and \n",
      "more\n",
      "Zilliz\n",
      "Weaviate\n",
      "Cloud-native vec-\n",
      "tor database that \n",
      "stores both objects \n",
      "and vectors\n",
      "Open \n",
      "SaaS\n",
      "Started in \n",
      "2018 as a \n",
      "tradition-\n",
      "al graph \n",
      "database, \n",
      "first re-\n",
      "leased in \n",
      "2019\n",
      "BSD\n",
      "Cus-\n",
      "tom \n",
      "HNSW \n",
      "algo-\n",
      "rithm \n",
      "that \n",
      "sup-\n",
      "ports \n",
      "CRUD\n",
      "SeMI \n",
      "Technolo-\n",
      "gies\n",
      "\n",
      "  Database \n",
      "provider\n",
      "Description\n",
      "Busi-\n",
      "ness \n",
      "model\n",
      "First re-\n",
      "leased\n",
      "License\n",
      "Index-\n",
      "ing\n",
      "Organiza-\n",
      "tion\n",
      "Chroma\n",
      "Commercial open-\n",
      "source embedding \n",
      "store\n",
      "(Partly \n",
      "open) \n",
      "\n",
      "  •\t\n",
      "License: Whether it is open- or closed-source.\n",
      "\n",
      "  •\t\n",
      "Indexing: The algorithmic approach to similarity/vector search taken by this search engine \n",
      "and its unique capabilities.\n",
      "\n",
      "Subtopics:\n",
      "  I took the liberty of highlighting for \n",
      "each search engine the following perspectives:\n",
      "•\t\n",
      "Value proposition: What is the unique feature that sets this vector search engine apart \n",
      "from others?\n",
      "•\t\n",
      "Business model: The general type of the engine, whether it’s a vector database, big data \n",
      "platform, or managed/self-hosted.\n",
      "\n",
      "----------------------------------------\n",
      "Page 169:\n",
      "Topics:\n",
      "  Building a Chatbot like ChatGPT\n",
      "146\n",
      "Pinecone\n",
      "Fast and scalable \n",
      "applications using \n",
      "embeddings from \n",
      "AI models\n",
      "SaaS\n",
      "First \n",
      "released in \n",
      "2019\n",
      "Propri-\n",
      "etary\n",
      "Built \n",
      "on top \n",
      "of Faiss\n",
      "Pinecone \n",
      "Systems \n",
      "Inc\n",
      "Vespa\n",
      "Commercial \n",
      "open-source vector \n",
      "database that sup-\n",
      "ports vector search, \n",
      "lexical search, and \n",
      "search\n",
      "Open \n",
      "SaaS\n",
      "Originally \n",
      "a web \n",
      "search en-\n",
      "gine (all-\n",
      "theweb), \n",
      "acquired \n",
      "by Yahoo! \n",
      "in 2003, \n",
      "and later \n",
      "developed \n",
      "into and \n",
      "open-\n",
      "sourced as \n",
      "Vespa in \n",
      "2017\n",
      "Apache 2.0\n",
      "HNSW, \n",
      "BM25\n",
      "Yahoo!\n",
      "Marqo\n",
      "Cloud-native \n",
      "commercial open-\n",
      "source search and \n",
      "analytics engine\n",
      "Open \n",
      "SaaS\n",
      "2022\n",
      "Apache 2.0\n",
      "HNSW\n",
      "S2Search \n",
      "Australia \n",
      "Pty Ltd\n",
      "Table 5.1:\n",
      "  I’ve omitted many solutions, \n",
      "such as FaissDB and Hasty.ai, and focused on a few ones that are integrated into LangChain.\n",
      "\n",
      "  For the open-source databases, the GitHub star histories give a good idea of their popularity and \n",
      "traction.\n",
      "Subtopics:\n",
      "  Vector databases\n",
      "In the preceding table, I’ve left out other aspects such as architecture, support for sharding, and \n",
      "in-memory processing.\n",
      "  Here’s the plot over time (source: star-history.com):\n",
      "\n",
      "  There are many vector database providers.\n",
      "----------------------------------------\n",
      "Page 170:\n",
      "Topics:\n",
      "  Star history of open-source vector databases on GitHub\n",
      "You can see that milvus is immensely popular; however, other libraries such as qdrant, weviate, \n",
      "and chroma have been catching up.\n",
      "\n",
      "  Import the necessary modules:\n",
      "from langchain.vectorstores import Chroma \n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "2.\t\n",
      "Create an instance of Chroma and provide the documents (splits) and the embedding \n",
      "method:\n",
      "vectorstore = Chroma.from_documents(documents=docs, \n",
      "embedding=OpenAIEmbeddings())\n",
      "\n",
      "  Chapter 5\n",
      "147\n",
      "Figure 5.4:\n",
      "  To use Chroma in LangChain, you need to follow these steps:\n",
      "1.\t\n",
      "\n",
      "  Let’s see an example of \n",
      "a vector store implementation in LangChain!\n",
      "\n",
      "  Chroma\n",
      "This vector store is optimized for storing and querying vectors using Chroma as a backend.\n",
      "  In LangChain, vector storage can be implemented using the vectorstores module.\n",
      "Subtopics:\n",
      "  This module \n",
      "provides various classes and methods for storing and querying vectors.\n",
      "  Chroma \n",
      "takes over for encoding and comparing vectors based on their angular similarity.\n",
      "\n",
      "----------------------------------------\n",
      "Page 171:\n",
      "Topics:\n",
      "  Building a Chatbot like ChatGPT\n",
      "148\n",
      "3.\t\n",
      "We can query the vector store to retrieve similar vectors:\n",
      "similar_vectors = vector_store.query(query_vector, k)\n",
      "\n",
      "  This will load and chunk up the paper about Mistal 7B. Please note that the \n",
      "download will be a PDF, and you’ll need to have the pymupdf library installed. \n",
      "\n",
      "  However, for \n",
      "sake of completeness, you can get the docs argument for the preceding chro-\n",
      "ma vector store like this:\n",
      "from langchain.document_loaders import ArxivLoader\n",
      "from langchain.text_splitter import \n",
      "CharacterTextSplitter\n",
      "loader = ArxivLoader(query=\"2310.06825\")\n",
      "documents = loader.load()\n",
      "text_splitter = CharacterTextSplitter(chunk_size=1000, \n",
      "chunk_overlap=0)\n",
      "docs = text_splitter.split_documents(documents)\n",
      "\n",
      "  In this section, we’ll look at how we can put them together in a pipeline for building a chatbot \n",
      "with RAG.\n",
      "  Loading and retrieving in LangChain\n",
      "LangChain implements a toolchain of different building blocks for building retrieval systems. \n",
      "\n",
      "  In practice, \n",
      "there are two building blocks for us to pick up if we want to build a chatbot, most importantly \n",
      "document loaders and retrievers, both of which we’ll look at now.\n",
      "\n",
      "  The documents (or splits, as seen in Chapter 5, Building a Chatbot Like \n",
      "ChatGPT) will be embedded and stored in the Chroma vector database.\n",
      "Subtopics:\n",
      "  In this section, we’ve learned a lot of the basics of embeddings and vector stores.\n",
      "  We’ll \n",
      "discuss document loaders in another section of this chapter.\n",
      "  Here, query_vector is the vector you want to find similar vectors to, and k is the number of similar \n",
      "vectors you want to retrieve.\n",
      "\n",
      "  This includes data loaders, document transformers, embedding models, vector stores, \n",
      "and retrievers.\n",
      "\n",
      "  We’ve also seen \n",
      "how to work with embeddings and documents in vector stores and vector databases.\n",
      "----------------------------------------\n",
      "Page 172:\n",
      "Topics:\n",
      "  You can use the LangChain integration hub to browse and select \n",
      "the appropriate loader for your data source.\n",
      "  Document loaders\n",
      "Document loaders are used to load data from a source as Document objects, which consist of \n",
      "text and associated metadata.\n",
      "  There are several types of integrations available, such as document \n",
      "loaders for loading a simple .txt file (TextLoader), loading the text contents of a web page \n",
      "(WebBaseLoader), loading articles from Arxiv (ArxivLoader), or loading a transcript of a YouTube \n",
      "video (YoutubeLoader).\n",
      "  Retrievers in LangChain can wrap the loading and vector \n",
      "storage into a single step.\n",
      "  Let’s look at document loaders in LangChain!\n",
      "  In the actual pipeline of implementing RAG, these \n",
      "come as the first step.\n",
      "\n",
      "  For webpages, the Diffbot integration gives a clean extraction of the con-\n",
      "tent.\n",
      "  In LangChain, we can load our documents from many sources and in a bunch of formats through \n",
      "the integrated document loaders.\n",
      "  Chapter 5\n",
      "149\n",
      "The relationship between them is illustrated in the diagram here (source: LangChain documen-\n",
      "tation):\n",
      "Figure 5.5: Vector stores and data loaders\n",
      "In LangChain, we first load documents through data loaders.\n",
      "Subtopics:\n",
      "  We’ll mostly skip transformations in this chapter; however, you’ll find \n",
      "explanations with examples of data loaders, embeddings, storage mechanisms, and retrievers.\n",
      "\n",
      "  Then we can transform them and \n",
      "pass these documents to a vector store as embedding.\n",
      "  We can then query the vector store or a \n",
      "retriever associated with the vector store.\n",
      "  Other integrations exist for images such as providing image captions (ImageCaptionLoader).\n",
      "\n",
      "  Once you have selected the loader, you can load the \n",
      "document using the specified loader.\n",
      "\n",
      "----------------------------------------\n",
      "Page 173:\n",
      "Topics:\n",
      "  Building a Chatbot like ChatGPT\n",
      "150\n",
      "Document loaders have a load() method that loads data from the configured source and returns \n",
      "it as documents.\n",
      "  In LangChain, vector retrieval in agents or chains is done via retrievers, which access vector stor-\n",
      "age.\n",
      "  Similarly, we can load documents from Wikipedia:\n",
      "from langchain.document_loaders import WikipediaLoader\n",
      "loader = WikipediaLoader(\"LangChain\")\n",
      "documents = loader.load()\n",
      "\n",
      "  Here is an example of a document loader for loading data from a text file:\n",
      "from langchain.document_loaders import TextLoader\n",
      "loader = TextLoader(file_path=\"path/to/file.txt\")\n",
      "documents = loader.load()\n",
      "\n",
      "  Retrievers in LangChain\n",
      "Retrievers in LangChain are a type of component that is used to search and retrieve information \n",
      "from a given index stored in a vector store as a backend, such as Chroma, to index and search \n",
      "embeddings.\n",
      "  Here are a few examples of retrievers:\n",
      "•\t\n",
      "BM25 retriever: This retriever uses the BM25 algorithm to rank documents based on their \n",
      "relevance to a given query.\n",
      "Subtopics:\n",
      "  Each document consists of page_content (the text content of the document) and \n",
      "metadata (associated metadata such as the source URL or title).\n",
      "\n",
      "  They may also have a lazy_load() method for loading data into memory as \n",
      "and when they are needed.\n",
      "\n",
      "  Retrievers play a crucial role in answering questions over documents, as they are \n",
      "responsible for retrieving relevant information based on the given query.\n",
      "\n",
      "  It is a popular information retrieval algorithm that considers \n",
      "term frequency and document length.\n",
      "\n",
      "  It’s important to note that the specific implementation of document loaders may vary depending \n",
      "on the programming language or framework being used.\n",
      "\n",
      "  Let’s now see how retrievers work.\n",
      "\n",
      "  The documents variable will contain the loaded documents, which can be accessed for further \n",
      "processing.\n",
      "----------------------------------------\n",
      "Page 174:\n",
      "Topics:\n",
      "  These are just a few examples of retrievers available in LangChain.\n",
      "  •\t\n",
      "kNN retriever: This utilizes the well-known k-nearest neighbors algorithm to retrieve \n",
      "relevant documents based on their similarity to a given query.\n",
      "\n",
      "  Chapter 5\n",
      "151\n",
      "•\t\n",
      "TF-IDF retriever: This retriever uses the TF-IDF (Term Frequency-Inverse Document \n",
      "Frequency) algorithm to rank documents based on the importance of terms in the docu-\n",
      "ment collection.\n",
      "  [\"cat\", \"dog\", \"computer\", \"animal\"]\n",
      "retriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())\n",
      "\n",
      "  A Wikipedia retriever allows users to retrieve Wikipedia pages or documents from the website \n",
      "Wikipedia.\n",
      "  The purpose of a Wikipedia retriever is to provide easy access to the vast amount of \n",
      "information available on Wikipedia and enable users to extract specific information or knowl-\n",
      "edge from it.\n",
      "\n",
      "  Here is an example of how to create a kNN retriever using embeddings from OpenAI:\n",
      "from langchain.retrievers import KNNRetriever \n",
      "from langchain.embeddings import OpenAIEmbeddings \n",
      "words =\n",
      "  •\t\n",
      "Dense retriever: This retriever uses dense embeddings to retrieve documents.\n",
      "  The functionality of an Arxiv retriever includes specifying the maximum number of documents \n",
      "to be downloaded, retrieving relevant documents based on a query, and accessing the metadata \n",
      "information of the retrieved documents.\n",
      "\n",
      "  For example, the purpose of an Arxiv retriever is to retrieve scientific articles from \n",
      "the Arxiv.org archive.\n",
      "Subtopics:\n",
      "  It assigns higher weights to terms that are rare in the collection but occur \n",
      "frequently in a specific document.\n",
      "\n",
      "  It is a tool that allows users to search for and download scholarly articles \n",
      "in various fields such as physics, mathematics, computer science, and more. \n",
      "\n",
      "  It encodes \n",
      "documents and queries into dense vectors, and calculates the similarity between them \n",
      "using cosine similarity or other distance metrics.\n",
      "\n",
      "  Each retriever has its own \n",
      "strengths and weaknesses, and the choice of retriever depends on the specific use case and re-\n",
      "quirements.\n",
      "  Let’s see a few retrievers, what they are good for, and how we can customize a retriever.\n",
      "kNN retriever\n",
      "To use the kNN retriever, you need to create a new instance of the retriever and provide it with a \n",
      "list of texts.\n",
      "----------------------------------------\n",
      "Page 175:\n",
      "Topics:\n",
      "  In LangChain, the PubMedRetriever class is used to interact with the PubMed database and re-\n",
      "trieve relevant documents based on a given query.\n",
      "  The \n",
      "method then retrieves relevant documents related to the query from PubMed and returns them \n",
      "as a list.\n",
      "  [Document(page_content='dog', metadata={}),\n",
      " Document(page_content='animal', metadata={}),\n",
      " Document(page_content='cat', metadata={}),\n",
      " Document(page_content='computer', metadata={})]\n",
      "PubMed retriever\n",
      "There are a few more specialized retrievers in LangChain, such as the one from PubMed.\n",
      "  Here’s an example of how to use the PubMed retriever in LangChain:\n",
      "from langchain.retrievers import PubMedRetriever \n",
      "retriever = PubMedRetriever() \n",
      "documents = retriever.get_relevant_documents(\"COVID\")\n",
      "for document in documents:\n",
      "    print(document.metadata[\"Title\"])\n",
      "In this example, the get_relevant_documents() method is called with the query \"COVID\".\n",
      "  Building a Chatbot like ChatGPT\n",
      "152\n",
      "Once the retriever is created, you can use it to retrieve relevant documents by calling the get_\n",
      "relevant_documents() method and passing a query string.\n",
      "  Each document contains the \n",
      "page content and metadata:\n",
      "\n",
      "  PubMed contains millions of citations for biomedical \n",
      "literature from various sources.\n",
      "\n",
      "  The get_relevant_documents() method of \n",
      "the class takes a query as input and returns a list of relevant documents from PubMed.\n",
      "\n",
      "  I am get the following titles as printed output:\n",
      "The COVID-19 pandemic highlights the need for a psychological support in \n",
      "systemic sclerosis patients.\n",
      "\n",
      "  A PubMed \n",
      "retriever is a component in LangChain that helps to incorporate biomedical literature retrieval \n",
      "into their language model applications.\n",
      "Subtopics:\n",
      "  Here is an example of how to use the kNN retriever:\n",
      "result = retriever.get_relevant_documents(\"dog\") \n",
      "print(result)\n",
      "\n",
      "  The retriever will return a list of \n",
      "documents that are most relevant to the query.\n",
      "\n",
      "  This will output a list of documents that are relevant to the query.\n",
      "----------------------------------------\n",
      "Page 176:\n",
      "Topics:\n",
      "  We’ll assume you have the environment installed with the neces-\n",
      "sary libraries and the API keys as per the instructions in Chapter 3, Getting Started with LangChain.\n",
      "\n",
      "  Here is an example of how a retriever can be implemented:\n",
      "from langchain.schema import Document, BaseRetriever\n",
      "class MyRetriever(BaseRetriever):\n",
      "    def get_relevant_documents(self, query: str, **kwargs) -> \n",
      "list[Document]: \n",
      "        # Implement your retrieval logic here \n",
      "        # Retrieve and process documents based on the query \n",
      "        # Return a list of relevant documents \n",
      "        relevant_documents =\n",
      "  Custom retrievers\n",
      "We can implement our own custom retrievers in LangChain by creating a class that is inher-\n",
      "ited from the BaseRetriever abstract class.\n",
      "  Chapter 5\n",
      "153\n",
      "Host genetic polymorphisms involved in long-term symptoms of COVID-19.\n",
      "\n",
      "  Association Between COVID-19 Vaccination and Mortality after Major \n",
      "Operations.\n",
      "\n",
      "Subtopics:\n",
      "  The class should implement the get_relevant_\n",
      "documents() method, which takes a query string as input and returns a list of relevant documents.\n",
      "\n",
      "  [] \n",
      "        # Your retrieval logic goes here… \n",
      "        return relevant_documents\n",
      "You can customize this method to perform any retrieval operations you need, such as querying \n",
      "a database or searching through indexed documents.\n",
      "\n",
      "  Let’s imple-\n",
      "ment a chatbot with a retriever!\n",
      "\n",
      "  Now that we’ve learned about vector stores and retrievers, let’s put all of this to use.\n",
      "  Implementing a chatbot\n",
      "We’ll implement a chatbot now.\n",
      "  Once you have implemented your retriever class, you can create an instance of it and call the \n",
      "get_relevant_documents() method to retrieve relevant documents based on a query.\n",
      "\n",
      "----------------------------------------\n",
      "Page 177:\n",
      "Topics:\n",
      "  Document loader\n",
      "As mentioned, we want to be able to read different formats:\n",
      "from typing import Any\n",
      "from langchain.document_loaders import (\n",
      "  PyPDFLoader, TextLoader,\n",
      "  UnstructuredWordDocumentLoader,\n",
      "  UnstructuredEPubLoader\n",
      ")\n",
      "class EpubReader(UnstructuredEPubLoader):\n",
      "    def __init__(self, file_path: str | list[str], ** kwargs:\n",
      "  \"\"\"\n",
      "    supported_extentions = {\n",
      "        \".pdf\": PyPDFLoader,\n",
      "        \".txt\": TextLoader,\n",
      "        \".epub\": EpubReader,\n",
      "        \".docx\": UnstructuredWordDocumentLoader,\n",
      "\n",
      "  Let’s start with the document loader.\n",
      "\n",
      "  We’ll generalize this with several formats and make this available through an interface in a web \n",
      "browser through Streamlit.\n",
      "  Any):\n",
      "        super().__init__(file_path, **kwargs, mode=\"elements\", \n",
      "strategy=\"fast\")\n",
      "class DocumentLoaderException(Exception):\n",
      "    pass\n",
      "class DocumentLoader(object):\n",
      "    \"\"\"Loads in a document with a supported extension.\n",
      "  Building a Chatbot like ChatGPT\n",
      "154\n",
      "To implement a simple chatbot in LangChain, you can follow this recipe:\n",
      "1.\t\n",
      "Set up a document loader.\n",
      "\n",
      "  2.\t\n",
      "Store documents in a vector store.\n",
      "\n",
      "  3.\t\n",
      "Set up a chatbot with retrieval from the vector storage.\n",
      "\n",
      "Subtopics:\n",
      "  In production, for a corporate deployment for customer engagement, you can imagine that these \n",
      "documents are already loaded in, and your vector storage can just be static.\n",
      "\n",
      "  You’ll be able to drop in your document and start asking questions. \n",
      "\n",
      "----------------------------------------\n",
      "Page 178:\n",
      "Topics:\n",
      "  Vector storage\n",
      "This step includes setting up embedding mechanisms, vector storage, and a pipeline to pass our \n",
      "documents through:\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.vectorstores import DocArrayInMemorySearch\n",
      "from langchain.schema import Document, BaseRetriever\n",
      "def configure_retriever(docs: list[Document]) ->\n",
      "  We’ll now implement the loader logic:\n",
      "import logging\n",
      "import pathlib\n",
      "from langchain.schema import Document\n",
      "def load_document(temp_filepath: str) -> list[Document]:\n",
      "    \"\"\"Load a file and return it as a list of documents.\n",
      "  This gives us interfaces to read PDF, text, EPUB, and Word documents with different extensions. \n",
      "\n",
      "  Chapter 5\n",
      "155\n",
      "        \".doc\": UnstructuredWordDocumentLoader\n",
      "    }\n",
      "\n",
      "Subtopics:\n",
      "  BaseRetriever:\n",
      "    \"\"\"Retriever to use.\n",
      "  \"\"\"\n",
      "\n",
      "  \"\"\"\n",
      "    ext = pathlib.\n",
      "  This doesn’t handle many errors now, but this can be extended if needed.\n",
      "  Path(temp_filepath).suffix\n",
      "    loader = DocumentLoader.supported_extentions.get(ext)\n",
      "    if not loader:\n",
      "        raise DocumentLoaderException(\n",
      "            f\"Invalid extension type {ext}, cannot load this type of file\"\n",
      "        )\n",
      "    loader = loader(temp_filepath)\n",
      "    \n",
      "  Now we can make this \n",
      "loader available from the interface and connect it to vector storage.\n",
      "\n",
      "  docs = loader.load()\n",
      "    logging.info(docs)\n",
      "    return docs\n",
      "\n",
      "----------------------------------------\n",
      "Page 179:\n",
      "Topics:\n",
      "  For the retriever, we have two main options:\n",
      "•\t\n",
      "Similarity-search: We can retrieve document according to similarity.\n",
      "\n",
      "  •\t\n",
      "Maximum Marginal Relevance (MMR): We can apply diversity-based re-ranking of doc-\n",
      "uments during retrieval to get results that cover different perspectives or points of view \n",
      "from the documents retrieved so far.\n",
      "\n",
      "  We are using DocArray as our in-memory vector storage.\n",
      "  DocArray provides various features like \n",
      "advanced indexing, comprehensive serialization protocols, a unified Pythonic interface, and more. \n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "156\n",
      "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_\n",
      "overlap=200)\n",
      "    splits = text_splitter.split_documents(docs)\n",
      "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
      "    return vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \n",
      "\"fetch_k\": 4})\n",
      "\n",
      "  We can initialize DocArray with different distance metrics such as cosine and Euclidean – cosine \n",
      "is the default.\n",
      "\n",
      "  MMR mitigates retrieval redundancy and mitigates the bias inherent in \n",
      "the document collection.\n",
      "  Then we’ve set up a small model from Hugging Face \n",
      "for embeddings and an interface to DocArray for taking splits, creating embeddings, and storing \n",
      "them.\n",
      "  We’ve opted for MMR.\n",
      "  We’ve set the k parameter to 2, which means we will get 2 documents \n",
      "back from retrieval.\n",
      "\n",
      "Subtopics:\n",
      "  Retrieval can be improved by contextual compression, a technique where retrieved documents \n",
      "are compressed, and irrelevant information is filtered out.\n",
      "  Instead of returning the full documents \n",
      "as-is, contextual compression uses the context of the given query to extract and return only the \n",
      "relevant information.\n",
      "  Further, it offers efficient and intuitive handling of multimodal data for tasks such as natural \n",
      "language processing, computer vision, and audio processing.\n",
      "\n",
      "  This helps to reduce the cost of processing and improve the quality of re-\n",
      "sponses in retrieval systems.\n",
      "\n",
      "  Finally, our retriever is looking up documents by maximum marginal relevance.\n",
      "\n",
      "  We are splitting our documents in chunks.\n",
      "  This helps \n",
      "retrieve a wider breadth of relevant information from different perspectives, rather than just re-\n",
      "petitive, redundant hits.\n",
      "  In the similarity search, we can set a similarity score threshold.\n",
      "----------------------------------------\n",
      "Page 180:\n",
      "Topics:\n",
      "  •\t\n",
      "\n",
      "  We can integrate compression here with a simple switch statement at the end (replacing the \n",
      "return statement):\n",
      "if not use_compression:\n",
      "    return retriever\n",
      "embeddings_filter = EmbeddingsFilter(\n",
      "  embeddings=embeddings, similarity_threshold=0.76\n",
      ")\n",
      "return ContextualCompressionRetriever(\n",
      "  base_compressor=embeddings_filter,\n",
      "  base_retriever=retriever\n",
      ")\n",
      "\n",
      "  Chapter 5\n",
      "157\n",
      "The base compressor is responsible for compressing the contents of individual documents based \n",
      "on the context of the given query.\n",
      "  The first two compressors require an LLM to call, which means it can be slow and costly.\n",
      "  Therefore, \n",
      "EmbeddingsFilter can be a more efficient alternative.\n",
      "\n",
      "  We have a few options for contextual compression:\n",
      "•\t\n",
      "LLMChainExtractor:\n",
      "  It uses a language model, such as GPT-3, to perform the com-\n",
      "pression.\n",
      "  EmbeddingsFilter:\n",
      "  When a query is made \n",
      "to the contextual compression retriever, it first passes the query to the base retriever to retrieve \n",
      "relevant documents.\n",
      "  •\t\n",
      "LLMChainFilter:\n",
      "Subtopics:\n",
      "  This applies a similarity filter based on the document and the query \n",
      "in terms of embeddings.\n",
      "\n",
      "  Finally, the compressed documents, containing only \n",
      "the relevant information, are returned as the response.\n",
      "\n",
      "  The compressor can filter out irrelevant information and return only the relevant parts \n",
      "of the document.\n",
      "\n",
      "  Then, it uses the base compressor to compress the contents of these doc-\n",
      "uments based on the context of the query.\n",
      "  This passes over the returned documents and extracts from each \n",
      "only the relevant content.\n",
      "\n",
      "  The base retriever is the document storage system that retrieves the documents based on the \n",
      "query.\n",
      "  This is slightly simpler; it only filters only the relevant documents (rather \n",
      "than the content from the documents).\n",
      "\n",
      "  It can be any retrieval system, such as a search engine or a database.\n",
      "----------------------------------------\n",
      "Page 181:\n",
      "Topics:\n",
      "  One final thing for the retrieval logic is taking the documents and passing them to the retriever \n",
      "setup:\n",
      "import os\n",
      "import tempfile\n",
      "def configure_qa_chain(uploaded_files):\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "158\n",
      "Please note that I’ve just made up a new variable, use_compression.\n",
      "  streaming=True\n",
      "    )\n",
      "    # Passing in a max_tokens_limit amount automatically\n",
      "    # truncates the tokens when prompting your llm!\n",
      "    return ConversationalRetrievalChain.from_llm(\n",
      "        llm, retriever=retriever, memory=memory, verbose=True, max_tokens_\n",
      "limit=4000\n",
      "    )\n",
      "\n",
      "  We can set up the chat chain:\n",
      "from langchain.chains import ConversationalRetrievalChain\n",
      "from langchain.chains.base import Chain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "def configure_chain(retriever: BaseRetriever) -> Chain:\n",
      "    \"\"\"Configure chain with a retriever.\n",
      "  For our chosen compressor, EmbeddingsFilter, we need to include two more additional imports:\n",
      "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
      "from langchain.retrievers import ContextualCompressionRetriever\n",
      "Now that we have the mechanism to create the retriever.\n",
      "  \"\"\"\n",
      "    # Setup memory for contextual conversation\n",
      "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_\n",
      "messages=True)\n",
      "    # Setup LLM and QA chain; set temperature low to keep hallucinations \n",
      "in check\n",
      "    llm = ChatOpenAI(\n",
      "        model_name=\"gpt-3.5-turbo\", temperature=0,\n",
      "Subtopics:\n",
      "  We can feed the use_\n",
      "compression parameter through configure_qa_chain() to the configure_retriever() method \n",
      "(not shown here).\n",
      "\n",
      "----------------------------------------\n",
      "Page 182:\n",
      "Topics:\n",
      "  TemporaryDirectory()\n",
      "    for file in uploaded_files:\n",
      "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
      "        with open(temp_filepath, \"wb\") as f:\n",
      "            f.write(file.getvalue())\n",
      "        docs.extend(load_document(temp_filepath))\n",
      "    \n",
      "  if user_query:\n",
      "    stream_handler = StreamlitCallbackHandler(assistant)\n",
      "    response = qa_chain.run(user_query, callbacks=[stream_handler])\n",
      "    st.markdown(response)\n",
      "\n",
      "  As mentioned, we’ll \n",
      "use streamlit again:\n",
      "import streamlit as st\n",
      "from langchain.callbacks import StreamlitCallbackHandler\n",
      "st.set_page_config(page_title=\"LangChain: Chat with Documents\", page_\n",
      "icon=\"\n",
      "\")\n",
      "st.title(\"\n",
      " LangChain:\n",
      "  Chapter 5\n",
      "159\n",
      "    \"\"\"Read documents, configure retriever, and the chain.\n",
      "Subtopics:\n",
      "  \"\"\"\n",
      "    docs =\n",
      "  retriever = configure_retriever(docs=docs)\n",
      "    return configure_chain(retriever=retriever)\n",
      "Now that we have the logic of the chatbot, we need to set up the interface.\n",
      "  []\n",
      "    temp_dir = tempfile.\n",
      "  uploaded_files = st.sidebar.file_uploader(\n",
      "    label=\"Upload files\",\n",
      "    type=list(DocumentLoader.supported_extentions.keys()),\n",
      "    accept_multiple_files=True\n",
      ")\n",
      "if not uploaded_files:\n",
      "    st.info(\"Please upload documents to continue.\n",
      "  \")\n",
      "    st.stop()\n",
      "qa_chain = configure_qa_chain(uploaded_files)\n",
      "assistant = st.chat_message(\"assistant\")\n",
      "user_query\n",
      "  = st.chat_input(placeholder=\"Ask me anything!\")\n",
      "\n",
      "  Chat with Documents\")\n",
      "\n",
      "----------------------------------------\n",
      "Page 183:\n",
      "Topics:\n",
      "  Let’s have a \n",
      "look at memory and its mechanisms in LangChain.\n",
      "\n",
      "  Chatbot interface with document loaders in different formats\n",
      "You can see the full implementation on GitHub.\n",
      "  It’s important to note that LangChain has limitations on input size and cost.\n",
      "  We’ll look at these use cases in Chapter 8, Customizing \n",
      "LLMs and Their Output.\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "160\n",
      "This gives us a chatbot with retrieval that’s usable via a visual interface, and also has drop-in \n",
      "functionality for custom documents that you need to ask questions about.\n",
      "Figure 5.6:\n",
      "  Memory is a component in the LangChain framework that allows chatbots and language models \n",
      "to remember previous interactions and information.\n",
      "  Addi-\n",
      "tionally, fine-tuning models or hosting the LLM in-house can be more complex and less accurate \n",
      "compared to using commercial solutions.\n",
      "  You may need to \n",
      "consider workarounds to handle larger knowledge bases or optimize the cost of API usage.\n",
      "  You can play around with the chatbot to see how \n",
      "it works and when it doesn’t.\n",
      "\n",
      "Subtopics:\n",
      "  Memory\n",
      "Memory enables chatbots to retain information from previous interactions, maintaining con-\n",
      "tinuity and conversational context.\n",
      "  The chatbot can reference this holistic perspective of the conversation to respond ap-\n",
      "propriately.\n",
      "  Specifically, memory facilitates accuracy by retaining contextual understanding across the entire \n",
      "dialogue.\n",
      "  Memory also enhances personalization and faithfulness by consistently recognizing \n",
      "facts and details from past interactions.\n",
      "\n",
      "  This is analogous to human recall, which allows coherent, \n",
      "meaningful dialogue.\n",
      "  Without memory, chatbots struggle to comprehend references to prior \n",
      "exchanges, resulting in disjointed, unsatisfying conversations.\n",
      "\n",
      "  It is essential in applications like chatbots \n",
      "because it enables the system to maintain context and continuity in conversations.\n",
      "----------------------------------------\n",
      "Page 184:\n",
      "Topics:\n",
      "  streaming=True\n",
      ")\n",
      "chain = ConversationChain(llm=llm, memory=memory)\n",
      "# User inputs a message\n",
      "user_input = \"Hi, how are you?\"\n",
      "# Processing the user input in the conversation chain\n",
      "response = chain.predict(input=user_input)\n",
      "# Printing the response\n",
      "print(response)\n",
      "\n",
      "  Chapter 5\n",
      "161\n",
      "By storing knowledge from message sequences, memory permits extracting insights to improve \n",
      "performance over time.\n",
      "  # User inputs another message\n",
      "user_input = \"What's the weather like today?\n",
      "  Conversation buffers\n",
      "Here’s a practical example in Python that demonstrates how to use the LangChain memory feature:\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.chains import ConversationChain\n",
      "# Creating a conversation chain with memory\n",
      "memory = ConversationBufferMemory()\n",
      "llm = ChatOpenAI(\n",
      "  model_name=\"gpt-3.5-turbo\", temperature=0,\n",
      "  Architectures like LangChain implement memory so chatbots can build \n",
      "on previous exchanges, answer follow-up questions, and sustain natural, logical dialogues.\n",
      "\n",
      "Subtopics:\n",
      "  Further advances in retention and reasoning with long-term memory could lead \n",
      "to more meaningful and productive human-AI interaction.\n",
      "\n",
      "  \"\n",
      "# Processing the user input in the conversation chain\n",
      "response = chain.predict(input=user_input)\n",
      "# Printing the response\n",
      "print(response)\n",
      "# Printing the conversation history stored in memory\n",
      "print(memory.chat_memory.messages)\n",
      "\n",
      "  Overall, memory is a crucial component for sophisticated chatbots, allowing them to learn from \n",
      "conversations and mimic the recall and contextual awareness that comes naturally to human \n",
      "interlocutors.\n",
      "----------------------------------------\n",
      "Page 185:\n",
      "Topics:\n",
      "  Here’s a simple example of how to use \n",
      "ConversationBufferWindowMemory in LangChain:\n",
      "from langchain.memory import ConversationBufferWindowMemory\n",
      "memory = ConversationBufferWindowMemory(k=1)\n",
      "\n",
      "  It takes two ar-\n",
      "guments: user_input and model_output.\n",
      "  ConversationBufferWindowMemory is a memory type provided by LangChain that keeps track \n",
      "of the interactions in a conversation over time.\n",
      "  In this example, the window size is set to 1, meaning that only the last interaction will be stored \n",
      "in memory.\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "162\n",
      "In this example, we create a conversation chain with memory using ConversationBufferMemory, \n",
      "which is a simple wrapper that stores the messages in a variable.\n",
      "  Unlike ConversationBufferMemory, which \n",
      "retains all previous interactions, ConversationBufferWindowMemory only keeps the last k \n",
      "interactions, where k is the window size specified.\n",
      "  Instead of constructing the memory separately from the chain, we could have simplified things:\n",
      "conversation = ConversationChain(\n",
      "    llm=llm,\n",
      "    verbose=True,\n",
      "    memory=ConversationBufferMemory()\n",
      ")\n",
      "\n",
      "  Additionally, we print the conversation history stored in memory using memory.chat_memory.\n",
      "messages.\n",
      "Subtopics:\n",
      "  The conversation chain retains the memory \n",
      "of previous interactions, allowing it to provide context-aware responses.\n",
      "\n",
      "  The save_context() method is used to store inputs and outputs.\n",
      "  The user’s inputs are processed \n",
      "using the predict() method of the conversation chain.\n",
      "  After processing the user inputs, we print the response generated by the conversation chain. \n",
      "\n",
      "  You can use the \n",
      "load_memory_variables() method to view the stored content.\n",
      "  To get the history as a list of \n",
      "messages, a return_messages parameter is set to True.\n",
      "  We’ll see examples of this in this section.\n",
      "\n",
      "  These represent the user’s input and the corresponding \n",
      "model’s output for a given interaction.\n",
      "\n",
      "  memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
      "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
      "\n",
      "  We can use the save_context() method to save the context of each interaction.\n",
      "  We are setting verbose to True to see the prompts.\n",
      "\n",
      "----------------------------------------\n",
      "Page 186:\n",
      "Topics:\n",
      "  If the AI does not know the answer to a \n",
      "question, it truthfully says it does not know.\n",
      "\n",
      "  Import the necessary classes and modules from LangChain:\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.prompts.prompt import PromptTemplate\n",
      "llm = OpenAI(temperature=0)\n",
      "\n",
      "  Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI Assistant:\"\"\"\n",
      "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], \n",
      "template=template)\n",
      "conversation = ConversationChain(\n",
      "    prompt=PROMPT,\n",
      "    llm=llm,\n",
      "    verbose=True,\n",
      "    memory=ConversationBufferMemory(ai_prefix=\"AI Assistant\"),\n",
      ")\n",
      "In this example, the AI prefix is set to AI Assistant instead of the default AI.\n",
      "\n",
      "  2.\t\n",
      "Define a new prompt template that includes the customized prefixes.\n",
      "  To customize the conversational memory, you can follow these steps:\n",
      "1.\t\n",
      "\n",
      "  The AI is talkative and provides lots of specific \n",
      "details from its context.\n",
      "  We can also customize the conversational memory in LangChain, which involves modifying the \n",
      "prefixes used for the AI and human messages, as well as updating the prompt template to reflect \n",
      "these changes.\n",
      "\n",
      "  Chapter 5\n",
      "163\n",
      "We can see the message with memory.load_memory_variables({}).\n",
      "\n",
      "  You can do this by \n",
      "creating a PromptTemplate object with the desired template string:\n",
      "template = \"\"\"The following is a friendly conversation between a \n",
      "human and an AI.\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 187:\n",
      "Topics:\n",
      "  Here’s an example:\n",
      "from langchain.memory import ConversationSummaryMemory\n",
      "from langchain.llms import OpenAI\n",
      "# Initialize the summary memory and the language model\n",
      "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "164\n",
      "Remembering conversation summaries\n",
      "ConversationSummaryMemory is a type of memory in LangChain that generates a summary of the \n",
      "conversation as it progresses.\n",
      "  Then, use the save_context() method to save the interaction context, \n",
      "which includes the user input and AI output.\n",
      "  # Save the context of an interaction\n",
      "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
      "# Load the summarized memory\n",
      "memory.load_memory_variables({})\n",
      "Storing knowledge graphs\n",
      "In LangChain, we can also extract information from the conversation as facts and store these by \n",
      "integrating a knowledge graph as the memory.\n",
      "  There’s functionality in LangChain for knowledge graphs for retrieval; however, LangChain also \n",
      "provides memory components to automatically create a knowledge graph based on our conver-\n",
      "sation messages.\n",
      "\n",
      "  To use ConversationSummaryMemory, first create an instance of it, passing the language model \n",
      "(llm) as an argument.\n",
      "Subtopics:\n",
      "  Relationships capture the connections and \n",
      "associations between entities, providing contextual information and enabling semantic reasoning.\n",
      "\n",
      "  A knowledge graph is a structured knowledge representation model that organizes information \n",
      "in the form of entities, attributes, and relationships.\n",
      "  This can enhance the capabilities of language \n",
      "models and enable them to leverage structured knowledge during text generation and inference.\n",
      "\n",
      "  Instead of storing all messages verbatim, it condenses the informa-\n",
      "tion, providing a summarized version of the conversation.\n",
      "  In \n",
      "a knowledge graph, entities can be any concept, object, or thing in the world, and attributes de-\n",
      "scribe the properties or characteristics of these entities.\n",
      "  It represents knowledge as a graph, where \n",
      "entities are represented as nodes and relationships between entities are represented as edges.\n",
      "  This is particularly useful for extended \n",
      "conversations, where including all previous messages might exceed token limits.\n",
      "\n",
      "  To retrieve the summarized conversation history, \n",
      "use the load_memory_variables() method.\n",
      "\n",
      "----------------------------------------\n",
      "Page 188:\n",
      "Topics:\n",
      "  Combining several memory mechanisms\n",
      "LangChain also allows combining multiple memory strategies using the CombinedMemory class. \n",
      "\n",
      "  For instance, \n",
      "one memory could be used to store the complete conversation log:\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory, CombinedMemory, \n",
      "ConversationSummaryMemory\n",
      "# Initialize language model (with desired temperature parameter)\n",
      "llm = OpenAI(temperature=0)\n",
      "\n",
      "  = \"\"\"The following is a friendly conversation between a \n",
      "human and an AI.\n",
      "  If the AI does not know the answer to a question, it \n",
      "truthfully says it does not know.\n",
      "\n",
      "  Chapter 5\n",
      "165\n",
      "We’ll instantiate the ConversationKGMemory class and pass your LLM instance as the llm pa-\n",
      "rameter:\n",
      "from langchain.memory import ConversationKGMemory\n",
      "from langchain.llms import OpenAI\n",
      "llm = OpenAI(temperature=0)\n",
      "\n",
      "  # Define Conversation Buffer Memory (for retaining all past messages)\n",
      "conv_memory = ConversationBufferMemory(memory_key=\"chat_history_lines\", \n",
      "input_key=\"input\")\n",
      "# Define Conversation Summary Memory (for summarizing conversation)\n",
      "summary_memory = ConversationSummaryMemory(llm=llm, input_key=\"input\")\n",
      "# Combine both memory types\n",
      "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
      "# Define Prompt Template\n",
      "_DEFAULT_TEMPLATE\n",
      "  The AI is talkative and provides lots of specific details \n",
      "from its context.\n",
      "Subtopics:\n",
      "  This is useful when you want to maintain various aspects of the conversation history.\n",
      "  Summary of conversation:\n",
      "{history}\n",
      "\n",
      "  memory = ConversationKGMemory(llm=llm)\n",
      "As the conversation progresses, we can save relevant information from the knowledge graph into \n",
      "the memory using the save_context() function of ConversationKGMemory.\n",
      "\n",
      "----------------------------------------\n",
      "Page 189:\n",
      "Topics:\n",
      "  We then combine \n",
      "these memories using CombinedMemory.\n",
      "  Building a Chatbot like ChatGPT\n",
      "166\n",
      "Current conversation:\n",
      "{chat_history_lines}\n",
      "Human: {input}\n",
      "AI:\"\"\"\n",
      "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\", \"chat_\n",
      "history_lines\"], template=_DEFAULT_TEMPLATE)\n",
      "# Initialize the Conversation Chain\n",
      "conversation = ConversationChain(llm=llm, verbose=True, memory=memory, \n",
      "prompt=PROMPT)\n",
      "# Start the conversation\n",
      "conversation.run(\"Hi!\")\n",
      "\n",
      "  To use this, the memory buffer needs to be instantiated with the LLM, and max_token_limit. \n",
      "\n",
      "  In this example, we first instantiate the language model and the several types of memories \n",
      "we’re using – ConversationBufferMemory for retaining the full conversation history and \n",
      "ConversationSummaryMemory for creating a summary of the conversation.\n",
      "  This long-term memory with fast vector search and \n",
      "configurable summarization enables more capable conversational AI with context awareness.\n",
      "\n",
      "  A practical example of using Zep is to integrate it as the long-term memory for a chatbot or AI \n",
      "app.\n",
      "  Zep, being one such \n",
      "example, provides a persistent backend to store, summarize, and search chat histories using vec-\n",
      "tor embeddings and auto-token counting.\n",
      "  We also define a prompt template that accommodates \n",
      "our memory usage and, finally, we create and run ConversationChain by providing our language \n",
      "model, memory, and prompt to it.\n",
      "\n",
      "  By using the ZepMemory class, developers can initialize a ZepMemory instance with the Zep \n",
      "server URL, API key, and a unique session identifier for the user.\n",
      "  This allows the chatbot or AI app \n",
      "to store and retrieve chat history or other relevant information.\n",
      "\n",
      "Subtopics:\n",
      "  ConversationSummaryBufferMemory offers a method called predict_new_summary(), which can \n",
      "be used directly to generate a conversation summary.\n",
      "\n",
      "  Long-term persistence\n",
      "There are also different ways of storing conversations in dedicated backends.\n",
      "  ConversationSummaryBufferMemory is used to keep a buffer of recent interactions in memory and \n",
      "compiles old interactions into a summary instead of completely flushing them out.\n",
      "  The threshold \n",
      "for flushing interactions is determined by token length and not by the number of interactions.\n",
      "\n",
      "----------------------------------------\n",
      "Page 190:\n",
      "Topics:\n",
      "  memory = ZepMemory( \n",
      "    session_id=session_id, \n",
      "    url=ZEP_API_URL, \n",
      "    api_key=ZEP_API_KEY, \n",
      "    memory_key=\"chat_history\", \n",
      ")\n",
      "\n",
      "  As mentioned, once the memory is set up, you \n",
      "can use it in your chatbot’s chain or with your AI agent to store and retrieve chat history or other \n",
      "relevant information.\n",
      "  Chapter 5\n",
      "167\n",
      "For example, in Python, you can initialize a ZepMemory instance as follows:\n",
      "from langchain.memory import ZepMemory  \n",
      "ZEP_API_URL = \"http://localhost:8000\" \n",
      "ZEP_API_KEY = \"<your JWT token>\"\n",
      "session_id = str(uuid4()) \n",
      "\n",
      "  Overall, Zep simplifies the process of persisting, searching, and enriching \n",
      "chatbot or AI app histories, allowing developers to focus on developing their AI applications \n",
      "rather than building memory infrastructure.\n",
      "\n",
      "  Please note that the URL and \n",
      "API key need to be set according to your setup.\n",
      "  This sets up a ZepMemory instance that you can use in your chains.\n",
      "Subtopics:\n",
      "  The constitution serves as a framework for ensuring that the \n",
      "chatbot operates within the desired boundaries and provides a positive user experience.\n",
      "\n",
      "  This is an important part \n",
      "of any application that we’d want to deploy for customers.\n",
      "\n",
      "  It outlines the standards and principles that the chatbot \n",
      "should adhere to, such as avoiding offensive language, promoting respectful interactions, and \n",
      "maintaining ethical standards.\n",
      "  Moderating responses\n",
      "The role of moderation in chatbots is to ensure that the bot’s responses and conversations are \n",
      "appropriate, ethical, and respectful.\n",
      "  Mod-\n",
      "eration is crucial for creating a safe, respectful, and inclusive environment for users, protecting \n",
      "brand reputation, and complying with legal obligations.\n",
      "\n",
      "  In the context of moderation, a constitution refers to a set of guidelines or rules that govern the \n",
      "behavior and responses of the chatbot.\n",
      "  In the next section, we’ll look at using moderation to make sure responses are adequate.\n",
      "  It involves implementing mechanisms to filter out offensive \n",
      "or inappropriate content and discouraging abusive behavior from users.\n",
      "----------------------------------------\n",
      "Page 191:\n",
      "Topics:\n",
      "  •\t\n",
      "Legal compliance: Depending on the jurisdiction, there may be legal requirements for \n",
      "moderating content and ensuring that it complies with laws and regulations.\n",
      "  By implementing rules and consequences, such as the “two strikes” \n",
      "rule mentioned in the example, the developer can discourage users from using provocative \n",
      "language or engaging in abusive behavior.\n",
      "\n",
      "  •\t\n",
      "Protecting users from inappropriate content\n",
      "  In LangChain, first, you would create an instance of the OpenAIModerationChain class, which is \n",
      "a pre-built moderation chain provided by LangChain.\n",
      "  You can add a moderation chain to an LLMChain instance or a Runnable instance to ensure that \n",
      "the generated output from the language model is not harmful.\n",
      "\n",
      "  Building a Chatbot like ChatGPT\n",
      "168\n",
      "Moderation and having a constitution are important in chatbots for several reasons:\n",
      "•\t\n",
      "Ensuring ethical behavior: Chatbots can interact with a wide range of users, including \n",
      "vulnerable individuals.\n",
      "  •\t\n",
      "Maintaining brand reputation: Chatbots often represent a brand or organization.\n",
      "Subtopics:\n",
      "  : Moderation helps prevent the dissemi-\n",
      "nation of inappropriate or offensive language, hate speech, or any content that may be \n",
      "harmful or offensive to users.\n",
      "  •\t\n",
      "Preventing abusive behavior: Moderation can discourage users from engaging in abusive \n",
      "or improper behavior.\n",
      "  It creates a safe and inclusive environment for users to \n",
      "interact with the chatbot.\n",
      "\n",
      "  The specific handling method \n",
      "depends on your application’s requirements.\n",
      "\n",
      "  You can choose to throw an error in the chain and handle it in your application, or you can \n",
      "return a message to the user explaining that the text was harmful.\n",
      "  = OpenAIModerationChain()\n",
      "\n",
      "  If the content passed into the moderation chain is deemed harmful, there are a few ways to han-\n",
      "dle it.\n",
      "  Having a \n",
      "constitution or set of guidelines helps the developer adhere to these legal requirements.\n",
      "\n",
      "  By \n",
      "implementing moderation, the developer can ensure that the bot’s responses align with \n",
      "the brand’s values and maintain a positive reputation.\n",
      "\n",
      "  Moderation helps ensure that the bot’s responses are ethical, re-\n",
      "spectful, and do not promote harmful or offensive content.\n",
      "\n",
      "  This chain is specifically designed to detect \n",
      "and filter out harmful content:\n",
      "from langchain.chains import OpenAIModerationChain \n",
      "moderation_chain\n",
      "----------------------------------------\n",
      "Page 192:\n",
      "Topics:\n",
      "  This allows you to chain multiple chains together in \n",
      "a sequential manner:\n",
      "chain = llm_chain | moderation_chain\n",
      "Now, when you want to generate text using the language model, you would pass your input text \n",
      "through the moderation chain first, and then through the language model chain.\n",
      "\n",
      "  To append the moderation chain to the language model chain, you can use the SequentialChain \n",
      "class or the LCEL (which is recommended).\n",
      "  We can do this using the LCEL syntax, which we’ve introduced in Chapter 4, \n",
      "Building Capable Assistants:\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema import StrOutputParser\n",
      "cot_prompt = PromptTemplate.from_template(\n",
      "    \"{question} \\nLet's think step by step!\"\n",
      ")\n",
      "\n",
      "  This is a chain with a Chain of Thought (CoT) prompt, which includes the instruction to think \n",
      "step by step.\n",
      "\n",
      "  I’ve added an example for moderation to the chatbot app on GitHub.\n",
      "\n",
      "  The first chain will come up with a preliminary answer.\n",
      "  Chapter 5\n",
      "169\n",
      "Next, you would create an instance of the LLMChain class or of a Runnable instance, which rep-\n",
      "resents your language model chain.\n",
      "  llm_chain = cot_prompt | ChatOpenAI() | StrOutputParser()\n",
      "\n",
      "Subtopics:\n",
      "  Further, guardrails can be used to define the behavior of the language model on specific topics, \n",
      "prevent it from engaging in discussions on unwanted topics, guide the conversation along a \n",
      "predefined path, enforce a particular language style, extract structured data, and more.\n",
      "\n",
      "  This is where you define your prompt and interact with the \n",
      "language model.\n",
      "  response = chain.invoke({\"question\": \"What is the future of \n",
      "programming?\"}) \n",
      "\n",
      "  Then, the moderation chain will evaluate \n",
      "this answer and filter out any harmful content.\n",
      "  If the input text is deemed harmful, the moder-\n",
      "ation chain can either throw an error or return a message indicating that the text is not allowed. \n",
      "\n",
      "----------------------------------------\n",
      "Page 193:\n",
      "Topics:\n",
      "  Building a Chatbot like ChatGPT\n",
      "170\n",
      "In the context of LLMs, guardrails (rails for short) refer to specific ways of controlling the model’s \n",
      "output.\n",
      "  This ensures that the output is in line with your desired \n",
      "tone, formality, or specific language requirements.\n",
      "•\t\n",
      "Structured data extraction: Guardrails can be used to extract structured data from the \n",
      "conversation.\n",
      "  •\t\n",
      "Predefined dialogue paths: Guardrails enable you to define a predefined path for the \n",
      "conversation.\n",
      "  This ensures that the language model or chatbot follows a specific flow \n",
      "and provides consistent responses.\n",
      "•\t\n",
      "Language style: Guardrails allow you to specify the language style that the language \n",
      "model or chatbot should use.\n",
      "Subtopics:\n",
      "  We went into detail on methods for loading documents and information, including \n",
      "vector storage and embedding. \n",
      "\n",
      "  We discussed the importance of proactive communication.\n",
      "  Overall, guardrails provide a way to add programmable rules and constraints to LLMs and chatbots, \n",
      "making them more trustworthy, safe, and secure in their interactions with users.\n",
      "  In this chapter, we focused \n",
      "on retrieving relevant data from sources through vector search and injecting it into the context. \n",
      "\n",
      "  They provide a means to add programmable constraints and guidelines to ensure the \n",
      "output of the language model aligns with desired criteria.\n",
      "\n",
      "  We explored retrieval \n",
      "mechanisms, including vector storage, with the goal of improving the accuracy of chatbot re-\n",
      "sponses.\n",
      "  By appending \n",
      "the moderation chain to your language model chain, you can ensure that the generated text is \n",
      "moderated and safe for use in your application.\n",
      "\n",
      "  Summary\n",
      "In the previous chapter, we discussed tool-augmented LLMs, which involve the utilization of \n",
      "external tools or knowledge resources such as document corpora.\n",
      "  The chapter started with an overview of chatbots, their evolution, and the current state of chat-\n",
      "bots, highlighting the practical implications and enhancements of the capabilities of the current \n",
      "technology.\n",
      "  I \n",
      "also introduced retrieval and vector mechanisms, and we discussed implementing a chatbot, the \n",
      "importance of memory mechanisms, and the importance of appropriate responses.\n",
      "\n",
      "  You can prevent it from engaging in discussions on unwanted \n",
      "or sensitive topics like politics.\n",
      "\n",
      "  This retrieved data serves as additional information to augment the prompts given to LLMs.\n",
      "  This can be useful for capturing specific information or performing actions \n",
      "based on user inputs.\n",
      "\n",
      "  Here are a few ways guardrails can be used:\n",
      "•\t\n",
      "Controlling topics: Guardrails allow you to define the behavior of your language model or \n",
      "chatbot on specific topics.\n",
      "----------------------------------------\n",
      "Page 194:\n",
      "Topics:\n",
      "  2.\t \n",
      "  What is vector search?\n",
      "6.\t\n",
      "\n",
      "  1.\t\n",
      "Please name 5 different chatbots!\n",
      "\n",
      "  What are some important aspects of developing a chatbot?\n",
      "3.\t\n",
      "\n",
      "  Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "  What does RAG stand for?\n",
      "4.\t\n",
      "\n",
      "  What is an embedding?\n",
      "5.\t\n",
      "\n",
      "  What is a retriever in LangChain?\n",
      "9.\t\n",
      "\n",
      "  8.\t \n",
      "  What is memory and what are the memory options in LangChain?\n",
      "10.\t \n",
      "  Chapter 5\n",
      "171\n",
      "Additionally, we discussed memory mechanisms for maintaining knowledge and the state of \n",
      "ongoing conversations.\n",
      "  What is a vector database?\n",
      "7.\t\n",
      "Please name 5 different vector databases!\n",
      "\n",
      "Subtopics:\n",
      "  What is moderation, what’s a constitution, and how do they work?\n",
      "\n",
      "  I’d recommend you \n",
      "go back to the corresponding sections of this chapter if you are unsure about any of them:\n",
      "\n",
      "  Questions\n",
      "Please see if you can produce the answers to these questions from memory.\n",
      "  The features discussed in this chapter serve as a starting point to investigate issues like memory, \n",
      "context, and the moderation of speech, but they can also be interesting for issues like halluci-\n",
      "nations.\n",
      "\n",
      "  The chapter concluded with a discussion on moderation, emphasizing \n",
      "the importance of ensuring that responses are respectful and aligned with organizational values.\n",
      "\n",
      "----------------------------------------\n",
      "Page 195:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 196:\n",
      "Topics:\n",
      "  We’ll go through the design choices and show some of the results \n",
      "that we got in an agent implementation of only a few lines of Python with LangChain.\n",
      "  While this book is about integrating generative AI particularly LLMs into software applications, in \n",
      "this chapter, we’ll talk about how we can leverage LLMs to help in software development.\n",
      "  6\n",
      "Developing Software with \n",
      "Generative AI\n",
      "\n",
      "  In short, the main sections in this chapter are:\n",
      "•\t\n",
      "Software development and AI\n",
      "•\t\n",
      "Writing code with LLMs\n",
      "•\t\n",
      "Automated software development\n",
      "We’ll begin the chapter by giving a broad overview of the current state of using AI for software \n",
      "development.\n",
      "\n",
      "  This is \n",
      "a big topic; software development has been highlighted in reports by several consultancies, such \n",
      "as KPMG and McKinsey, as one of the domains impacted most by generative AI.\n",
      "\n",
      "  We’ll first discuss how LLMs could help in coding tasks, and I’ll provide an overview to see how far \n",
      "we have come in automating software development.\n",
      "  Throughout the chapter, we’ll work on different practical approaches to automatic software de-\n",
      "velopment, which you can find in the software_development directory in the GitHub repository \n",
      "for the book at https://github.com/benman1/generative_ai_with_langchain.\n",
      "\n",
      "Subtopics:\n",
      "  We’ll \n",
      "mention many possible extensions to this approach.\n",
      "\n",
      "  Then, we’ll play around with a few models, \n",
      "evaluating the generated code qualitatively.\n",
      "  Next, we’ll implement a fully automated agent for \n",
      "software development tasks.\n",
      "----------------------------------------\n",
      "Page 197:\n",
      "Topics:\n",
      "  A McKinsey report from the same month \n",
      "highlighted software development as a function, where generative AI can have a significant impact \n",
      "in terms of cost reduction and efficiency gain.\n",
      "\n",
      "  Today’s AI assistants integrate predictive typing, syntax checking, code generation, and other \n",
      "features to directly support software development workflows, realizing early aspirations to au-\n",
      "tomate programming itself.\n",
      "\n",
      "  New code LLMs such as ChatGPT and Microsoft’s Copilot are highly popular generative AI models, \n",
      "with millions of users and significant productivity-boosting capabilities.\n",
      "  Developing Software with Generative AI\n",
      "174\n",
      "Software development and AI\n",
      "\n",
      "  With neural networks and deep \n",
      "learning advancing in the 1990s and 2000s, machine learning techniques began to be applied to \n",
      "improve analysis capabilities for program synthesis, bug detection, vulnerability discovery, and \n",
      "automating other programming tasks.\n",
      "\n",
      "  Object-oriented \n",
      "languages like Simula and Smalltalk in the 1960s-70s introduced new paradigms for modularity \n",
      "through objects and classes.\n",
      "\n",
      "  A June 2023 report by KPMG estimated that about 25% of \n",
      "software development tasks could be automated away.\n",
      "  It is commonly used in Integrated Development Environments (IDEs) \n",
      "to assist developers in writing code.\n",
      "\n",
      "  The emergence of powerful AI systems like ChatGPT has sparked great interest in using AI as \n",
      "a tool to assist software developers.\n",
      "  Early procedural languages like FORTRAN and \n",
      "COBOL in the 1950s enabled this by introducing control structures, variables, and other high-lev-\n",
      "el constructs.\n",
      "Subtopics:\n",
      "  Integrated development \n",
      "environments evolved to provide intelligent assistance for coding, testing, and debugging.\n",
      "  Static \n",
      "and dynamic program analysis tools helped identify issues in code.\n",
      "  There are different tasks \n",
      "related to programming that LLMs can tackle, such as these:\n",
      "•\t\n",
      "Code completion: This task involves predicting the next code element based on the sur-\n",
      "rounding code.\n",
      "  The history of software development has been marked by efforts to increase abstraction from \n",
      "machine code to focus more on problem-solving.\n",
      "  As codebases expanded, maintaining quality became more challenging, leading to methodologies \n",
      "like agile development with iterative cycles and continuous integration.\n",
      "  As programs grew larger, structured programming concepts emerged to improve \n",
      "code organization through modularity, encapsulation, and stepwise refinement.\n",
      "----------------------------------------\n",
      "Page 198:\n",
      "Topics:\n",
      "  •\t\n",
      "Test generation: Similar to code completion, LLMs can generate unit tests (Codet: Code \n",
      "Generation with Generated Tests; Bei Chen and others, 2022) and other types of tests en-\n",
      "hancing the maintainability of a codebase.\n",
      "\n",
      "  •\t\n",
      "Bug finding/fixing: AI systems can reduce manual debugging efforts and enhance software \n",
      "reliability and security.\n",
      "  AI programming assistants combine the interactivity of earlier systems with innovative natural \n",
      "language processing.\n",
      "  Code LLMs\n",
      "Quite a few AI models have emerged, each with their own strengths and weaknesses, which are \n",
      "continuously competing to improve and deliver better results.\n",
      "  Performance continues to improve \n",
      "with models like StarCoder, though data quality can also play a key role.\n",
      "  Chapter 6\n",
      "175\n",
      "•\t\n",
      "Code summarization/documentation:\n",
      "  Developers can query programming problems in plain English or describe \n",
      "desired functions, receiving generated code or debugging tips.\n",
      "  Let’s look at the current performance of AI systems for coding, particularly code LLMs.\n",
      "\n",
      "  Powerful pre-trained models like GPT-3 and GPT-4 enable context-aware, conversational support. \n",
      "\n",
      "  •\t\n",
      "Code search: The objective of code search is to find the most relevant code snippets based \n",
      "on a given natural language query.\n",
      "Subtopics:\n",
      "  This task involves learning the joint embeddings of the \n",
      "query and code snippets to return the expected ranking order of code snippets.\n",
      "\n",
      "  As an alternative, \n",
      "LLMs can spot problems within code and (when prompted) correct them.\n",
      "  However, risks remain around code \n",
      "quality, security, and excessive dependence.\n",
      "  Thus, these \n",
      "systems can reduce manual debugging efforts and help improve software reliability and \n",
      "security.\n",
      "\n",
      "  Striking the right balance of computer augmentation \n",
      "while maintaining human oversight is an ongoing challenge.\n",
      "\n",
      "  This task aims to generate a natural language sum-\n",
      "mary or documentation for a given block of source code.\n",
      "  This summary helps developers \n",
      "understand the purpose and function of the code without having to read the actual code.\n",
      "\n",
      "  Studies show LLMs aid \n",
      "workflow efficiency but need more robustness, integration, and communication abilities.\n",
      "\n",
      "  These approaches also empower bug detection, repair recommendations, automated testing \n",
      "tools, and code search.\n",
      "\n",
      "  Many bugs and vulnerabilities are hard to find for programmers, \n",
      "although there are typical patterns for which code validation tools exist.\n",
      "----------------------------------------\n",
      "Page 199:\n",
      "Topics:\n",
      "  Developing Software with Generative AI\n",
      "176\n",
      "Microsoft’s GitHub Copilot, which is based on OpenAI’s Codex, draws on open-source code to \n",
      "suggest full code blocks in real time.\n",
      "  •\t\n",
      "OpenAI’s ChatGPT in 2022 demonstrated exceptionally coherent natural \n",
      "language conversations about coding.\n",
      "•\t\n",
      "DeepMind’s AlphaTensor and AlphaDev in 2022 demonstrated AI’s ability \n",
      "to discover novel, human-competitive algorithms, unlocking performance \n",
      "optimizations.\n",
      "\n",
      "  A common metric on HumanEval is \n",
      "pass@k (pass@1) – this refers to the fraction of correct samples when generating k code samples \n",
      "per problem.\n",
      "\n",
      "  The dataset includes 164 programming problems that cover various aspects, \n",
      "such as language comprehension, algorithms, and simple mathematics.\n",
      "  •\t\n",
      "DeepMind’s AlphaCode in 2022 matched human programming speed, show-\n",
      "ing the ability to generate full programs.\n",
      "\n",
      "  To illustrate the progress made in creating software, let’s look at quantitative results in a bench-\n",
      "mark: the HumanEval dataset, introduced in the Codex paper (Evaluating Large Language Models \n",
      "Trained on Code, 2021), is designed to test the ability of LLMs to complete functions based on \n",
      "their signature and docstring.\n",
      "  Recent milestones:\n",
      "•\t\n",
      "OpenAI’s Codex model in 2021 could generate code snippets from natural \n",
      "language descriptions, showing promise for assisting programmers.\n",
      "•\t\n",
      "GitHub’s Copilot, launched in 2021, was an early integration of LLMs into \n",
      "IDEs for autocompletion, achieving rapid adoption.\n",
      "\n",
      "  A descendant of the GPT-3 model, it has been \n",
      "fine-tuned on publicly available code from GitHub, 159 gigabytes of Python code \n",
      "from 54 million GitHub repositories, for programming applications.\n",
      "\n",
      "  This chart summarizes the AI models on the HumanEval task (number of parameters against the \n",
      "pass@1 performance on HumanEval).\n",
      "  It can parse natural language and generate \n",
      "code, and it powers GitHub Copilot.\n",
      "  According to a GitHub report in June 2023, developers ac-\n",
      "cepted the AI assistant’s suggestions about 30 percent of the time, which suggests that the tool \n",
      "can provide useful suggestions, with less experienced developers profiting the most.\n",
      "\n",
      "  Codex is a model developed by OpenAI.\n",
      "Subtopics:\n",
      "  A few performance metrics are self-reported:\n",
      "\n",
      "  Some of the problems \n",
      "are comparable to simple software interview questions.\n",
      "  It evaluates the functional correctness of synthesizing programs \n",
      "from docstrings.\n",
      "----------------------------------------\n",
      "Page 200:\n",
      "Topics:\n",
      "  The Pile was used in the training of Meta’s Llama, Yandex’s YaLM 100B, and many \n",
      "others.\n",
      "\n",
      "  This is mainly based on the Big Code Models \n",
      "Leaderboard, which is hosted on Hugging Face, but I’ve added a few more models for comparison, \n",
      "and I’ve omitted models with more than 70 billion parameters.\n",
      "  For example, at least about 11% of the code in The Pile, a dataset that was curated \n",
      "by EleutherAI’s GPT-Neo for training open-source alternatives of the GPT models, is from GitHub \n",
      "(102.18 GB).\n",
      "  Chapter 6\n",
      "177\n",
      "Figure 6.1: Model comparison on HumanEval coding task benchmark\n",
      "You can see lines marking the performance of closed-source models such as GPT-4, GPT-4 with \n",
      "reflection, PaLM-Coder 540B, GPT-3.5, and Claude 2.\n",
      "Subtopics:\n",
      "  All models can do coding at some level, since the data used in training most LLMs includes some \n",
      "source code.\n",
      "  Some models have self-reported \n",
      "performance, so you should take this with a grain of salt.\n",
      "\n",
      "----------------------------------------\n",
      "Page 201:\n",
      "Topics:\n",
      "  Developing Software with Generative AI\n",
      "178\n",
      "Although HumanEval has been broadly used as a benchmark for code LLMs, there are a multitude \n",
      "of benchmarks for programming.\n",
      "  Here’s an example question and the response from an advanced \n",
      "computer science test given to Codex (source: My AI Wants to Know if This Will Be on the Exam: \n",
      "Testing OpenAI’s Codex on CS2 Programming Exercises by James Finnie-Ansley and others, 2023):\n",
      "Figure 6.2: A question given in a CS2 exam (left) and the Codex response\n",
      "Most recently, the paper Textbooks Are All You Need by Suriya Gunasekar and others at Microsoft \n",
      "Research (2023) introduced phi-1, a 1.3B-parameter Transformer-based language model for code. \n",
      "\n",
      "  Generating complete programs that demonstrate a deep understanding of the problem and plan-\n",
      "ning involved requires fundamentally different capabilities than producing short code snippets \n",
      "that mainly translate specifications directly into API calls.\n",
      "  The authors start with a 3 TB corpus of code from The Stack and Stack Overflow. \n",
      "\n",
      "  Results show phi-1 matches or exceeds \n",
      "the performance of models over 10x its size on benchmarks like HumanEval and MBPP.\n",
      "\n",
      "  The small 1.3B-parameter phi-1 model is trained on this filtered data.\n",
      "  Separately, GPT-3.5 generates 1B tokens mim-\n",
      "icking a textbook style.\n",
      "  An LLM filters this to select 6B high-quality tokens.\n",
      "  However, novel reasoning-focused strategies like the Reflexion framework (Reflexion: Language \n",
      "Agents with Verbal Reinforcement Learning by Noah Shinn and others; 2023) can lead to enormous \n",
      "improvements even for short code snippets.\n",
      "Subtopics:\n",
      "  Reflexion enables trial-and-error-based learning, \n",
      "with language agents verbally reflecting on task feedback and storing this experience in an ep-\n",
      "isodic memory buffer. \n",
      "\n",
      "  Instead of brute-force scaling, data quality should take precedence.\n",
      "  Recursively filtering and retraining on selected data could enable further improvements.\n",
      "\n",
      "  The paper demonstrates how high-quality data can enable smaller models to match larger models \n",
      "for code tasks.\n",
      "  The \n",
      "authors reduce costs by using a smaller LLM to select data, rather than an expensive full evalu-\n",
      "ation.\n",
      "  While recent models can achieve im-\n",
      "pressive performance on snippet generation, there remains a massive step up in difficulty in \n",
      "creating full programs.\n",
      "\n",
      "  The core conclusion is that high-quality data significantly impacts model performance, potentially \n",
      "altering scaling laws.\n",
      "  Phi-1 \n",
      "is then fine-tuned on exercises synthesized by GPT-3.5.\n",
      "----------------------------------------\n",
      "Page 202:\n",
      "Topics:\n",
      "  This demonstrates the substantial potential of reasoning-driven approaches to overcome limita-\n",
      "tions and boost the performance of language models like GPT-4 for programming.\n",
      "  It is available at Hugging Face Spaces at this URL: https://huggingface.co/spaces/\n",
      "bigcode/bigcode-playground\n",
      "\n",
      "  StarCoder\n",
      "Let’s have a look at StarCoder, which is a small model for code generation and quite capable of \n",
      "doing that.\n",
      "  From \n",
      "LangChain, we can call OpenAI’s LLMs, PaLM’s code-bison, or a variety of open-source models, \n",
      "for example, through Replicate, Hugging Face Hub, or – for local models – Llama.cpp, GPT4All, \n",
      "or Hugging Face pipeline integrations.\n",
      "\n",
      "  On coding tasks, \n",
      "Reflexion significantly outperformed previous state-of-the-art models, achieving 91% pass@1 \n",
      "accuracy on the HumanEval benchmark compared to just 67% for GPT-4 as reported originally \n",
      "by OpenAI, although that metric was surpassed later, as the graph shows.\n",
      "\n",
      "  Chapter 6\n",
      "179\n",
      "This reflection and memory of past outcomes guides better future decisions.\n",
      "  I’ve listed a few examples before, such as ChatGPT or Bard.\n",
      "  In the next section, we’ll see how we can generate software code with LLMs and how we can \n",
      "execute this from within LangChain.\n",
      "\n",
      "  As more capable models emerge, thoughtfully integrating AI assistance into developer \n",
      "workflows raises important considerations around human-AI collaboration, establishing trust, \n",
      "and ethical usage.\n",
      "  With careful oversight \n",
      "and further technical development to ensure reliability and transparency, AI programming as-\n",
      "sistants have immense potential to increase productivity by automating tedious tasks, while \n",
      "empowering human developers to focus their creativity on solving complex problems.\n",
      "Subtopics:\n",
      "  The rapid progress in applying large language models to automate programming tasks is encour-\n",
      "aging, but limitations persist, especially in robustness, generalization, and true semantic under-\n",
      "standing.\n",
      "  Rather than \n",
      "just relying on pattern recognition, integrating symbolic reasoning into model architectures and \n",
      "training could provide a path toward more human-like semantic understanding and planning \n",
      "abilities for generating complete programs in the future.\n",
      "\n",
      "  We can use one of the publicly available \n",
      "models for generating code.\n",
      "  However, \n",
      "fully realizing this potential requires continued progress on the technical challenges, further \n",
      "developing standards and best practices, and proactive engagement with the legal and ethical \n",
      "issues surrounding these emerging technologies.\n",
      "\n",
      "  Ongoing research is actively exploring approaches to make these models more \n",
      "accurate, safe, and beneficial for both programmers and society at large.\n",
      "  Writing code with LLMs\n",
      "Let’s start off by applying a model to write code for us.\n",
      "----------------------------------------\n",
      "Page 203:\n",
      "Topics:\n",
      "  We can’t say “write a class that…” but we can ask it to complete a \n",
      "text, where we prompt the model with # dataclass of customer including an alphanumeric \n",
      "id, a name, and a birthday – let’s try this!\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "180\n",
      "This screenshot shows the model in a playground on Hugging Face Spaces:\n",
      "Figure 6.3: StarCoder Models Playground\n",
      "We can choose between different models: StarCoder, StarCoderPlus, and StarCoderBase.\n",
      "  Please \n",
      "note that, as the description says, the StarCoder model is not instruction-tuned, which means \n",
      "that we can’t give it tasks to do.\n",
      "  class Customer:\n",
      "    cust_id : str = field()   # this defines how to create instances from \n",
      "the class; it's called \"field\" because we're creating fields within our \n",
      "object/instance\n",
      "    firstname :\n",
      "  For \n",
      "anything non-trivial, we need to increase the max new tokens setting.\n",
      "\n",
      "  str = \"\"      # these are default values if no value was \n",
      "provided when instantiating objects using this class\n",
      "    lastname : str = \"\"       # note that there can only ever exist one \n",
      "instance variable per line - you'll get errors otherwise!\n",
      "\n",
      "  We can toggle settings for temperature, max new tokens, top-n, and a repetition penalty.\n",
      "Subtopics:\n",
      "  This is useful for storing data in databases or other \n",
      "persistent storages (e.g., files).\n",
      "\n",
      "  I got the following code, which \n",
      "gives us a useful data model for our hypothetical customer:\n",
      "@dataclass(frozen=True)  # frozen means that the class cannot be modified \n",
      "after creation.\n",
      "  We should be getting a code snippet as a response on the webpage.\n",
      "----------------------------------------\n",
      "Page 204:\n",
      "Topics:\n",
      "  Chapter 6\n",
      "181\n",
      "    def __post_init__(self):\n",
      "        \"\"\"This method runs automatically once upon instantiation\"\"\"\n",
      "        self._validate_cust_id()    # validate input before saving \n",
      "anything into memory\n",
      "        self.__set_fullname__()     # set full name based on given inputs\n",
      "    @property        # getter function used by Python interpreter whenever \n",
      "trying to access property directly instead of through.\n",
      "  + [''] * max((1,-len([y.strip().title() for y in \n",
      "re.split(\"[\\W]+\", self.lastname)])) )\n",
      "        print(names)\n",
      "        self.firstname=' '.join(filter(None,[n[:3]for n in \n",
      "reversed(names)])+[''])\n",
      "\n",
      "  == True\n",
      "        except Exception as err:\n",
      "            raise TypeError('First character of CustID should be numeric') \n",
      "from None\n",
      "    #### setter functions go here\n",
      "    def __set_fullname__(self):\n",
      "        '''sets the customers' full name'''\n",
      "        names =\n",
      "  notation e.g.: \n",
      "myobj.firstname vs just myobj.firstname()\n",
      "    def fullname(self)->str:\n",
      "        return f\"{self.lastname}, {self.firstname}\"\n",
      "    ## private methods below here\n",
      "    ### validation functions go here\n",
      "    def _validate_cust_id(self):\n",
      "        assert len(self.cust_id)==8, \\\n",
      "            ValueError(\"Customer ID must have exactly eight characters\")\n",
      "        try:\n",
      "            int(self.cust_id[0])\n",
      "Subtopics:\n",
      "  [x.strip().title() for x in re.split(\"[\\W]+\", self.\n",
      "firstname)]\n",
      "----------------------------------------\n",
      "Page 205:\n",
      "Topics:\n",
      "  You can see this code in the customer.py file in \n",
      "the book’s GitHub repo under the software_development folder.\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "182\n",
      "        self.lastname =''.join([' '*(4-len(l))+' '+ l[-2:]if not \n",
      "l=='I' else '' for l in filter(None,[n[:-2].upper()+','*bool(re.\n",
      "search('[aeiou]',n))+n[-2:].upper() for n in reversed(names)])]).\n",
      "\n",
      "  I would expect the cus-\n",
      "tomer ID to be assigned based on a hashing function – I can’t see why the ID should have exactly \n",
      "eight characters as enforced in the validation function for the property.\n",
      "\n",
      "  We need to add these two lines to the top:\n",
      "import re\n",
      "from dataclasses import dataclass, field\n",
      "This doesn’t throw an error when we run it as is, which is impressive.\n",
      "  Here we implement our data model for customers, a dataclass with\n",
      "fields firstname, lastname, customer_id (a hash).\n",
      "\n",
      "Subtopics:\n",
      "  However, there are a few \n",
      "things not ideal or even wrong with it when we look at it in a bit more detail.\n",
      "\n",
      "  The logic \n",
      "about creating a variable name from firstname and lastname using regular expressions in a \n",
      "non-standard syntax is strange to say the least – and incorrect.\n",
      "  The filter on the reversed name is also highly suspect.\n",
      "\n",
      "  The preceding code is missing the imports, which would usually come before the comment prompt, \n",
      "so I can’t fault the model for it.\n",
      "  The last name gets dropped in \n",
      "the process.\n",
      "  We started off on the wrong foot. \n",
      "\n",
      "  \"\"\"\n",
      "\n",
      "  That’s not realistic. \n",
      "\n",
      "  There are some print() statements left \n",
      "in the code; often this is done for debugging purposes.\n",
      "  rstrip(',')\n",
      "\n",
      "  I’ll leave it at that.\n",
      "  This has given us some promising ideas and a structure to start with, but it’s \n",
      "not production-level code.\n",
      "  We started a code snippet with bad syntax and expected code that works.\n",
      "  Let’s try again, and start with a prompt that is more up to standard:\n",
      "\"\"\"Customer data model.\n",
      "\n",
      "  It doesn’t even work.\n",
      "  Let’s try again to get something more useful out of the model.\n",
      "  There are more problems with this code that affect its correctness and readability, as well as a \n",
      "few other, more crucial problems, including attempting to write read-only attributes.\n",
      "  It’s \n",
      "not ideal to have empty strings as defaults for firstname and lastname.\n",
      "  firstname \n",
      "and lastname in the data class can’t be changed after creation because they are frozen.\n",
      "  I don’t like the commenting style and lack of docstrings.\n",
      "  This shouldn’t be in production code.\n",
      "----------------------------------------\n",
      "Page 206:\n",
      "Topics:\n",
      "  if not hasattr(other,'fullname'):              \n",
      "                setattr(other, 'fullname',f'{other.firstname}{other.\n",
      "lastname}')\n",
      "\n",
      "  class Customer:\n",
      "    \"\"\"A class representing one single customer.\n",
      "  Chapter 6\n",
      "183\n",
      "We get the following result:\n",
      "@dataclass(frozen=True)  # frozen means that the object cannot be modified \n",
      "after creation!\n",
      "\n",
      "  \"\"\"\n",
      "    firstname: str = \"\"\n",
      "    lastname: str = \"\"\n",
      "    id_: int = field(default=-1024, init=False)\n",
      "   \n",
      "    def __post_init__(self):\n",
      "        self.__hash__()\n",
      "       \n",
      "    @property   \n",
      "    def fullname(self)->str:\n",
      "        return f\"{self.firstname} {self.lastname}\"   \n",
      "           \n",
      "    def __eq__(self, other:\"Customer\")->bool:       \n",
      "        if isinstance(other, type(self)):\n",
      "            return all([\n",
      "                getattr(self, attr)==getattr(other,attr)\n",
      "                for attr in [\"firstname\", \"lastname\"]])           \n",
      "        else:\n",
      "            raise TypeError(\"Cannot compare different types\")       \n",
      "               \n",
      "    \n",
      "  def __lt__(self, other:\"Customer\"): \n",
      "        \n",
      "Subtopics:\n",
      "  try:           \n",
      "            assert len({type(x) for x in [self, other]})==1            \n",
      "           \n",
      "            if not hasattr(self,\"fullname\"):               \n",
      "                setattr(self, 'fullname',f'{self.firstname}{self.\n",
      "lastname}')\n",
      "               \n",
      "            \n",
      "----------------------------------------\n",
      "Page 207:\n",
      "Topics:\n",
      "  StarChat, which is based on StarCoder, \n",
      "is available on Hugging Face at https://huggingface.co/spaces/HuggingFaceH4/starchat-\n",
      "playground.\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "184\n",
      "            return self.fullname < other.fullname\n",
      "           \n",
      "        except AssertionError as e: \n",
      "            print('Error:',e )\n",
      "     \n",
      "    def __repr__(self):   \n",
      "        return f\"Customer('{self.firstname}', '{self.\n",
      "\n",
      "  Second, it’s again attempting to set an attribute after initialization of the class that’s supposed \n",
      "to be frozen, showing a lack of understanding of frozen attributes.\n",
      "\n",
      "  After fixing these two problems, we get our first Customer().\n",
      "  You can see this code in the customer2.py file in the book’s GitHub repo, again in the software \n",
      "development folder.\n",
      "\n",
      "  There is some boilerplate code \n",
      "for comparing two different customer objects.\n",
      "  After fixing this, we can initialize our customer, look at the attributes, and compare one customer \n",
      "to another.\n",
      "  First, it’s missing the imports, something I don’t understand given our prompt, which \n",
      "should be a module docstring found at the start of a file.\n",
      "Subtopics:\n",
      "  But then there’s another problem, \n",
      "where the customer ID is referenced with the wrong name, demonstrating a lack of consistency. \n",
      "\n",
      "  lastname}','{hex(abs(self._customer_id))[-6:]})\"  \n",
      "   \n",
      "    def __hash__(self):      \n",
      "        hsh = abs(hash((self.firstname+self.lastname)))\n",
      "  % ((sys.maxsize + \n",
      "1)*2)+ sys.maxsize*3     \n",
      "        self.id_=hsh        \n",
      "       \n",
      "        return hsh\n",
      "It’s good to see the customer ID is created using a hash as expected.\n",
      "  The imports would come right after this. \n",
      "\n",
      "  However, again, there are problems like the ones \n",
      "before.\n",
      "  I can see how this approach is starting to become useful for writing boilerplate code.\n",
      "\n",
      "  StarChat\n",
      "Let’s try an instruction-tuned model so we can give it tasks!\n",
      "----------------------------------------\n",
      "Page 208:\n",
      "Topics:\n",
      "  Chapter 6\n",
      "185\n",
      "This screenshot shows an example in StarChat, but please note that not all the code is visible:\n",
      "Figure 6.4: StarChat implementing a function in Python for calculating prime numbers\n",
      "People who own playgrounds on HuggingFace can pause or take down their play-\n",
      "ground whenever they wish.\n",
      "  If you can’t access the HuggingFace StarChat playground \n",
      "for whatever reason, there are lots of other playgrounds that you could try, first of all, \n",
      "the BigCode playground, which enables access to StarCoderPlus, StarCoderBase, and \n",
      "StarCoder: https://huggingface.co/spaces/bigcode/bigcode-playground\n",
      "You can also find quite a few playgrounds that are made available by other people, \n",
      "for example:\n",
      "•\t\n",
      "A StarCoder playground by Sanjay Wankhede: https://huggingface.co/\n",
      "spaces/sanjayw/starcoder-playground\n",
      "•\t\n",
      "A playground for Code Llama models: https://huggingface.co/spaces/\n",
      "codellama/codellama-playground\n",
      "•\t\n",
      "Joshua Lochner’s AI Code playground that allows switching between three \n",
      "models including CodeGen-Mono 350M: https://huggingface.co/\n",
      "spaces/Xenova/ai-code-playground\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 209:\n",
      "Topics:\n",
      "  Within LangChain, we can use the HuggingFaceHub integration like this:\n",
      "from langchain import HuggingFaceHub\n",
      "llm = HuggingFaceHub(\n",
      "    task=\"text-generation\",\n",
      "    repo_id=\"HuggingFaceH4/starchat-alpha\",\n",
      "    model_kwargs={\n",
      "        \"temperature\": 0.5,\n",
      "        \"max_length\": 1000\n",
      "    }\n",
      ")\n",
      "print(llm(text))\n",
      "\n",
      "  Llama 2\n",
      "Llama 2 is not one of the best models for coding, with a pass@1 of about 29%; however, we can \n",
      "try it out on Hugging Face chat:\n",
      "\n",
      "  As of late 2023, this LangChain integration has had some issues with timeouts – hopefully, this \n",
      "will be fixed soon.\n",
      "  Developing Software with Generative AI\n",
      "186\n",
      "You can find the complete code listing on GitHub.\n",
      "\n",
      "  For this example, which is usually covered in first-year Computer Science courses, no imports \n",
      "are needed.\n",
      "Subtopics:\n",
      "  It executes right away and gives \n",
      "the expected result.\n",
      "  We are not going to use it here.\n",
      "\n",
      "  The algorithm’s implementation is straightforward.\n",
      "  In this case, text is any prompt you want to give the model.\n",
      "\n",
      "----------------------------------------\n",
      "Page 210:\n",
      "Topics:\n",
      "  We can even try a small \n",
      "local model:\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "checkpoint = \"Salesforce/codegen-350M-mono\"\n",
      "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
      "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
      "pipe = pipeline(\n",
      "    task=\"text-generation\",\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    max_new_tokens=500\n",
      "\n",
      "  Llama 2 implements this well and the \n",
      "explanations are spot on.\n",
      "  Chapter 6\n",
      "187\n",
      "Figure 6.5: Hugging Face chat with Llama 2 at https://huggingface.co/chat/\n",
      "Please note that this is only the beginning of the output.\n",
      "  Well done, StarCoder and Llama 2!\n",
      "Subtopics:\n",
      "  Or was this just too easy?\n",
      "Small local model\n",
      "There are so many ways to accomplish code completion or generation.\n",
      "----------------------------------------\n",
      "Page 211:\n",
      "Topics:\n",
      "  Developing Software with Generative AI\n",
      "188\n",
      ")\n",
      "text = \"\"\"\n",
      "def calculate_primes(n):\n",
      "    \\\"\\\"\\\"Create a list of consecutive integers from 2 up to N.\n",
      "    For example:\n",
      "    >>> calculate_primes(20)\n",
      "    \n",
      "  [2, 3, 5, 7, 11, 13, 17, 19]\n",
      "    \\\"\\\"\\\"\n",
      "\"\"\"\n",
      "The preceding code is prompting CodeGen, a model by Salesforce (A Conversational Paradigm for \n",
      "Program Synthesis; Erik Nijkamp and colleagues, 2022).\n",
      "  Alternatively, we can wrap this pipeline via the LangChain integration:\n",
      "from langchain import HuggingFacePipeline\n",
      "llm = HuggingFacePipeline(pipeline=pipe)\n",
      "llm(text)\n",
      "\n",
      "  CodeGen 350 Mono received a pass@1 \n",
      "performance of 12.76% in HumanEval.\n",
      "  We could use this pipeline in a LangChain agent; however, please note that this model is not \n",
      "instruction-tuned, so you cannot give it tasks, only completion tasks.\n",
      "  I am getting something similar to the StarCoder output.\n",
      "  Since this model was released before the HumanEval benchmark, the performance statistics for \n",
      "the benchmark were not part of the initial publication.\n",
      "\n",
      "  As of July 2023, new versions of CodeGen have been released \n",
      "with only 6B parameters, which are very competitive.\n",
      "  This last model was trained on the BigQuery dataset containing C, C++, Go, Java, JavaScript, and \n",
      "Python, as well as the Big Python dataset, which consists of 5.5 TB of Python code.\n",
      "\n",
      "  This clocks in at a performance of 26.13%. \n",
      "\n",
      "Subtopics:\n",
      "  I had to add an import math, but the \n",
      "function works.\n",
      "\n",
      "  There’s also the more convenient constructor method, HuggingFacePipeline.\n",
      "from_model_id().\n",
      "\n",
      "  Output:\n",
      "  You can also use these \n",
      "models for code embeddings. \n",
      "\n",
      "  We can now get the output from the pipeline like this:\n",
      "completion = pipe(text)\n",
      "print(completion[0][\"generated_text\"])\n",
      "\n",
      "  This is a bit verbose.\n",
      "----------------------------------------\n",
      "Page 212:\n",
      "Topics:\n",
      "  Chapter 6\n",
      "189\n",
      "Other models that have been instruction-tuned and are available for chat can act as your techie \n",
      "assistant to help with providing advice, documenting and explaining existing code, or translating \n",
      "code into other programming languages – for the last task, they need to have been trained on \n",
      "enough samples in these languages.\n",
      "\n",
      "  Publicly available \n",
      "models like GPT-3 can produce initial code from prompts, but the results often require refinement \n",
      "before use, as issues like incorrect logic may appear.\n",
      "  Models trained on coding \n",
      "prompts like StarCoder reliably generate valid code-matching prompts and conventions.\n",
      "  Automating software development\n",
      "In LangChain, we have several integrations for code execution, like LLMMathChain, which executes \n",
      "Python code to solve math questions, and BashChain, which executes Bash terminal commands, \n",
      "which can help with system administration tasks.\n",
      "  This approach of solving problems with code can, however, work quite well, as we’ll see here:\n",
      "from langchain.llms.openai import OpenAI\n",
      "from langchain.agents import load_tools, initialize_agent, AgentType\n",
      "llm = OpenAI()\n",
      "tools = load_tools([\"python_repl\"])\n",
      "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_\n",
      "DESCRIPTION, verbose=True)\n",
      "result = agent(\"What are the prime numbers until 20?\")\n",
      "print(result)\n",
      "\n",
      "  We can see how the prime number calculations get processed quite well under the hood between \n",
      "OpenAI’s LLM and the Python interpreter:\n",
      "Entering new AgentExecutor chain...\n",
      "I need to find a way to check if a number is prime\n",
      "\n",
      "Subtopics:\n",
      "  Fine-tuning specifically for programming \n",
      "tasks significantly improves control, accuracy, and task completion.\n",
      "  Smaller \n",
      "models are also capable options for lightweight code generation.\n",
      "\n",
      "  The discussion should serve as an introductory overview of code generation with \n",
      "LLMs, from prompting considerations to execution and real-world viability.\n",
      "  Please note that the approach taken here is a bit naïve; however, it is a good way to get started, \n",
      "nonetheless.\n",
      "  Let’s now try to implement a feedback cycle for code development, where we validate and run \n",
      "the code and change it based on feedback.\n",
      "\n",
      "  However, while useful for problem-solving, \n",
      "these don’t address the larger software development process.\n",
      "\n",
      "----------------------------------------\n",
      "Page 213:\n",
      "Topics:\n",
      "  This could involve an interactive loop where the LLM generates draft code, a human \n",
      "provides feedback steering it toward readable, maintainable code, and the model refines its output \n",
      "accordingly.\n",
      "  []\n",
      "for i in range(2, 21):\n",
      "    if is_prime(i):\n",
      "        prime_numbers.append(i)\n",
      "Observation:\n",
      "\n",
      "  The human developer provides high-level strategic guidance while the LLM handles \n",
      "the grunt work of writing code.\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "190\n",
      "Action: Python_REPL\n",
      "Action Input:\n",
      "def is_prime(n):\n",
      "    for i in range(2, n):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "Observation:\n",
      "\n",
      "  {'input': 'What are the prime numbers until 20?', 'output': '2, 3, 5, 7, \n",
      "11, 13, 17, 19'}\n",
      "We get to the right answer about the prime numbers.\n",
      "  Thought: I now know the prime numbers until 20\n",
      "Final Answer: 2, 3, 5, 7, 11, 13, 17, 19\n",
      "Finished chain.\n",
      "\n",
      "Subtopics:\n",
      "  The code generation approach can work for simple cases.\n",
      "  LLM can produce correct prime number \n",
      "calculations.\n",
      "  Thought: I need to loop through the numbers to check if they are prime\n",
      "Action: Python_REPL\n",
      "Action Input:\n",
      "prime_numbers =\n",
      "  There are a few interesting \n",
      "implementations around for this.\n",
      "\n",
      "  But real-world software \n",
      "demands modular, well-structured design with a separation of concerns.\n",
      "\n",
      "  To automate software creation rather than just problem-solving, we need more sophisticated \n",
      "approaches.\n",
      "  The next frontier is developing frameworks that enable human-LLM collaboration or – more \n",
      "generally – feedback loops for efficient, robust software delivery.\n",
      "----------------------------------------\n",
      "Page 214:\n",
      "Topics:\n",
      "  17100\n",
      "\n",
      "  \"\"\"\n",
      "    company = SoftwareCompany()\n",
      "    company.hire([ProductManager(), Architect(), ProjectManager(), \n",
      "Engineer()])\n",
      "    company.invest(investment)\n",
      "    company.start_project(idea)\n",
      "    await company.run(n_round=n_round)\n",
      "\n",
      "  Developer-friendly \n",
      "workflow.\n",
      "45600\n",
      "MetaGPT\n",
      "https://github.com/geekan/\n",
      "MetaGPT\n",
      "Alexander \n",
      "Wu\n",
      "Multiple GPT agents play \n",
      "development roles based on \n",
      "a team Standard Operating \n",
      "Procedure (SOP).\n",
      "\n",
      "  Chapter 6\n",
      "191\n",
      "For example, the MetaGPT library approaches this with an agent simulation, where different \n",
      "agents represent job roles in a company or IT department:\n",
      "from metagpt.software_company import SoftwareCompany\n",
      "from metagpt.roles import ProjectManager, ProductManager, Architect, \n",
      "Engineer\n",
      "async def startup(idea: str, investment: float = 3.0, n_round: int = 5):\n",
      "    \"\"\"Run a startup.\n",
      "  30700\n",
      "ChatDev\n",
      "https://github.com/OpenBMB/\n",
      "ChatDev\n",
      "OpenBMB \n",
      "(Open Lab \n",
      "for Big \n",
      "Model Base)\n",
      "Multi-agent organization that \n",
      "collaborates via meetings.\n",
      "\n",
      "  This table gives an overview of a few projects:\n",
      "project\n",
      "maintainer\n",
      "description\n",
      "stars\n",
      "GPT Engineer\n",
      "https://github.com/AntonOsika/\n",
      "gpt-engineer\n",
      "Anton Osika\n",
      "Generates full codebases from \n",
      "prompts.\n",
      "  Another library for automated software \n",
      "development is llm-strategy by Andreas Kirsch, which generates code for data classes using \n",
      "decorator patterns.\n",
      "\n",
      "Subtopics:\n",
      "  Be a boss.\n",
      "  This is an example from the MetaGPT documentation.\n",
      "  This is an inspiring use case of an agent simulation.\n",
      "  You need to have MetaGPT installed for \n",
      "this to work.\n",
      "\n",
      "----------------------------------------\n",
      "Page 215:\n",
      "Topics:\n",
      "  The key steps involve the LLM breaking down the software project into subtasks through prompts \n",
      "and then attempting to complete each step.\n",
      "  After executing each subtask, the LLM then assesses if it has completed successfully.\n",
      "  Runs code against tests.\n",
      "480\n",
      "LangChain Coder\n",
      "https://github.com/haseeb-\n",
      "heaven/LangChain-Coder\n",
      "Haseeb \n",
      "Heaven\n",
      "Web code generation/\n",
      "completion with OpenAI and \n",
      "Vertex AI.\n",
      "58\n",
      "Code-it\n",
      "https://github.com/ChuloAI/\n",
      "code-it\n",
      "ChuloAI\n",
      "Iteratively refines code by \n",
      "steering LLM prompts based \n",
      "on execution.\n",
      "46\n",
      "Table 6.6: Overview of different LLM software development projects\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "192\n",
      "GPT Pilot\n",
      "https://github.com/Pythagora-\n",
      "io/gpt-pilot\n",
      "Pythagora\n",
      "Human oversees step-by-step \n",
      "coding toward production \n",
      "apps.\n",
      "14800\n",
      "DevOpsGPT\n",
      "https://github.com/kuafuai/\n",
      "DevOpsGPT\n",
      "KuafuAI\n",
      "Converts requirements to \n",
      "working software with LLMs \n",
      "and DevOps.\n",
      "5100\n",
      "Code Interpreter API\n",
      "https://github.com/shroominic/\n",
      "codeinterpreter-api/\n",
      "Dominic \n",
      "Bäumer\n",
      "Executes Python from \n",
      "prompts locally with \n",
      "sandboxing.\n",
      "3400\n",
      "CodiumAI PR-Agent\n",
      "https://github.com/Codium-ai/\n",
      "pr-agent\n",
      "Codium\n",
      "Analyzes pull requests \n",
      "and provides auto-review \n",
      "commands.\n",
      "2600\n",
      "GPTeam\n",
      "https://github.com/101dotxyz/\n",
      "GPTeam\n",
      "101dotxyz\n",
      "Collaborative agents with \n",
      "memory and reflection.\n",
      "1400\n",
      "CodeT\n",
      "https://github.com/microsoft/\n",
      "CodeT/tree/main/CodeT\n",
      "Microsoft \n",
      "Research\n",
      "Generates code and tests. \n",
      "\n",
      "Subtopics:\n",
      "  This feedback loop of planning, attempting, and \n",
      "reviewing allows it to iteratively refine its process.\n",
      "\n",
      "  If not, it \n",
      "tries to debug the issue or reformulates the plan.\n",
      "  For example, prompts can instruct the model to set \n",
      "up directories, install dependencies, write boilerplate code, and so on.\n",
      "\n",
      "----------------------------------------\n",
      "Page 216:\n",
      "Topics:\n",
      "  We can implement a simple feedback loop in various ways in LangChain, for example, using the \n",
      "PlanAndExecute chain, a ZeroShotAgent, or BabyAGI. \n",
      "\n",
      "  Automatic software development with LLMs can also be explored with projects such as Auto-GPT \n",
      "or Baby-GPT.\n",
      "  Chapter 6\n",
      "193\n",
      "The code-It project by Paolo Rechia and GPT Engineer by Anton Osika both follow a pattern as \n",
      "illustrated in this graph for Code-It (source: https://github.com/ChuloAI/code-it):\n",
      "Figure 6.6: Code-It control flow\n",
      "Many of these steps consist of specific prompts that are sent to LLMs with instructions to break \n",
      "down the project or set up the environment.\n",
      "Subtopics:\n",
      "  However, these systems often get stuck in failure loops.\n",
      "  The agent architecture is \n",
      "key to the robustness of the system.\n",
      "\n",
      "  It’s quite impressive to implement the full feedback \n",
      "loop with all the tools.\n",
      "\n",
      "----------------------------------------\n",
      "Page 217:\n",
      "Topics:\n",
      "  The main idea is to set up a chain and execute it with the objective of writing software, like this:\n",
      "from langchain import OpenAI\n",
      "from langchain_experimental.plan_and_execute import load_chat_planner, \n",
      "load_agent_executor, PlanAndExecute\n",
      "llm = OpenAI()\n",
      "planner = load_chat_planner(llm)\n",
      "executor = load_agent_executor(\n",
      "    llm,\n",
      "    tools,\n",
      "    verbose=True,\n",
      ")\n",
      "agent_executor = PlanAndExecute(\n",
      "    planner=planner,\n",
      "    executor=executor,\n",
      "    verbose=True,\n",
      "    handle_parsing_errors=\"Check your output and make sure it conforms!\n",
      "  \",\n",
      "    return_intermediate_steps=True\n",
      ")\n",
      "agent_executor.run(\"Write a tetris game in python!\")\n",
      "\n",
      "  One thing we need is clear instructions for a language model to write Python code in a certain \n",
      "form – we can reference syntax guidelines, for example:\n",
      "from langchain import PromptTemplate, LLMChain, OpenAI\n",
      "\n",
      "  In the code on GitHub, you can \n",
      "see different architectures to try out.\n",
      "\n",
      "  As mentioned already, the code \n",
      "on GitHub features many other implementation options; for example, agent \n",
      "architectures can be found there as well.\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "194\n",
      "We’ve discussed the basics of these two agent architectures in Chapter 5, Building a Chatbot like \n",
      "ChatGPT.\n",
      "Subtopics:\n",
      "  There are a few more pieces to this implementation, but simple work like this could already write \n",
      "some code, depending on the instructions that we give.\n",
      "\n",
      "  Let’s go with PlanAndExecute, which is quite common.\n",
      "  Since I just want to show the idea here, I am omitting defining the tools \n",
      "for now – we'll come to this in a moment.\n",
      "----------------------------------------\n",
      "Page 218:\n",
      "Topics:\n",
      "  Chapter 6\n",
      "195\n",
      "DEV_PROMPT = (\n",
      "    \"You are a software engineer who writes Python code given tasks or \n",
      "objectives.\n",
      "  \"\n",
      "    \"Come up with a python code for this task: {task}\"\n",
      "    \"Please use PEP8 syntax and comments!\"\n",
      ")\n",
      "software_prompt = PromptTemplate.from_template(DEV_PROMPT)\n",
      "software_llm = LLMChain(\n",
      "    llm=OpenAI(\n",
      "        temperature=0,\n",
      "        max_tokens=1000\n",
      "    ),\n",
      "    prompt=software_prompt\n",
      ")\n",
      "\n",
      "  Models like Codex, PythonCoder, and \n",
      "AlphaCode are designed for code generation capabilities.\n",
      "\n",
      "  We also need to execute the code to test \n",
      "it and provide meaningful feedback to the LLM.\n",
      "  For execution and feedback, the LLM itself does not have inherent capabilities to save files, run \n",
      "programs, or integrate with external environments.\n",
      "  That’s where LangChain’s tools come in.\n",
      "\n",
      "  The LLM can then generate enhanced code incorporating this \n",
      "feedback.\n",
      "\n",
      "  The tools argument to the executor allows specifying Python modules, libraries, and other re-\n",
      "sources that can extend the LLM’s reach.\n",
      "  Based on the tool outputs, we can provide feedback to the LLM on which parts of the code worked \n",
      "and which need improvement.\n",
      "Subtopics:\n",
      "  When using LLMs for code generation, it’s important to choose a model architecture that is opti-\n",
      "mized for producing software code specifically.\n",
      "  I’ve chosen a longer context, \n",
      "so we don’t get cut off in the middle of a function, and a low temperature, so it doesn’t get too wild.\n",
      "\n",
      "  For example, we can use tools to write the code to file, \n",
      "execute it with different inputs, capture the outputs, check correctness, analyze style, and more.\n",
      "\n",
      "  Models trained on more general textual data may \n",
      "not reliably generate syntactically correct and logically sound code.\n",
      "  However, just generating raw code text is not sufficient.\n",
      "  This allows us to iteratively refine and improve \n",
      "the code quality.\n",
      "\n",
      "  We need an LLM that has seen many code examples during its training and can thus generate \n",
      "coherent functions, classes, control structures, and so on.\n",
      "----------------------------------------\n",
      "Page 219:\n",
      "Topics:\n",
      "  Developing Software with Generative AI\n",
      "196\n",
      "Over multiple generations, the human-LLM loop allows for the creation of well-structured, robust \n",
      "software that meets the desired specifications.\n",
      "  The LLM chain powers the code generation while the execute_code() \n",
      "method handles running it.\n",
      "\n",
      "  The LLM brings raw coding productivity while \n",
      "the tools and human oversight ensure quality.\n",
      "\n",
      "  The human provides the task and validates the results while the LLM handles \n",
      "translating descriptions to code.\n",
      "  Let’s see how we can implement this – let’s define the tools argument as promised:\n",
      "from langchain.tools import Tool\n",
      "from software_development.python_developer import PythonDeveloper, \n",
      "PythonExecutorInput\n",
      "software_dev = PythonDeveloper(llm_chain=software_llm)\n",
      "code_tool = Tool.from_function(\n",
      "    func=software_dev.run,\n",
      "    name=\"PythonREPL\",\n",
      "    description=(\n",
      "        \"You are a software engineer who writes Python code given a \n",
      "function description or task.\n",
      "  Here it goes:\n",
      "class PythonDeveloper():\n",
      "    \"\"\"Execution environment for Python code.\n",
      "Subtopics:\n",
      "  llm_chain = llm_chain\n",
      "\n",
      "  \"\n",
      "    ),\n",
      "    \n",
      "  \"\"\"\n",
      "    def __init__(\n",
      "            self,\n",
      "            llm_chain: Chain,\n",
      "    ):\n",
      "        self.\n",
      "  The main idea is that it provides a pipeline to go from natural language task \n",
      "descriptions to generated Python code to executing that code safely, capturing the output, and \n",
      "validating that it runs.\n",
      "  The PythonDeveloper class has all the logic about taking tasks given in any form and translat-\n",
      "ing them into code.\n",
      "  args_schema=PythonExecutorInput\n",
      ")\n",
      "\n",
      "  This environment enables automating the development cycle of coding and testing from language \n",
      "specifications.\n",
      "----------------------------------------\n",
      "Page 220:\n",
      "Topics:\n",
      "  ns = dict(__file__=filename, __name__=\"__main__\")\n",
      "                function = compile(code, \"<>\", \"exec\")\n",
      "                with redirect_stdout(io.StringIO()) as f:\n",
      "                    exec(function, ns)\n",
      "                    return f.getvalue()\n",
      "I am again leaving out a few pieces – the error handling in particular is very simplistic here.\n",
      "  In the \n",
      "implementation on GitHub, we can distinguish various kinds of errors we are getting, such as these:\n",
      "•\t\n",
      "ModuleNotFoundError:\n",
      "  Chapter 6\n",
      "197\n",
      "    def write_code(self, task: str) -> str:\n",
      "        return self.llm_chain.run(task)\n",
      "    def run(\n",
      "            self,\n",
      "            task: str,\n",
      "    ) -> str:\n",
      "        \"\"\"Generate and Execute Python code.\n",
      "  \"\"\"\n",
      "        code = self.write_code(task)\n",
      "        try:\n",
      "            return self.execute_code(code, \"main.py\")\n",
      "        except Exception as ex:\n",
      "            return str(ex)\n",
      "    def execute_code(self, code: str, filename: str) -> str:\n",
      "        \"\"\"Execute a python code.\n",
      "  •\t\n",
      "FileNotFoundError:\n",
      "  •\t\n",
      "NameError: Using variable names that don’t exist.\n",
      "•\t\n",
      "SyntaxError: The parentheses in the code haven’t been closed or it is not even code.\n",
      "\n",
      "Subtopics:\n",
      "  I’ve implemented logic to install packages for ModuleNotFoundError, and clearer messages for \n",
      "some of these problems.\n",
      "  •\t\n",
      "SystemExit: If something more dramatic happens and Python crashes.\n",
      "\n",
      "  In the case of missing images, we could add a generative image model to \n",
      "create them.\n",
      "  The code relies on files that don’t exist.\n",
      "  I’ve implemented logic to install these packages.\n",
      "\n",
      "  I’ve found a few times that \n",
      "the code tried showing images that were made up.\n",
      "\n",
      "  \"\"\"\n",
      "        try:\n",
      "            with set_directory(Path(self.path)):\n",
      "                \n",
      "  Returning all this as enriched feedback to the code generation results in increasingly \n",
      "specific output such as this:\n",
      "\n",
      "  This means that the code tries to work with packages that we \n",
      "don’t have installed.\n",
      "----------------------------------------\n",
      "Page 221:\n",
      "Topics:\n",
      "  With this out of the way, let’s define tools\n",
      "  •\t\n",
      "Bugs in the code could cause crashes or unwanted behavior on the host machine.\n",
      "•\t\n",
      "Resource usage like CPU, memory, and disk could be unchecked.\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "198\n",
      "Write a basic tetris game in Python with no syntax errors, properly closed \n",
      "strings, brackets, parentheses, quotes, commas, colons, semi-colons, and \n",
      "braces, no other potential syntax errors, and including the necessary \n",
      "imports for the game\n",
      "The Python code itself gets compiled and executed in a subdirectory and we redirect the output \n",
      "of the Python execution to capture it; this is implemented as Python contexts.\n",
      "\n",
      "  For \n",
      "Python, options include RestrictedPython, pychroot, setuptools’ DirectorySandbox, and code-\n",
      "box-api.\n",
      "  We \n",
      "could implement safety and style guardrails similar to what we discussed in Chapter 5, Building \n",
      "a Chatbot like ChatGPT.\n",
      "\n",
      "  So, essentially, any code executed from an LLM has significant power over the local system.\n",
      "  Ideally, LLM-generated code should first be thoroughly inspected and its resource usage profiled, \n",
      "vulnerabilities scanned, and functionality unit tested before being run on production systems.\n",
      "  There are several security risks involved:\n",
      "•\t\n",
      "The LLM could produce code with vulnerabilities or backdoors either inadvertently due \n",
      "to its training or maliciously if adversarially manipulated.\n",
      "\n",
      "Subtopics:\n",
      "  When generating code using large language models, it is important to be careful about running \n",
      "that code, especially on a production system.\n",
      "  There are tools and frameworks that can sandbox generated code and limit its authority.\n",
      "  Risks \n",
      "like crashes, hacks, and data loss from blindly running unverified code could be substantial.\n",
      "  These allow enclosing the code in virtual environments or restricting access to sensitive \n",
      "OS functions.\n",
      "\n",
      "  This \n",
      "makes security a major concern compared to running code in isolated environments like note-\n",
      "books or sandboxes.\n",
      "\n",
      "  While sandboxing tools can provide additional protection, it’s best to be cautious and only execute \n",
      "LLM code in disposable or isolated environments until trust in the model is established.\n",
      "  Safe \n",
      "practices are crucial as LLMs become part of software pipelines.\n",
      "\n",
      "  It is not sandboxed or containerized.\n",
      "\n",
      "  :\n",
      "ddg_search = DuckDuckGoSearchResults()\n",
      "tools = [\n",
      "    codetool,\n",
      "\n",
      "  •\t\n",
      "The generated code interacts directly with the underlying operating system, allowing \n",
      "access to files, networks, and so on.\n",
      "----------------------------------------\n",
      "Page 222:\n",
      "Topics:\n",
      "  import pygame\n",
      "import sys\n",
      "# Initialize pygame\n",
      "pygame.init()\n",
      "# Set the window size\n",
      "window_width\n",
      "  When working with this tool, I’ve seen a few implementations of Rock, Paper, Scissors \n",
      "instead of Tetris, so it’s important to understand the objective.\n",
      "\n",
      "  = 600\n",
      "# Create the window\n",
      "\n",
      "  Chapter 6\n",
      "199\n",
      "    Tool(\n",
      "        name=\"DDGSearch\",\n",
      "        func=ddg_search.run,\n",
      "        description=(\n",
      "            \"Useful for research and understanding background of \n",
      "objectives.\n",
      "  When running our agent executor with the objective of implementing Tetris, the results are a \n",
      "bit different every time.\n",
      "  = 800\n",
      "\n",
      "  # This code is written in PEP8 syntax and includes comments to explain the \n",
      "code\n",
      "# Import the necessary modules\n",
      "\n",
      "Subtopics:\n",
      "  \"\n",
      "        )\n",
      "    )\n",
      "]\n",
      "An internet search is worth adding to ensure we are implementing something related to our ob-\n",
      "jective.\n",
      "  window_height\n",
      "  I find here that the pygame library is installed.\n",
      "  \"\n",
      "            \"Input: an objective.\n",
      "  \"\n",
      "            \"Output: background information about the objective.\n",
      "  We can see the agent activity in the intermittent results.\n",
      "  The following code snippet is not the final product, \n",
      "but it brings up a window:\n",
      "\n",
      "  Looking at this, \n",
      "I am observing searches for requirements and game mechanics, and code is repeatedly being \n",
      "produced and executed.\n",
      "\n",
      "----------------------------------------\n",
      "Page 223:\n",
      "Topics:\n",
      "  You can see this in the GitHub repo.\n",
      "\n",
      "  However, in \n",
      "terms of functionality, it’s very far from Tetris.\n",
      "\n",
      "  It’s also amazingly simple and basic, consisting only of about 340 lines of Python, including \n",
      "the imports, which you can find on GitHub.\n",
      "\n",
      "  Developing Software with Generative AI\n",
      "200\n",
      "window = pygame.display.set_mode((window_width, window_height))\n",
      "# Set the window title\n",
      "pygame.display.set_caption('My Game')\n",
      "# Set the background color\n",
      "background_color = (255, 255, 255)\n",
      "\n",
      "  QUIT:\n",
      "            pygame.quit()\n",
      "            sys.exit()\n",
      "    # Fill the background with the background color\n",
      "    window.fill(background_color)\n",
      "    # Update the display\n",
      "    pygame.display.update()\n",
      "\n",
      "Subtopics:\n",
      "  I think a better approach could be to break down all the functionality into functions and maintain \n",
      "a list of functions to call, which can be used in all subsequent generations of code.\n",
      "  We could also define additional tools such as a planner that breaks down the tasks into functions. \n",
      "\n",
      "  An advantage to \n",
      "our approach is, however, that it’s easy to debug, since all steps including searches and generated \n",
      "code are written to a log file in the implementation.\n",
      "\n",
      "  This implementation of a fully automated agent for software development is still quite experimen-\n",
      "tal.\n",
      "  # Main game loop\n",
      "while True:\n",
      "    # Check for events\n",
      "    for event in pygame.event.get():\n",
      "        # Quit if the user closes the window\n",
      "        if event.type == pygame.\n",
      "  Finally, we could try a test-driven development approach or have a human give feedback rather \n",
      "than a fully automated process. \n",
      "\n",
      "  The code is not too bad in terms of syntax – I guess the prompt must have helped.\n",
      "----------------------------------------\n",
      "Page 224:\n",
      "Topics:\n",
      "  In Chapter 7, LLMs for Data Science, we’ll work with LLMs \n",
      "for applications in data science and machine learning.\n",
      "\n",
      "  I’d recommend \n",
      "you go back to the corresponding sections of this chapter if you are unsure about any of them:\n",
      "1.\t\n",
      "\n",
      "  What options do we have available to establish a feedback loop for writing code?\n",
      "6.\t\n",
      "\n",
      "  4.\t\n",
      "\n",
      "  Chapter 6\n",
      "201\n",
      "LLMs can produce reasonable sets of test cases from high-level descriptions.\n",
      "  How do you measure a code LLM’s performance on coding tasks?\n",
      "3.\t\n",
      "Which code LLMs are available, both open- and closed-source?\n",
      "\n",
      "  How does the Reflexion strategy work?\n",
      "5.\t\n",
      "\n",
      "  What do you think is the impact of generative AI on software development?\n",
      "\n",
      "  2.\t\n",
      "\n",
      "  Generating implementation code \n",
      "first and then deriving tests risks baking in incorrect behavior.\n",
      "  Explicitly providing feedback helps the LLM improve over iterations.\n",
      "\n",
      "  As for now, human guidance on high-level design and rigorous \n",
      "review seem indispensable to prevent subtle errors, and the future likely involves collaboration \n",
      "between humans and AI.\n",
      "\n",
      "Subtopics:\n",
      "  We didn’t implement semantic code search in this chapter since it’s very similar to the chatbot \n",
      "implementation in the previous chapter.\n",
      "  The process works in small \n",
      "steps – generate a test, review and enhance it, and use the final version’s changes to inform the \n",
      "next test or code generation.\n",
      "  We’ve seen how the suggested solutions seem superficially correct but don’t perform the task \n",
      "or are full of bugs.\n",
      "  Summary\n",
      "In this chapter, we’ve discussed LLMs for source code and how they can help in developing soft-\n",
      "ware.\n",
      "  What can LLMs do to help in software development?\n",
      "\n",
      "  Alternatively, we could have used human feedback \n",
      "or implemented tests.\n",
      "\n",
      "  But human oversight \n",
      "is essential to catch subtle mistakes and validate completeness.\n",
      "  This could have significant implications re-\n",
      "garding safety and reliability.\n",
      "  We’ve applied a few models for code generation using naïve approaches and we’ve \n",
      "evaluated them qualitatively.\n",
      "  The right flow is specifying expect-\n",
      "ed behavior, vetting test cases, and then creating code that passes.\n",
      "  There are quite a few areas where LLMs can benefit software development, mostly as coding \n",
      "assistants.\n",
      "  Questions\n",
      "Please look to see if you can produce the answers to these questions from memory.\n",
      "  However, we can get a sense that – with the right architectural setup – LLMs \n",
      "could feasibly learn to automate coding pipelines.\n",
      "  In programming, as we’ve seen, compiler errors and results of code \n",
      "execution can be used to provide feedback.\n",
      "----------------------------------------\n",
      "Page 225:\n",
      "Topics:\n",
      "  Developing Software with Generative AI\n",
      "202\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 226:\n",
      "Topics:\n",
      "  We can set up agents to run SQL or tabular data in pandas.\n",
      "  7\n",
      "LLMs for Data Science\n",
      "This chapter is about how generative AI can automate data science.\n",
      "  The main sections in this chapter are:\n",
      "•\t\n",
      "The impact of generative models on data science\n",
      "•\t\n",
      "Automated data science\n",
      "•\t\n",
      "Using agents to answer data science questions\n",
      "•\t\n",
      "Data exploration with LLMs\n",
      "\n",
      "  In this chapter, we’ll first discuss how data science is affected by generative AI and then cover an \n",
      "overview of automation in data science.\n",
      "\n",
      "  Next, we’ll discuss how we can use code generation and tools in diverse ways to answer questions \n",
      "related to data science.\n",
      "  Throughout the chapter, we’ll work on different approaches to doing data science with LLMs, \n",
      "which you can find in the data_science directory in the GitHub repository for this book at \n",
      "https://github.com/benman1/generative_ai_with_langchain.\n",
      "\n",
      "  A lot of the \n",
      "current approaches that fall within the domain of Automated Machine Learning (AutoML) can \n",
      "help data scientists increase their productivity and make data science processes more repeatable. \n",
      "\n",
      "Subtopics:\n",
      "  Generative AI, in particular \n",
      "LLMs, has the potential to accelerate scientific progress across various domains, especially by \n",
      "providing efficient analysis of research data and aiding in literature review processes.\n",
      "  This can come in the form of doing a simulation or enriching our dataset \n",
      "with additional information.\n",
      "  We’ll see how we can ask \n",
      "questions about the dataset, statistical questions about the data, or ask for visualizations.\n",
      "\n",
      "  Finally, we’ll shift the focus to the exploratory analysis of structured \n",
      "datasets.\n",
      "----------------------------------------\n",
      "Page 227:\n",
      "Topics:\n",
      "  Another area where generative AI can benefit researchers is in performing literature reviews and \n",
      "identifying research gaps.\n",
      "  •\t\n",
      "Creating new features from existing data: Generative AI can be used to create new fea-\n",
      "tures from existing data.\n",
      "  By automating certain aspects of data anal-\n",
      "ysis, generative AI saves time and resources, allowing researchers to focus on higher-level tasks.\n",
      "\n",
      "  The impact of generative models on data science\n",
      "Generative AI and LLMs like GPT-4 have brought about significant changes in the field of data \n",
      "science and analysis.\n",
      "  LLMs for Data Science\n",
      "204\n",
      "Before delving into how data science can be automated, let’s start by discussing how generative \n",
      "AI will impact data science!\n",
      "\n",
      "  We’ve looked at this aspect of using generative AI models in Chapter 4, Building Capable Assistants.\n",
      "\n",
      "  Other data science use cases for generative AI are:\n",
      "•\t\n",
      "Automatically generating synthetic data: Generative AI can be used to automatically \n",
      "generate synthetic data that can be used to train machine learning models.\n",
      "  According to recent reports by the likes of McKinsey and KPMG, the consequences of AI relate to \n",
      "what data scientists will work on, how they will work, and who can work on data science tasks. \n",
      "\n",
      "  Generative \n",
      "AI models, such as ChatGPT, can understand and generate human-like responses, making them \n",
      "valuable tools for enhancing research productivity.\n",
      "\n",
      "Subtopics:\n",
      "  Generative AI plays a crucial role in analyzing and interpreting research data.\n",
      "  These models, particularly LLMs, can revolutionize all the steps involved in \n",
      "data science in many ways, offering exciting opportunities for researchers and analysts.\n",
      "  ChatGPT and similar models can summarize vast amounts of informa-\n",
      "tion from academic papers or articles, providing a concise overview of existing knowledge.\n",
      "  This \n",
      "helps researchers identify gaps in the literature and guide their own investigations more efficiently. \n",
      "\n",
      "  The principal areas of key impact include:\n",
      "\n",
      "  This can be helpful for businesses that are looking \n",
      "to gain new insights from their data.\n",
      "\n",
      "  This can be helpful for businesses that are looking to improve \n",
      "the accuracy of their machine learning models.\n",
      "\n",
      "  •\t\n",
      "Identifying patterns in data: Generative AI can be used to identify patterns in data that \n",
      "would not be visible to human analysts.\n",
      "  This can be \n",
      "helpful for businesses that do not have access to enormous amounts of real-world data.\n",
      "\n",
      "  These models can \n",
      "assist in data exploration, uncover hidden patterns or correlations, and provide insights that \n",
      "may not be apparent through traditional methods.\n",
      "----------------------------------------\n",
      "Page 228:\n",
      "Topics:\n",
      "  •\t\n",
      "Limitations remain: Current models still have accuracy limitations, bias issues, and lack \n",
      "of controllability.\n",
      "  Regarding the democratization and innovation of data science, more specifically, generative AI \n",
      "is also having an impact on the way that data is visualized.\n",
      "  •\t\n",
      "Changes to data science skills: Demand may shift from coding expertise to abilities in data \n",
      "governance, ethics, translating business problems, and overseeing AI systems.\n",
      "\n",
      "  In the past, data visualizations were \n",
      "often static and two-dimensional.\n",
      "  This expands the use of AI beyond \n",
      "data scientists.\n",
      "\n",
      "  Chapter 7\n",
      "205\n",
      "•\t\n",
      "Democratization of AI: Generative models allow many more people to leverage AI by \n",
      "generating text, code, and data from simple prompts.\n",
      "  By auto-generating code, data, and text, generative AI can accel-\n",
      "erate development and analysis workflows.\n",
      "  Again, one of the biggest changes that generative AI is bringing about is the democratization of data \n",
      "science.\n",
      "  •\t\n",
      "Importance of governance: Rigorous governance over development and ethical use of \n",
      "generative AI models will be critical to maintaining stakeholder trust.\n",
      "\n",
      "  However, generative AI can be used to create interactive and \n",
      "three-dimensional visualizations that can help to make data more accessible and understandable. \n",
      "\n",
      "  However, generative AI is making it possible for people with \n",
      "less technical expertise to create and use data models.\n",
      "  •\t\n",
      "Innovation in data science: Generative AI is bringing about the ability to explore data in \n",
      "new and more creative ways, and generate new hypotheses and insights that would not \n",
      "have been possible with traditional methods\n",
      "•\t\n",
      "Disruption of industries: New applications of generative AI could disrupt industries by \n",
      "automating tasks or enhancing products and services.\n",
      "Subtopics:\n",
      "  In the past, data science was a very specialized field that required a deep understanding \n",
      "of statistics and machine learning.\n",
      "  Data teams will need to identify \n",
      "high-impact use cases.\n",
      "\n",
      "  This is making it easier for people to understand and interpret data, which can lead to better \n",
      "decision-making.\n",
      "\n",
      "  •\t\n",
      "Increased productivity:\n",
      "  Data experts are needed to oversee responsible development.\n",
      "\n",
      "  This allows data scientists and analysts to \n",
      "focus on higher-value tasks.\n",
      "\n",
      "  This is opening up the field of data science \n",
      "to a much wider range of people.\n",
      "\n",
      "----------------------------------------\n",
      "Page 229:\n",
      "Topics:\n",
      "  LLMs for Data Science\n",
      "206\n",
      "LLMs and generative AI can play a crucial role in automated data science by offering several \n",
      "benefits:\n",
      "•\t\n",
      "Natural language interaction: LLMs allow for natural language interaction, enabling users \n",
      "to communicate with the model using plain English or other languages.\n",
      "  Further, we could think that generative AI algorithms should be able to learn from user interac-\n",
      "tions and adapt their recommendations based on individual preferences or past behaviors.\n",
      "  For example, it can generate \n",
      "code such as SQL to retrieve data, clean data, handle missing values, or create visualiza-\n",
      "tions.\n",
      "  •\t\n",
      "Automated report generation: LLMs can generate automated reports summarizing the \n",
      "key findings of EDA.\n",
      "  Finally, generative AI models can identify errors or anomalies in the data during EDA by learning \n",
      "patterns from existing datasets (intelligent error identification).\n",
      "  They \n",
      "improve over time through continuous adaptive learning and user feedback, providing more \n",
      "personalized and useful insights during automated EDA.\n",
      "\n",
      "  Overall, LLMs and generative AI can enhance automated EDA by simplifying user interaction, \n",
      "generating code snippets, identifying errors/anomalies efficiently, automating report generation, \n",
      "facilitating comprehensive data exploration, visualization creation, and adapting to user prefer-\n",
      "ences for more effective analysis of large and complex datasets.\n",
      "\n",
      "  •\t\n",
      "Data exploration and visualization: Generative AI algorithms can explore large datasets \n",
      "comprehensively and generate visualizations that reveal underlying patterns, relationships \n",
      "between variables, outliers, or anomalies in the data automatically.\n",
      "  •\t\n",
      "Code generation: Generative AI can automatically generate code snippets to perform spe-\n",
      "cific analysis tasks during Exploratory Data Analysis (EDA).\n",
      "Subtopics:\n",
      "  Their strength is creativity, not accuracy, and \n",
      "therefore, researchers must exercise critical thinking and ensure that the outputs generated by \n",
      "these models are accurate, unbiased, and aligned with rigorous scientific standards.\n",
      "\n",
      "  As we’ve seen earlier, LLMs work \n",
      "by analogy and struggle with reasoning and math.\n",
      "  They can detect inconsistencies \n",
      "and highlight potential issues quickly and accurately.\n",
      "\n",
      "  This feature saves time and reduces the need for manual coding.\n",
      "\n",
      "  This helps users gain \n",
      "a holistic understanding of the dataset without manually creating each visualization.\n",
      "\n",
      "  However, while these models offer immense potential to enhance research and aid in literature \n",
      "review processes, they should not be treated as infallible sources.\n",
      "  This makes it \n",
      "easier for non-technical users to interact with and explore the data using everyday lan-\n",
      "guage, without requiring expertise in coding or data analysis.\n",
      "\n",
      "  These reports provide insights into various aspects of the dataset, \n",
      "such as statistical summary, correlation analysis, feature importance, and so on, making \n",
      "it easier for users to understand and present their findings.\n",
      "\n",
      "----------------------------------------\n",
      "Page 230:\n",
      "Topics:\n",
      "  Features like Copilot in Microsoft Fabric provide conversational \n",
      "language experiences, allowing users to create dataflows, generate code or entire functions, build \n",
      "machine learning models, visualize results, and even develop custom conversational language \n",
      "experiences.\n",
      "\n",
      "  Chapter 7\n",
      "207\n",
      "One notable example is Microsoft’s Fabric, which incorporates a chat interface powered by gen-\n",
      "erative AI.\n",
      "  By leveraging LLMs like OpenAI \n",
      "models, Fabric enables real-time access to valuable insights.\n",
      "\n",
      "  Therefore, organizations must ensure that they have reliable data \n",
      "pipelines in place and employ data quality management practices while using Fabric for analysis.\n",
      "\n",
      "  While the possibilities of generative AI in data analytics are promising, caution must be exercised. \n",
      "\n",
      "  With the integration of Azure OpenAI Service at every layer, Fabric harnesses generative AI’s power \n",
      "to unlock the full potential of data.\n",
      "  Tasks include data collecting, clean-\n",
      "ing, analyzing, and visualizing.\n",
      "  The reliability and accuracy of LLMs should be verified using first-principles reasoning and rigor-\n",
      "ous analysis.\n",
      "  Automated data science\n",
      "Data science is a field that combines computer science, statistics, and business analytics to extract \n",
      "knowledge and insights from data.\n",
      "  ChatGPT (and Fabric in extension) often produces incorrect SQL queries.\n",
      "Subtopics:\n",
      "  It addresses \n",
      "various aspects of an organization’s analytics needs and provides role-specific experiences for \n",
      "different teams involved in the analytics process, such as data engineers, warehousing profes-\n",
      "sionals, scientists, analysts, and business users.\n",
      "\n",
      "  Data scientists are also tasked with building predictive models \n",
      "to help in decision-making processes.\n",
      "  The responsibilities of a data scientist are wide-ranging and often involve multiple \n",
      "steps that vary depending on the specific role and industry.\n",
      "  This is fine when used by \n",
      "analysts who can check the validity of the output but a total disaster as a self-service analytics tool \n",
      "for non-technical business users.\n",
      "  Fabric stands out among other analytics products due to its comprehensive approach.\n",
      "  They then use this information to help businesses make better \n",
      "decisions.\n",
      "  Data scientists use a variety of tools and techniques to collect, \n",
      "clean, analyze, and visualize data.\n",
      "  While these models have shown their potential in ad hoc analysis, idea generation \n",
      "during research, and summarizing complex analyses, they may not always be suitable as self-ser-\n",
      "vice analytical tools for non-technical users due to the need for validation by domain experts.\n",
      "\n",
      "  All the tasks mentioned are crucial to data science but can \n",
      "be time-consuming and complex.\n",
      "\n",
      "  This allows users to ask data-related questions using natural language and receive \n",
      "instant answers without having to wait in a data request queue.\n",
      "----------------------------------------\n",
      "Page 231:\n",
      "Topics:\n",
      "  Some of the tasks for data science overlap with those of a software developer that we \n",
      "talked about in Chapter 6, Developing Software with Generative AI, namely, writing and deploying \n",
      "software, although with a narrower focus, on models.\n",
      "\n",
      "  This screenshot from the documentation shows the chat feature, the Jupyternaut chat (Jupyter AI):\n",
      "Figure 7.1: Jupyter AI – Jupyternaut chat\n",
      "\n",
      "  Data science platforms like KNIME, H2O, and RapidMiner provide unified analytics engines to \n",
      "preprocess data, extract features, and build models.\n",
      "  Jupyter AI allows conversing with a virtual assistant to \n",
      "explain code, identify errors, and create notebooks. \n",
      "\n",
      "  LLMs integrated into these platforms, such \n",
      "as GitHub Copilot or Jupyter AI, can generate code for data processing, analysis, and visualiza-\n",
      "tion based on natural language prompts.\n",
      "  LLMs for Data Science\n",
      "208\n",
      "Automating various aspects of the data science workflow allows data scientists to focus more on \n",
      "creative problem-solving while enhancing productivity.\n",
      "Subtopics:\n",
      "  Recent tools are making different stages \n",
      "of the process more efficient by enabling faster iterations and less manual coding for common \n",
      "workflows.\n",
      "----------------------------------------\n",
      "Page 232:\n",
      "Topics:\n",
      "  In LangChain, there’s an integration with Zapier, which is an automation tool that can be used \n",
      "to connect different applications and services.\n",
      "  There are many ETL tools, including commercial ones such as AWS Glue, Google Dataflow, Am-\n",
      "azon Simple Workflow Service (SWF), dbt, Fivetran, Microsoft SSIS, IBM InfoSphere DataStage, \n",
      "Talend Open Studio, or open-source tools such as Airflow, Kafka, and Spark.\n",
      "  In the context of data science or analytics, we refer to ETL (extract, transform, and load) as the \n",
      "process that not only takes data from one or more sources (data collection) but also prepares it \n",
      "for specific use cases.\n",
      "\n",
      "  Chapter 7\n",
      "209\n",
      "It should be plain to see that having a chat like that at your fingertips to ask questions, create \n",
      "simple functions, or change existing functions can be a boon to data scientists.\n",
      "\n",
      "  In Python, there are \n",
      "many more tools (too many to list them all), such as pandas for data extraction and processing, \n",
      "and even celery and joblib, which can serve as ETL orchestration tools.\n",
      "\n",
      "  Overall, automated data science can accelerate analytics and ML application development.\n",
      "  In \n",
      "the following sections, we’ll investigate different tasks in turn, and we’ll highlight how generative \n",
      "AI can contribute to improving the workflow and create efficiency gains in areas such as data \n",
      "collection, visualization and EDA, preprocessing and feature engineering, and finally, AutoML. \n",
      "\n",
      "  Data collection\n",
      "Automated data collection is the process of collecting data without human intervention.\n",
      "Subtopics:\n",
      "  The best tool for automatic data collection will depend on the specific needs of the business. \n",
      "\n",
      "  It \n",
      "allows data scientists to focus on higher-value and creative aspects of the process.\n",
      "  Businesses should consider the type of data they need to collect, the volume of data they need to \n",
      "collect, and the budget they have available.\n",
      "\n",
      "  LLMs offer an accelerated way to gather and process data, \n",
      "notably excelling in the organization of unstructured datasets.\n",
      "\n",
      "  It can help businesses to collect data more \n",
      "quickly and efficiently, and it can free up human resources to focus on other tasks.\n",
      "\n",
      "  Let’s look at each of these areas in more detail.\n",
      "\n",
      "  This can be used to automate the process of data \n",
      "collection from a variety of sources.\n",
      "  Automatic \n",
      "data collection can be a valuable tool for businesses.\n",
      "  Democratizing \n",
      "data science for business analysts is also a key motivation behind automating these workflows.\n",
      "----------------------------------------\n",
      "Page 233:\n",
      "Topics:\n",
      "  Automated feature engineering, on the other hand, is becoming \n",
      "essential to leveraging the full power of ML algorithms on complex real-world data.\n",
      "  Automated EDA and visualization refer to the process of using software tools and algorithms to \n",
      "automatically analyze and visualize data, without significant manual intervention.\n",
      "  Preprocessing and feature extraction\n",
      "Automated data preprocessing can include tasks such as data cleaning, data integration, data \n",
      "transformation, and feature extraction.\n",
      "  LLMs for Data Science\n",
      "210\n",
      "Visualization and EDA\n",
      "EDA involves manually exploring and summarizing data to understand its various aspects before \n",
      "performing machine learning tasks.\n",
      "  The use of generative AI in data visualization adds another dimension to automated EDA by gen-\n",
      "erating new visualizations based on user prompts, making the visualization and interpretation \n",
      "of data even more accessible.\n",
      "\n",
      "  However, with the advent of large datasets and the \n",
      "need for efficient analysis, automated EDA has become important.\n",
      "\n",
      "  It is related to the transform step in ETL, so there’s a lot of \n",
      "overlap in tools and techniques.\n",
      "Subtopics:\n",
      "  It helps in identifying patterns, detecting inconsistencies, \n",
      "testing assumptions, and gaining insights.\n",
      "  These tools also enable the more efficient exploration of complex datasets by generating inter-\n",
      "active visualizations that provide a comprehensive overview of the data.\n",
      "\n",
      "  During preprocessing and feature engineering, LLMs automate the cleaning, integration, and \n",
      "transformation of data.\n",
      "  The gains in efficiency must not undermine \n",
      "the need for checks against introducing inadvertent biases or errors through automation.\n",
      "\n",
      "  They can speed up the data analysis process, reducing the time spent \n",
      "on tasks like data cleaning, handling missing values, outlier detection, and feature engineering. \n",
      "\n",
      "  This includes \n",
      "removing errors and inconsistencies from the data and converting it into a format compatible \n",
      "with the analytical tools that will be used.\n",
      "\n",
      "  The adoption of these models promises to streamline processes, thereby \n",
      "improving privacy management by minimizing human handling of sensitive information during \n",
      "these stages.\n",
      "  While boosting flexibility and performance in preprocessing tasks, there remains a \n",
      "challenge in ensuring the safety and interpretability of automatically engineered features, which \n",
      "may not be as transparent as manually created ones.\n",
      "  These tools \n",
      "provide several benefits.\n",
      "----------------------------------------\n",
      "Page 234:\n",
      "Topics:\n",
      "  As one of the early broad-framework attempts, developed at the University of \n",
      "Waikato, it was penned in Java to automate the process for tabular data within the Weka machine \n",
      "learning suite.\n",
      "\n",
      "  Chapter 7\n",
      "211\n",
      "AutoML\n",
      "AutoML frameworks represent a noteworthy leap in the evolution of machine learning.\n",
      "  The basic idea of AutoML is illustrated in this diagram from the GitHub repo of the mljar AutoML \n",
      "library (source: https://github.com/mljar/mljar-supervised):\n",
      "Figure 7.2: How AutoML works\n",
      "Key to the value offered by AutoML systems is their contributory effect on ease of use and pro-\n",
      "ductivity growth.\n",
      "  The genesis of these frameworks can be traced back to innovations \n",
      "like Auto-WEKA.\n",
      "Subtopics:\n",
      "  These frameworks not only enhance \n",
      "the pace but also potentially elevate the quality of machine learning models.\n",
      "\n",
      "  Within typical developer environments, these systems enable the rapid iden-\n",
      "tification and productionizing of machine learning models, simplifying both comprehension \n",
      "and deployment processes.\n",
      "  By stream-\n",
      "lining the complete model development cycle, including tasks such as data cleaning, feature selec-\n",
      "tion, model training, and hyperparameter tuning, AutoML frameworks significantly economize on \n",
      "the time and effort customarily expended by data scientists.\n",
      "----------------------------------------\n",
      "Page 235:\n",
      "Topics:\n",
      "  Solutions like Google AutoML, \n",
      "Azure AutoML, and H2O’s offering are at the forefront of this revolution, delivering capabilities \n",
      "that extend ML accessibility to individuals beyond expert data scientists.\n",
      "\n",
      "  Frameworks such as PyCaret facilitate training multiple \n",
      "models concurrently with minimal code while maintaining a focus on time series data through \n",
      "specialized projects like Nixtla’s StatsForecast and MLForecast.\n",
      "\n",
      "  Moreover, while their impact through time savings and democratization of ML practices makes \n",
      "machine learning more accessible for those without extensive experience, their efficacy in auto-\n",
      "mating ML tasks can face limitations due to inherent task complexities.\n",
      "\n",
      "  In terms of safety, automated systems must be designed with fail-\n",
      "safe mechanisms to prevent the propagation of errors across successive layers of ML workflows. \n",
      "\n",
      "  More contemporary AutoML advancements have \n",
      "harnessed neural architecture search techniques to encapsulate vast portions of the ML pipeline, \n",
      "including unstructured data types like images, video, and audio.\n",
      "  LLMs for Data Science\n",
      "212\n",
      "Since the release of Auto-Weka, the landscape has vastly diversified with powerful frameworks \n",
      "such as auto-sklearn, autokeras, NASLib, Auto-PyTorch, TPOT, Optuna, AutoGluon, and Ray \n",
      "(tune).\n",
      "  The attributes characterizing AutoML frameworks are manifold: they provide deployment ca-\n",
      "pacities wherein certain solutions enable direct production embedding, especially cloud-based \n",
      "ones; others necessitate exportation in formats compatible with platforms like TensorFlow.\n",
      "Subtopics:\n",
      "  By conducting elaborate hyperparameter searches, their performance can meet \n",
      "or even surpass manual interventions.\n",
      "  The impact on privacy is consider-\n",
      "able; AutoML systems that utilize generative models can create synthetic data, reducing reliance \n",
      "on personal data repositories.\n",
      "  The flexibility offered by AutoML through LLM integration improves competitive performance \n",
      "by making it possible for non-experts to achieve expert-level model tuning. \n",
      "\n",
      "  Spawning across various programming languages, these frameworks lend themselves \n",
      "to an eclectic array of machine learning tasks.\n",
      "  Monitoring post-deployment \n",
      "is another operational feature to ensure sustained model performance over time.\n",
      "\n",
      "  AutoML has been revitalized with the inclusion of LLMs, as they bring automation to tasks such as \n",
      "feature selection, model training, and hyperparameter tuning.\n",
      "  Several frameworks \n",
      "highlight explainability as a paramount feature\n",
      "  The \n",
      "diversity in the data types handled is another facet, with a concentrated focus on tabular data-\n",
      "sets alongside deep learning frameworks catering to assorted data varieties.\n",
      "  A “black-box” scenario emerges quite frequently yielding difficulties in com-\n",
      "prehending the internal workings, which can impede problem debugging within AutoML models. \n",
      "\n",
      "  These modern solutions are equipped to adeptly deal with structured formats such as tables \n",
      "and time series.\n",
      "  Despite recent advancements, users are confronted with typical drawbacks associated with such \n",
      "automated systems.\n",
      "  – this is particularly pertinent where regulations \n",
      "or reliability are at stake in industries like healthcare and finance.\n",
      "----------------------------------------\n",
      "Page 236:\n",
      "Topics:\n",
      "  [2]:'Answer: 1024'\n",
      "Such capabilities, while adept at delivering straightforward numerical answers, are not as straight-\n",
      "forward to integrate into conventional EDA workflows.\n",
      "  Answer: 1024\n",
      "> Finished chain.\n",
      "\n",
      "  llm_math.run(\"What is 2 raised to the 10th power?\")\n",
      "We should see something like this:\n",
      ">\n",
      "  Using agents to answer data science questions\n",
      "Tools like LLMMathChain can be utilized to execute Python for answering computational queries. \n",
      "\n",
      "  As we’ve seen with \n",
      "Jupyter AI (Jupyternaut chat) – and in Chapter 6, Developing Software with Generative AI – there’s a \n",
      "lot of potential to increase efficiency by creating software with generative AI (code LLMs).\n",
      "  For instance, by chaining LLMs and tools, one can calculate mathematical powers and obtain \n",
      "results effortlessly:\n",
      "from langchain import OpenAI, LLMMathChain\n",
      "llm = OpenAI(temperature=0)\n",
      "llm_math = LLMMathChain.from_llm(llm, verbose=True)\n",
      "\n",
      "  What is 2 raised to the 10th power?\n",
      "2**10\n",
      "numexpr.evaluate(\"2**10\")\n",
      "\n",
      "  This is \n",
      "a good starting point for the practical part of this chapter as we investigate the use of generative \n",
      "AI in data science.\n",
      "  Chapter 7\n",
      "213\n",
      "With respect to ease of use, while AutoML with integrated LLMs offers simplified interfaces for \n",
      "model development pipelines, users must grapple with complex choices regarding model selec-\n",
      "tion and evaluation.\n",
      "\n",
      "  Other chains, like CPAL (CPALChain) \n",
      "and PAL (PALChain), can tackle more complex reasoning challenges, mitigating the risks of \n",
      "generative models producing implausible content; yet their practical applications remain elusive \n",
      "in real-world scenarios.\n",
      "\n",
      "Subtopics:\n",
      "  Let’s start to use agents to run code or call other tools to answer questions!\n",
      "\n",
      "  Entering new LLMMathChain chain...\n",
      "\n",
      "  We’ve already seen different agents with tools before.\n",
      "\n",
      "  As we’ll see in the next couple of sections, LLMs and tools can significantly accelerate data science \n",
      "workflows, reduce manual effort, and open up new analysis opportunities.\n",
      "----------------------------------------\n",
      "Page 237:\n",
      "Topics:\n",
      "  LLMs for Data Science\n",
      "214\n",
      "With PythonREPLTool, we can create simple visualizations of toy data or train with synthetic \n",
      "data, which can be nice for illustration or bootstrapping a project.\n",
      "  Train for 1000 epochs and print every 100 \n",
      "epochs.\n",
      "\n",
      "  Linear(1, 1)\n",
      ")\n",
      "loss_fn = torch.nn.\n",
      "  This is an example from the \n",
      "LangChain documentation:\n",
      "from langchain.agents.agent_toolkits import create_python_agent\n",
      "from langchain.tools.python.tool import PythonREPLTool\n",
      "from langchain.llms.openai import OpenAI\n",
      "from langchain.agents.agent_types import AgentType\n",
      "agent_executor = create_python_agent(\n",
      "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
      "    tool=PythonREPLTool(),\n",
      "    verbose=True,\n",
      "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
      ")\n",
      "agent_executor.run(\n",
      "    \"\"\"Understand, write a single neuron neural network in PyTorch.\n",
      "\n",
      "  This demonstrates constructing a single-neuron neural network using PyTorch, training it with \n",
      "synthetic data, and making predictions – all performed directly on the user’s machine.\n",
      "  We get this output back, which includes a prediction:\n",
      "Entering new AgentExecutor chain...\n",
      "I need to write a neural network in PyTorch and train it on the given data\n",
      "Action: Python_REPL\n",
      "Action Input:\n",
      "import torch\n",
      "model = torch.nn.\n",
      "  Return prediction for x = 5\"\"\"\n",
      ")\n",
      "\n",
      "  # Define the data\n",
      "x_data = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
      "\n",
      "Subtopics:\n",
      "  Sequential(\n",
      "    torch.nn.\n",
      "  MSELoss()\n",
      "optimizer = torch.optim.\n",
      "  SGD(model.parameters(), lr=0.01)\n",
      "\n",
      "  Take synthetic data for y=2x.\n",
      "  However, \n",
      "caution is advised as executing Python code without safeguards can pose security risks.\n",
      "\n",
      "----------------------------------------\n",
      "Page 238:\n",
      "Topics:\n",
      "  Here’s a simplistic example:\n",
      "from langchain.agents import load_tools, initialize_agent\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "  Observation: Epoch 100: 0.0043\n",
      "Epoch 200: 0.0023\n",
      "Epoch 300: 0.0013\n",
      "Epoch 400: 0.0007\n",
      "Epoch 500: 0.0004\n",
      "Epoch 600: 0.0002\n",
      "Epoch 700: 0.0001\n",
      "Epoch 800: 0.0001\n",
      "Epoch 900: 0.0000\n",
      "Epoch 1000: 0.0000\n",
      "Thought: I now know the final answer\n",
      "Final Answer: The prediction for x = 5 is y = 10.00.\n",
      "\n",
      "  if (epoch+1) % 100 == 0:\n",
      "        print(f'Epoch {epoch+1}: {loss.item():.4f}')\n",
      "    optimizer.zero_grad()\n",
      "    \n",
      "  Chapter 7\n",
      "215\n",
      "y_data = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
      "for epoch in range(1000):  # Train the model\n",
      "    y_pred = model(x_data)\n",
      "    loss = loss_fn(y_pred, y_data)\n",
      "    \n",
      "  For example, if our company offers flights from Tokyo, and we want to know the distances \n",
      "of our customers from Tokyo, we can use WolframAlpha as a tool.\n",
      "Subtopics:\n",
      "  Through iterative training displayed in verbose logs, users witness the progressive reduction of \n",
      "loss over epochs until a satisfactory prediction is attained.\n",
      "  LLMs and tools can be useful if we want to enrich our data with category or geographic informa-\n",
      "tion.\n",
      "  Despite this showcasing how a neural \n",
      "network learns and predicts over time, scaling this approach in practice would necessitate more \n",
      "sophisticated engineering efforts.\n",
      "\n",
      "  loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "  # Make a prediction\n",
      "x_pred = torch.tensor([[5.0]])\n",
      "y_pred = model(x_pred)\n",
      "\n",
      "----------------------------------------\n",
      "Page 239:\n",
      "Topics:\n",
      "  By combining LLMs with external tools like WolframAlpha, it’s possible to perform more challeng-\n",
      "ing data enrichment, such as calculating distances between cities, such as from Tokyo to New York \n",
      "City, Madrid, or Berlin.\n",
      "  The distance \n",
      "from Madrid, Spain to Tokyo is 8,845 miles.\n",
      "  Let’s ask and answer questions about structured datasets!\n",
      "\n",
      "  The distance from \n",
      "Madrid, Spain to Tokyo is 8,845 miles.\n",
      "  The distance from New York City to Tokyo is 6760 miles.\n",
      "  Entering new AgentExecutor chain...\n",
      "AI: The distance from New York City to Tokyo is 6760 miles.\n",
      "  The distance from Berlin, Germany \n",
      "to Tokyo is 6,845 miles.\n",
      "\n",
      "  LLMs for Data Science\n",
      "216\n",
      "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
      "llm = OpenAI(temperature=0)\n",
      "tools = load_tools(['wolfram-alpha'])\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "agent = initialize_agent(tools, llm, agent=\"conversational-react-\n",
      "description\", memory=memory, verbose=True)\n",
      "agent.run(\n",
      "    \"\"\"How far are these cities to Tokyo?\n",
      "* New York City\n",
      "* Madrid, Spain\n",
      "* Berlin\n",
      "\"\"\")\n",
      "Please make sure you’ve set the OPENAI_API_KEY and WOLFRAM_ALPHA_APPID environment variables \n",
      "as discussed in Chapter 3, Getting Started with LangChain.\n",
      "  The distance from Berlin, \n",
      "Germany to Tokyo is 6,845 miles.\n",
      "> Finished chain.\n",
      "\n",
      "Subtopics:\n",
      "  Nonetheless, these examples address relatively straightforward \n",
      "queries; deploying such implementations on a larger scale demands more extensive engineering \n",
      "strategies beyond those discussed.\n",
      "\n",
      "  Such integrations could significantly enhance the utility of datasets used \n",
      "in various business applications.\n",
      "  Here’s the output:\n",
      ">\n",
      "  '\n",
      "\n",
      "  However, we can give agents datasets to work with, and here is where it can get immensely pow-\n",
      "erful when we connect more tools.\n",
      "----------------------------------------\n",
      "Page 240:\n",
      "Topics:\n",
      "  from langchain.agents import create_pandas_dataframe_agent\n",
      "from langchain import PromptTemplate\n",
      "from langchain.llms.openai import OpenAI\n",
      "PROMPT = (\n",
      "    \"If you do not know the answer, say you don't know.\\n\"\n",
      "    \"Think step by step.\\n\"\n",
      "    \"\\n\"\n",
      "    \"Below is the query.\\n\"\n",
      "    \"Query: {query}\\n\"\n",
      ")\n",
      "prompt = PromptTemplate(template=PROMPT, input_variables=[\"query\"])\n",
      "\n",
      "  We \n",
      "can create a pandas DataFrame agent now and we’ll see how easy it is to get simple stuff done!\n",
      "\n",
      "  As we mentioned earlier, generative AI models such as ChatGPT have the ability to understand and \n",
      "generate human-like responses, making them valuable tools for enhancing research productivity. \n",
      "\n",
      "  Chapter 7\n",
      "217\n",
      "Data exploration with LLMs\n",
      "Data exploration is a crucial and foundational step in data analysis, allowing researchers to gain a \n",
      "comprehensive understanding of their datasets and uncover significant insights.\n",
      "  Let’s load up a dataset and work with that.\n",
      "  We can quickly get a dataset from scikit-learn:\n",
      "from sklearn.datasets import load_iris\n",
      "df = load_iris(as_frame=True)[\"data\"]\n",
      "The Iris dataset is well known – it’s a toy dataset, but it will help us illustrate the capabilities of \n",
      "using generative AI for data exploration.\n",
      "  llm = OpenAI()\n",
      "agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
      "\n",
      "  We’ll use a DataFrame in the following example.\n",
      "Subtopics:\n",
      "  Researchers can leverage ChatGPT’s capabilities to ask questions about statistical \n",
      "trends in numerical datasets or even query visualizations for image classification tasks.\n",
      "\n",
      "  With the emer-\n",
      "gence of LLMs like ChatGPT, researchers can harness the power of natural language processing \n",
      "to facilitate data exploration.\n",
      "\n",
      "  LLMs can help explore textual data and other forms of data, such as numerical datasets or multi-\n",
      "media content.\n",
      "  Asking our questions in natural language and getting responses in digestible pieces and shapes \n",
      "can be a great boost to analysis.\n",
      "\n",
      "----------------------------------------\n",
      "Page 241:\n",
      "Topics:\n",
      "  LLMs for Data Science\n",
      "218\n",
      "I’ve included instructions for the model to indicate uncertainty and to follow a step-by-step \n",
      "thought process, with the aim of reducing hallucinations.\n",
      "  Now we can query our agent against \n",
      "the DataFrame:\n",
      "agent.run(prompt.format(query=\"What's this dataset about?\"))\n",
      "\n",
      "  Iris dataset barplots\n",
      "The plot is not perfect.\n",
      "  Here is the plot:\n",
      "Figure 7.3:\n",
      "Subtopics:\n",
      "  In this case, I used df.plot.bar(rot=0, subplots=True).\n",
      "  agent.run(prompt.format(query=\"Plot each column as a barplot!\"))\n",
      "\n",
      "  We might want \n",
      "to introduce more tweaks, for example, to padding between the panels, the font size, or the \n",
      "placement of the legend, to make this really nice.\n",
      "\n",
      "  The output can be finicky and depends on the llm model parameter and \n",
      "on the instructions.\n",
      "  Let’s show how to get a visualization:\n",
      "\n",
      "  We get the answer This dataset is about the measurements of some type of flower, \n",
      "which is correct.\n",
      "\n",
      "----------------------------------------\n",
      "Page 242:\n",
      "Topics:\n",
      "  We get the answer with the intermediate steps as follows \n",
      "(shortened):\n",
      "df['difference'] = df['petal length (cm)'] - df['petal width (cm)']\n",
      "df.loc[df['difference'].idxmax()]\n",
      "Observation: sepal length (cm)    7.7\n",
      "sepal width (cm)     2.8\n",
      "petal length (cm)    6.7\n",
      "petal width (cm)     2.0\n",
      "difference           4.7\n",
      "Name: 122, dtype: float64\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Row 122 has the biggest difference between petal length and \n",
      "petal width.\n",
      "\n",
      "  Chapter 7\n",
      "219\n",
      "We can also ask to see the distributions of the columns visually, which will give us this neat plot:\n",
      "Figure 7.4: Iris dataset boxplots\n",
      "\n",
      "  We can request the plot to use other plotting backends, such as Seaborn; however, please note \n",
      "that these have to be installed.\n",
      "\n",
      "Subtopics:\n",
      "  We can also ask more questions about the dataset, like which row has the biggest difference \n",
      "between petal length and petal width.\n",
      "  I think that’s worth a pat on the back, LLM!\n",
      "\n",
      "----------------------------------------\n",
      "Page 243:\n",
      "Topics:\n",
      "  There’s also the PandasAI library, which uses LangChain under the hood and provides similar \n",
      "functionality.\n",
      "  LLMs for Data Science\n",
      "220\n",
      "We could extend this example by adding more instructions to the prompt about plotting, such \n",
      "as the sizes of plots.\n",
      "\n",
      "  DataFrame({\n",
      "\n",
      "  Thought: I now know the final answer\n",
      "Final Answer: The p-value of 6.639808432803654e-32 indicates that the two \n",
      "variables come from different distributions.\n",
      "\n",
      "  We can ask complex questions about the dataset with simple \n",
      "prompts in plain English.\n",
      "\n",
      "  It’s a bit harder to implement the same plotting logic in a Streamlit app, because we need to use \n",
      "the plotting functionality in corresponding Streamlit functions, for example, st.bar_chart(). \n",
      "\n",
      "  Here’s an example from the documentation with an example dataset:\n",
      "import pandas as pd\n",
      "from pandasai.llm import OpenAI\n",
      "from pandasai.schemas.df_config import Config\n",
      "from pandasai import SmartDataframe\n",
      "df = pd.\n",
      "  You can find explanations for this on the Streamlit blog (Building \n",
      "a Streamlit and scikit-learn app with ChatGPT).\n",
      "\n",
      "Subtopics:\n",
      "  However, this can be done as well.\n",
      "  What about statistical tests?\n",
      "agent.run(prompt.format(query=\"Validate the following hypothesis \n",
      "statistically: petal width and petal length come from the same \n",
      "distribution.\"))\n",
      "\n",
      "  That checks off the statistical test!\n",
      "  Action: python_repl_ast\n",
      "Action Input: from scipy.stats import ks_2samp\n",
      "Observation:\n",
      "Thought: I now have the necessary tools to answer this question.\n",
      "\n",
      "  We get this response:\n",
      "Thought: I should use a statistical test to answer this question.\n",
      "\n",
      "  Action: python_repl_ast\n",
      "Action Input: ks_2samp(df['petal width (cm)'], df['petal length (cm)'])\n",
      "Observation: KstestResult(statistic=0.6666666666666666, \n",
      "pvalue=6.639808432803654e-32, statistic_location=2.5, statistic_sign=1)\n",
      "\n",
      "----------------------------------------\n",
      "Page 244:\n",
      "Topics:\n",
      "  Chapter 7\n",
      "221\n",
      "    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \n",
      "\"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n",
      "    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, \n",
      "1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, \n",
      "14631844184064],\n",
      "    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, \n",
      "5.87, 5.12]\n",
      "})\n",
      "smart_df = SmartDataframe(df, config=Config(llm=OpenAI()))\n",
      "print(smart_df.chat(\"Which are the 5 happiest countries?\"))\n",
      "\n",
      "  An LLM will create the queries for us.\n",
      "  For data in SQL databases, we can connect with a SQLDatabaseChain.\n",
      "  The creation of a pandas DataFrame agent enabled simple analysis tasks and \n",
      "visualization requests, demonstrating the AI’s capacity to produce plots and specific data insights.\n",
      "\n",
      "  We are connecting to a database first.\n",
      "  Please note that PandasAI is not part of the setup for the book, so you’ll have to install it sepa-\n",
      "rately if you want to use it.\n",
      "\n",
      "  The LangChain documen-\n",
      "tation shows this example:\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.utilities import SQLDatabase\n",
      "from langchain_experimental.sql import SQLDatabaseChain\n",
      "db = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\n",
      "llm = OpenAI(temperature=0, verbose=True)\n",
      "\n",
      "  Through loading a dataset, such as the Iris dataset \n",
      "from scikit-learn, we can use an LLM-powered agent to query about data specifics in accessible, \n",
      "everyday language.\n",
      "  This will give us the requested result similar to before when we were using LangChain directly. \n",
      "\n",
      "Subtopics:\n",
      "  I would expect this to be \n",
      "particularly useful when we don’t know about the schema of the database.\n",
      "  This can also be quite powerful.\n",
      "  The SQLDatabaseChain \n",
      "can also check queries and autocorrect them if the use_query_checker option is set.\n",
      "\n",
      "  By following the outlined steps, we have leveraged the impressive natural language processing \n",
      "capabilities of LLMs for data exploration.\n",
      "  db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
      "db_chain.run(\"How many employees are there?\")\n",
      "\n",
      "  Then we can ask questions about the data in natural language. \n",
      "\n",
      "----------------------------------------\n",
      "Page 245:\n",
      "Topics:\n",
      "  LLMs for Data Science\n",
      "222\n",
      "We can not only inquire about the nature of the dataset verbally but also command the agent to \n",
      "generate visual representations such as barplots and boxplots for EDA.\n",
      "  When delving into more nuanced requests, such as identifying disparities between \n",
      "two data attributes, the agent adeptly added new columns and located pertinent numerical \n",
      "differences, showing its practical utility in drawing actionable conclusions.\n",
      "\n",
      "  Efforts extended beyond mere visualization as the application of statistical tests was also explored \n",
      "through concise English prompts, resulting in articulate interpretations of statistical operations \n",
      "like KS-tests performed by the agent.\n",
      "\n",
      "  Our exploration then shifted toward the use of LLMs for data exploration, building upon the \n",
      "techniques for ingesting and analyzing voluminous textual data detailed in Chapter 4, Building \n",
      "Capable Assistants, on question answering.\n",
      "  However, despite \n",
      "the remarkable strides in enabling and enhancing the work of data scientists through these AI \n",
      "tools, the current state of AI technology isn’t at a point where it can supplant human experts but \n",
      "rather augments their capabilities and broadens their analytical toolset.\n",
      "\n",
      "  Here, our focus turned to structured datasets, examining \n",
      "how SQL databases or tabular information could be effectively analyzed through LLM-powered \n",
      "exploratory processes.\n",
      "\n",
      "  To sum up our exploration, it is clear that AI technologies, illustrated by platforms such as ChatGPT \n",
      "plugins and Microsoft Fabric, hold transformative potential for data analysis.\n",
      "  Diving into code generation, we saw parallels with software development, as discussed in Chapter \n",
      "6, Developing Software with Generative AI, observing how tools and functions generated by LLMs \n",
      "can respond to queries or enhance datasets through augmentation techniques.\n",
      "  The capabilities of integrations aren’t limited to static datasets but extend to dynamic SQL data-\n",
      "bases where an LLM can automate query generation, even offering autocorrection for syntactical \n",
      "errors in SQL statements.\n",
      "  Summary\n",
      "Beginning with an examination of AutoML frameworks, this chapter highlighted the value these \n",
      "systems bring to the entirety of the data science pipeline, facilitating each stage from data prepa-\n",
      "ration to model deployment.\n",
      "  This included \n",
      "leveraging third-party tools like WolframAlpha to add external data points to existing datasets. \n",
      "\n",
      "Subtopics:\n",
      "  This capability particularly shines when schemas are unfamiliar.\n",
      "\n",
      "  Although these visualiza-\n",
      "tions might require additional fine-tuning for aesthetic refinement, they established a groundwork \n",
      "for analysis.\n",
      "  We then considered how the integration of LLMs can further elevate \n",
      "productivity and make data science more approachable for both technical and non-technical \n",
      "stakeholders.\n",
      "\n",
      "----------------------------------------\n",
      "Page 246:\n",
      "Topics:\n",
      "  How can we get an LLM to work with data?\n",
      "\n",
      "  How can generative AI help data scientists?\n",
      "4.\t\n",
      "\n",
      "  Why would we want to automate data science/analysis?\n",
      "3.\t\n",
      "\n",
      "  I recommend you go back to the corresponding sections of this chapter if you are unsure about \n",
      "any of them:\n",
      "1.\t\n",
      "\n",
      "  Chapter 7\n",
      "223\n",
      "In the next chapter, we’ll focus on conditioning techniques to improve the performance of LLMs \n",
      "through prompting and fine-tuning.\n",
      "\n",
      "  Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "  What kind of agents and tools can we use to answer simple questions?\n",
      "5.\t\n",
      "\n",
      "  What steps are involved in data science?\n",
      "2.\t \n",
      "Subtopics:\n",
      "  Questions\n",
      "Please have a look to see if you can come up with the answers to these questions from memory. \n",
      "\n",
      "----------------------------------------\n",
      "Page 247:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 248:\n",
      "Topics:\n",
      "  You can find the \n",
      "corresponding code in the GitHub repository for the book at https://github.com/benman1/\n",
      "generative_ai_with_langchain\n",
      "\n",
      "  8\n",
      "Customizing LLMs and Their \n",
      "Output\n",
      "This chapter is about techniques and best practices to improve the reliability and performance \n",
      "of LLMs in certain scenarios, such as complex reasoning and problem-solving tasks.\n",
      "Subtopics:\n",
      "  Prompt engineering is significant in unlocking \n",
      "LLM reasoning capabilities, and prompt techniques form a valuable toolkit for researchers and \n",
      "practitioners working with LLMs.\n",
      "  We’ll discuss and implement advanced prompt engineering \n",
      "strategies like few-shot learning, tree-of-thought, and self-consistency.\n",
      "\n",
      "  This pro-\n",
      "cess of adapting a model for a certain task or making sure that our model output corresponds to \n",
      "what we expect is called conditioning.\n",
      "  On the other hand, by providing additional input or context at inference time, LLMs can gen-\n",
      "erate text tailored to a particular task or style.\n",
      "  Fine-tuning involves training the pre-trained base model on specific tasks or datasets relevant \n",
      "to the desired application.\n",
      "  In this chapter, we’ll discuss fine-tuning and prompting \n",
      "as methods for conditioning.\n",
      "\n",
      "  Throughout the chapter, we’ll work on fine-tuning and prompting with LLMs.\n",
      "  This process allows the model to adapt, becoming more accurate and \n",
      "contextually relevant for the intended use case.\n",
      "\n",
      "----------------------------------------\n",
      "Page 249:\n",
      "Topics:\n",
      "  The two terms are not synonymous; while conditioning can include fine-tuning and \n",
      "is focused on influencing the model through various techniques at different layers of \n",
      "interaction, alignment is concerned with the fundamental and holistic calibration \n",
      "of the model’s behavior to human ethics and safety standards.\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "226\n",
      "The main sections in this chapter are:\n",
      "•\t\n",
      "Conditioning LLMs\n",
      "•\t\n",
      "Fine-tuning\n",
      "•\t\n",
      "Prompt engineering\n",
      "Let’s start by discussing conditioning, why it’s important, and how we can achieve it.\n",
      "\n",
      "  Conditioning techniques enable LLMs to comprehend and execute complex instructions, deliv-\n",
      "ering content that closely matches our expectations.\n",
      "  In this chapter, we’ll focus on fine-tuning \n",
      "and prompt techniques as two methods of conditioning.\n",
      "\n",
      "  While base models such as GPT-4 can generate \n",
      "impressive text on a wide range of topics, conditioning them can enhance their capabilities in \n",
      "terms of task relevance, specificity, and coherence, and can guide the model’s behavior to be in \n",
      "line with what is considered ethical and appropriate.\n",
      "Subtopics:\n",
      "  Furthermore, part of conditioning \n",
      "includes implementing safeguards to avert the production of malicious or harmful content, such \n",
      "as incorporating filters or training the model to avoid certain types of problematic outputs, thereby \n",
      "better aligning it with desired ethical standards.\n",
      "\n",
      "  This ranges from off-the-cuff interactions \n",
      "to systematic training that orients a model’s behavior toward reliable performance in specialist \n",
      "domains, like legal consultation or technical documentation.\n",
      "  This includes not only prompt crafting but also more systemic techniques, \n",
      "such as fine-tuning the model on specific datasets to adapt its responses to certain \n",
      "topics or styles persistently.\n",
      "\n",
      "  Conditioning LLMs\n",
      "Pre-training an LLM on diverse data to learn patterns of language results in a base model that \n",
      "has a broad understanding of diverse topics.\n",
      "  Alignment refers to the process and goal of training and modifying LLMs so that \n",
      "their general behavior, decision-making processes, and outputs conform to broader \n",
      "human values, ethical principles, and safety considerations.\n",
      "\n",
      "  Conditioning refers to a collection of methods used to direct the model’s generation \n",
      "of outputs.\n",
      "----------------------------------------\n",
      "Page 250:\n",
      "Topics:\n",
      "  Chapter 8\n",
      "227\n",
      "Conditioning can be applied at different points in a model’s life cycle.\n",
      "  One strategy involves \n",
      "fine-tuning the model on data that represents the intended use case to help the model specialize \n",
      "in that area.\n",
      "  Here is a table summarizing the dif-\n",
      "ferent techniques:\n",
      "Stage\n",
      "Technique\n",
      "Examples\n",
      "Training\n",
      "Data curation\n",
      "Training on diverse data\n",
      "Objective function\n",
      "Careful design of training objective\n",
      "Architecture and training \n",
      "process\n",
      "Optimizing model structure and training\n",
      "Fine-tuning\n",
      "Task specialization\n",
      "Training on specific datasets/tasks\n",
      "Inference-time \n",
      "conditioning\n",
      "Dynamic inputs\n",
      "Prefixes, control codes, and context \n",
      "examples\n",
      "Human oversight\n",
      "Human-in-the-loop\n",
      "Human review and feedback\n",
      "Table 8.1:\n",
      "  The ultimate goal is to ensure that human values are incorporated at all \n",
      "stages, from training to deployment, to create responsible and aligned AI systems.\n",
      "\n",
      "  In the next section, I will summarize key methods for conditioning such as fine-tuning and prompt \n",
      "engineering, discuss the rationale, and examine their relative pros and cons.\n",
      "Methods for conditioning\n",
      "With the advent of large pre-trained language models like GPT-3, there has been growing interest \n",
      "in techniques to adapt these models for downstream tasks.\n",
      "  Steering generative AI outputs\n",
      "Combining these techniques provides developers with more control over the behavior and outputs \n",
      "of generative AI systems.\n",
      "Subtopics:\n",
      "  Several approaches have been proposed for conditioning.\n",
      "  Another method involves conditioning the model dynamically at the \n",
      "time of inference, where the input prompt is tailored with additional context to shape the desired \n",
      "output.\n",
      "  This approach offers flexibility but can add complexity to the model’s operation in live \n",
      "environments.\n",
      "\n",
      "  As LLMs continue to develop, they \n",
      "will become even more effective and useful for a broader range of applications, and we can expect \n",
      "future advancements in fine-tuning and prompting techniques to help go even further in complex \n",
      "tasks that involve reasoning and tool use.\n",
      "\n",
      "  This method depends on the availability of such data and the ability to integrate it \n",
      "into the training process.\n",
      "----------------------------------------\n",
      "Page 251:\n",
      "Topics:\n",
      "  Customizing LLMs and Their Output\n",
      "228\n",
      "In this chapter, we emphasize fine-tuning and prompting, as they stand out for their effective-\n",
      "ness and prevalence in the conditioning of LLMs.\n",
      "  By \n",
      "integrating RLHF, we harness the nuanced understanding of human evaluators to further refine \n",
      "the model behavior, ensuring outputs that are not only relevant and accurate but also align with \n",
      "user intent and expectations.\n",
      "\n",
      "  To address these limitations, we explore strategies like adapters \n",
      "and Low-Rank Adaptation (LoRA), which introduce elements of sparsity or implement partial \n",
      "freezing of parameters to lighten the burden.\n",
      "\n",
      "  RLHF has exhibited the potential to profoundly improve the capa-\n",
      "bilities of language models like GPT-3, making fine-tuning an even more impactful technique.\n",
      "  RLHF is an online approach that fine-tunes LMs using human preferences.\n",
      "  It has three main steps:\n",
      "1.\t\n",
      "Supervised pre-training: The LM is first trained via standard supervised learning on \n",
      "human demonstrations.\n",
      "\n",
      "  2.\t\n",
      "Reward model training: A reward model is trained on human ratings of LM outputs to \n",
      "estimate a reward.\n",
      "\n",
      "  Let’s start off \n",
      "by discussing the reasons why InstructGPT, which was trained through RLHF, has had such a \n",
      "transformative impact.\n",
      "\n",
      "  Moreover, we delve into the transformative role of Reinforcement Learning with Human Feed-\n",
      "back (RLHF) within fine-tuning processes, where human feedback serves as a critical guide for \n",
      "the model’s learning trajectory.\n",
      "  Reinforcement learning with human feedback\n",
      "In their March 2022 paper, Ouyang and others from OpenAI demonstrated using RLHF with \n",
      "Proximal Policy Optimization (PPO) to align LLMs, like GPT-3, with human preferences.\n",
      "\n",
      "Subtopics:\n",
      "  Through careful crafting of input prompts and subsequent optimization and evaluations, \n",
      "these methods can steer the behavior of LLMs in desired directions without the need for heavy \n",
      "retraining.\n",
      "  All these different techniques for conditioning facilitate the development of LLMs that are both \n",
      "high-performing and aligned with desired outcomes across various applications.\n",
      "  Prompts can be carefully designed to elicit specific behaviors or to encapsulate partic-\n",
      "ular knowledge areas, providing a versatile and resource-savvy approach to model conditioning.\n",
      "\n",
      "  However, fine-tuning can be resource-intensive, presenting a trade-off between high performance \n",
      "and computational efficiency.\n",
      "  Prompt-based techniques, on the other hand, offer a way to dynamically condition LLMs at infer-\n",
      "ence time.\n",
      "  This method is aimed \n",
      "at enhancing model performance for particular objectives and is known to yield robust results. \n",
      "\n",
      "  Fine-tuning involves adjusting all parameters \n",
      "of a pre-trained model through additional training on specialized tasks.\n",
      "----------------------------------------\n",
      "Page 252:\n",
      "Topics:\n",
      "  The QLORA method is an extension of LoRA, which enables efficient fine-tuning of large models \n",
      "by backpropagating gradients through a frozen 4-bit quantized model into learnable low-rank \n",
      "adapters.\n",
      "  Chapter 8\n",
      "229\n",
      "3.\t\n",
      "RL fine-tuning: The LM is fine-tuned via reinforcement learning to maximize the expected \n",
      "reward from the reward model using an algorithm like PPO.\n",
      "\n",
      "  The main change, RLHF, allows incorporating nuanced human judgments into language model \n",
      "training through a learned reward model.\n",
      "  Starting in March 2022, OpenAI started releasing the GPT-3.5 series models, upgraded versions \n",
      "of GPT-3, which include fine-tuning with RLHF.\n",
      "\n",
      "  RL \n",
      "training can be unstable and computationally expensive; notwithstanding, its success inspired \n",
      "further research into refining RLHF techniques, reducing data requirements for alignment, and \n",
      "developing more powerful and accessible models for a wide range of applications.\n",
      "\n",
      "  Parameter-Efficient Fine-Tuning (PEFT) methods enable the use of small checkpoints for each \n",
      "task, making the models more portable.\n",
      "  InstructGPT opened up new avenues to improve language models by incorporating reinforce-\n",
      "ment learning from human feedback methods beyond traditional fine-tuning approaches.\n",
      "  It introduces trainable rank decomposition matrices into each layer of the Transformer archi-\n",
      "tecture to reduce the number of trainable parameters.\n",
      "  This small set of trained weights can be added on top of \n",
      "the LLM, allowing the same model to be used for multiple tasks without replacing the entire model.\n",
      "\n",
      "  QLORA reduces the memory requirements to fine-tune a 65B parameter model from \n",
      ">780 GB to <48 GB, without affecting runtime or predictive performance.\n",
      "\n",
      "  QLORA models \n",
      "achieve 99% of ChatGPT performance on Vicuna, using innovations like new data types and \n",
      "optimizers.\n",
      "  This allows you to fine-tune a 65B parameter model on a single GPU.\n",
      "  This new model can be \n",
      "used to follow instructions that are given in natural language, and it can answer questions in \n",
      "a way that’s more accurate and relevant than GPT-3.\n",
      "  InstructGPT outperformed GPT-3 on user \n",
      "preference, truthfulness, and harm reduction, despite having 100x fewer parameters.\n",
      "\n",
      "Subtopics:\n",
      "  As a result, human feedback can steer and improve \n",
      "language model capabilities beyond standard supervised fine-tuning.\n",
      "  Low-Rank Adaptation (LoRA) is a type of PEFT, where the pre-trained model weights are frozen. \n",
      "\n",
      "  Low-rank adaptation\n",
      "As LLMs become larger, it becomes difficult to train them on consumer hardware, and deploying \n",
      "them for each specific task becomes expensive.\n",
      "  LoRA achieves comparable model quality \n",
      "compared to fine-tuning while having fewer trainable parameters and higher training throughput.\n",
      "\n",
      "  There are a few methods that reduce computa-\n",
      "tional, memory, and storage costs while improving performance in low-data and out-of-domain \n",
      "scenarios.\n",
      "\n",
      "----------------------------------------\n",
      "Page 253:\n",
      "Topics:\n",
      "  Customizing LLMs and Their Output\n",
      "230\n",
      "In the next section, we’ll discuss methods to condition LLMs at inference time, which include \n",
      "prompt engineering.\n",
      "\n",
      "  •\t\n",
      "LLMs like BERT and GPT-3 have been shown to work well with 4–8-bit quan-\n",
      "tization via fine-tuning.\n",
      "\n",
      "  For example, weights could \n",
      "be quantized to 8-bit integers.\n",
      "\n",
      "  •\t\n",
      "This allows you to shrink a model size by up to 4x and improve throughput \n",
      "on specialized hardware.\n",
      "\n",
      "  •\t\n",
      "Quantization typically has a minor impact on model accuracy, especially \n",
      "with re-training.\n",
      "•\t\n",
      "Common quantization methods include scalar, vector, and product quan-\n",
      "tization, which quantize weights separately or in groups.\n",
      "•\t\n",
      "Activations can also be quantized by estimating their distribution and bin-\n",
      "ning appropriately.\n",
      "\n",
      "  LLM fine-tuning may not always be feasible or beneficial in certain scenarios:\n",
      "•\t\n",
      "Limited fine-tuning services: When models are only accessible through APIs that lack \n",
      "or have restricted fine-tuning capabilities\n",
      "•\t\n",
      "Insufficient data: In cases where there is a lack of data for fine-tuning, either for the \n",
      "specific downstream task or relevant application domain\n",
      "•\t\n",
      "Dynamic data: In cases of applications with frequently changing data, such as news-re-\n",
      "lated platforms, fine-tuning models frequently becomes challenging, leading to potential \n",
      "drawbacks\n",
      "Quantization refers to techniques to reduce the numerical precision of weights \n",
      "and activations in neural networks like LLMs.\n",
      "  Inference-time conditioning\n",
      "One commonly used approach is conditioning at inference time (an output generation phase), \n",
      "where specific inputs or conditions are provided dynamically to guide the output generation \n",
      "process.\n",
      "Subtopics:\n",
      "  •\t\n",
      "Quantization-aware training adjusts weights during training to minimize \n",
      "quantization loss.\n",
      "\n",
      "  The main purpose of quantization is \n",
      "to reduce the memory footprint and computational requirements of large models.\n",
      "\n",
      "  Some key points about the quantization of LLMs:\n",
      "•\t\n",
      "It involves representing weights and activations using fewer bits than a \n",
      "standard single-precision floating point (FP32).\n",
      "----------------------------------------\n",
      "Page 254:\n",
      "Topics:\n",
      "  •\t\n",
      "Constraining tokens: Forcing inclusion/exclusion of certain words\n",
      "•\t\n",
      "Metadata: Providing high-level info like genre, target audience, and so on\n",
      "Prompts can facilitate generating text that adheres to specific themes, styles, or even mimics a \n",
      "particular author’s writing style.\n",
      "  It has been shown that prompting provides easy control over large frozen models like GPT-4 and \n",
      "allows steering model behavior without extensive fine-tuning. \n",
      "\n",
      "  Powerful LLMs, such as GPT-3, can solve tasks with-\n",
      "out further training through prompting techniques.\n",
      "  Chapter 8\n",
      "231\n",
      "•\t\n",
      "Context-sensitive applications: Dynamic and context-specific applications like person-\n",
      "alized chatbots cannot perform fine-tuning based on individual user data\n",
      "For conditioning at inference time, most commonly, we provide a textual prompt or instruction \n",
      "at the beginning of the text generation process.\n",
      "  By conditioning LLM outputs on contextual information at \n",
      "runtime, these methods can guide models without relying on traditional fine-tuning processes.\n",
      "\n",
      "  Further examples include prepending relevant documents to prompts to assist LLMs with writing \n",
      "tasks (for example, news reports, Wikipedia pages, and company documents), or retrieving and \n",
      "prepending user-specific data (financial records, health data, and emails) before prompting an \n",
      "LLM to ensure personalized answers.\n",
      "  Zero-shot \n",
      "prompting involves no solved examples, while few-shot prompting includes a small number of \n",
      "examples of similar (problem and solution) pairs.\n",
      "\n",
      "Subtopics:\n",
      "  These techniques involve providing contextual information \n",
      "during inference time, such as for in-context learning or retrieval augmentation.\n",
      "\n",
      "  An example of prompt tuning is prefixing prompts, where instructions like “Write a child-friendly \n",
      "story about...” are prepended to the prompt.\n",
      "  Often, demonstrations are part of the instructions for reasoning tasks, where few-shot examples \n",
      "are provided to induce the desired behavior.\n",
      "  In this approach, the problem to be solved \n",
      "is presented to the model as a text prompt, with some text examples of similar problems and \n",
      "their solutions.\n",
      "  •\t\n",
      "Prefix tuning: Prepending trainable vectors to LLM layers.\n",
      "\n",
      "  Some common techniques for inference-time conditioning include:\n",
      "•\t\n",
      "Prompt tuning: Providing natural language guidance for intended behavior.\n",
      "  Sensitive \n",
      "to prompt design.\n",
      "\n",
      "  For example, in chatbot applications, conditioning \n",
      "the model with user messages helps it generate responses that are personalized and pertinent \n",
      "to the ongoing conversation.\n",
      "\n",
      "  This prompt can be a few sentences or even a \n",
      "single word, acting as an explicit indication of the desired output.\n",
      "\n",
      "  The model must provide a completion of the prompt via inference.\n",
      "----------------------------------------\n",
      "Page 255:\n",
      "Topics:\n",
      "  Customizing LLMs and Their Output\n",
      "232\n",
      "Prompting enables conditioning models on new knowledge with low overhead, but careful prompt \n",
      "engineering is needed for the best results.\n",
      "  Howard and Ruder (2018) demonstrated the effectiveness of fine-tuning models \n",
      "like ELMo and ULMFit on downstream tasks.\n",
      "  The seminal BERT model (Devlin and others., 2019) \n",
      "established fine-tuning of pre-trained transformers as the de facto approach in NLP.\n",
      "\n",
      "  In the next section, we’ll fine-tune a small open-source LLM (OpenLLaMa) for Question Answer-\n",
      "ing (QA) with PEFT and quantization, and we’ll deploy it on Hugging Face.\n",
      "\n",
      "  Similar ideas have been proposed for adapter approaches, such as parameter Efficient \n",
      "Transfer Learning (PELT) or Ladder Side-Tuning (LST).\n",
      "\n",
      "  The idea of fine-tuning pre-trained neural networks originated in computer vision research in \n",
      "the early 2010s.\n",
      "  In general, there are three advantages of fine-tuning that are immediately obvious to users of \n",
      "these models:\n",
      "•\t\n",
      "Steerability: The capability of models to follow instructions (instruction-tuning)\n",
      "•\t\n",
      "Reliable output-formatting: This is important, for example, for API calls/function calling)\n",
      "•\t\n",
      "Custom tone: This makes it possible to adapt the output style as appropriate to the task \n",
      "and audience.\n",
      "\n",
      "  Fine-tuning\n",
      "As we discussed in the first section of this chapter, the goal of model fine-tuning for LLMs is to \n",
      "optimize a model to generate outputs that are more specific to a task and context than the orig-\n",
      "inal foundation model.\n",
      "\n",
      "Subtopics:\n",
      "  In prefix tuning, continuous task-specific vectors are trained and supplied to models at inference \n",
      "time.\n",
      "  Their capabilities manifest only when adapted to ap-\n",
      "plications.\n",
      "  This is what we’ll discuss as part of this chapter.\n",
      "\n",
      "  This enables knowledge transfer from the general model while customizing it for specialized tasks.\n",
      "\n",
      "  •\t\n",
      "Alignment: The output of models should correspond to core values, for example, con-\n",
      "cerning safety, security, and privacy considerations.\n",
      "\n",
      "  Conditioning at inference time can also happen during sampling, such as grammar-based sam-\n",
      "pling, where the output can be constrained to be compatible with certain well-defined patterns, \n",
      "such as a programming language syntax.\n",
      "\n",
      "  The need for fine-tuning arises because pre-trained LMs are designed to model general linguistic \n",
      "knowledge, not specific downstream tasks.\n",
      "  Fine-tuning allows pre-trained weights to be updated for target datasets and objectives. \n",
      "\n",
      "----------------------------------------\n",
      "Page 256:\n",
      "Topics:\n",
      "  As a first step, we’ll set up fine-tuning with libraries and environment variables.\n",
      "\n",
      "  We’ll install all required libraries in the Google Colab environment – I am adding \n",
      "the versions of these libraries that I’ve used to make our fine-tuning repeatable:\n",
      "•\t\n",
      "peft: PEFT (version 0.5.0)\n",
      "•\t\n",
      "trl: Proximal Policy Optimization (0.6.0)\n",
      "•\t\n",
      "bitsandbytes: k-bit optimizers and matrix multiplication routines, needed for quanti-\n",
      "zation (0.41.1)\n",
      "•\t\n",
      "accelerate: train and use PyTorch models with multi-GPU, TPU, and mixed-precision \n",
      "(0.22.0)\n",
      "•\t\n",
      "transformers: Hugging Face transformers library with backends in JAX, PyTorch, and \n",
      "TensorFlow (4.32.0)\n",
      "•\t\n",
      "datasets: community-driven open-source library of datasets (2.14.4)\n",
      "•\t\n",
      "sentencepiece: Python wrapper for fast tokenization (0.1.99)\n",
      "•\t\n",
      "wandb: for monitoring the training progress on Weights and Biases (0.15.8)\n",
      "•\t\n",
      "langchain for loading the model back as a LangChain LLM after training (0.0.273)\n",
      "Google Colab is a computation environment that provides different means for hard-\n",
      "ware acceleration of computation tasks such as Tensor Processing Units (TPUs) and \n",
      "Graphical Processing Units (GPUs).\n",
      "  You can sign into a Colab \n",
      "environment at this URL: https://colab.research.google.com/\n",
      "\n",
      "  Chapter 8\n",
      "233\n",
      "In this section, we’ll fine-tune a model for question answering.\n",
      "  You can \n",
      "find the code in the notebooks directory in the GitHub repository for the book.\n",
      "\n",
      "  Please make sure you set your Google Colab machine settings in the top menu to TPU or GPU to \n",
      "make sure you have sufficient resources to run the following code and that the training doesn’t \n",
      "take too long.\n",
      "  This recipe is not specific to Lang-\n",
      "Chain, but we’ll point out a few customizations, where LangChain could be applicable.\n",
      "  We’ll run this on Google Colab instead of the local \n",
      "environment, where we can run fine-tuning of LLMs free of charge (with only a few restrictions).\n",
      "\n",
      "Subtopics:\n",
      "  For the task in this section, the free tier is sufficient.\n",
      "  Therefore, it’s a good idea to do fine-tuning in an environment where we can \n",
      "access powerful GPUs and memory resources.\n",
      "  Setup for fine-tuning\n",
      "Fine-tuning consistently achieves strong results across tasks but requires extensive computa-\n",
      "tional resources.\n",
      "  These are available both in free and professional \n",
      "tiers.\n",
      "----------------------------------------\n",
      "Page 257:\n",
      "Topics:\n",
      "  Weights and Biases (W&B) is an MLOps platform that can help developers monitor and docu-\n",
      "ment ML training workflows from end to end.\n",
      "  Creating a new API token on Hugging Face write permissions\n",
      "We can authenticate from the notebook like this:\n",
      "from huggingface_hub import notebook_login\n",
      "notebook_login()\n",
      "When prompted, paste your Hugging Face access token.\n",
      "\n",
      "  We can install these libraries from the Colab notebook as follows:\n",
      "!\n",
      "  Please note that if you want to push your model to Hugging Face later, you need to generate a new \n",
      "API token with write permissions on Hugging Face: https://huggingface.co/settings/tokens\n",
      "Figure 8.1:\n",
      "  Customizing LLMs and Their Output\n",
      "234\n",
      "\n",
      "  For W&B, we \n",
      "need to name the project; alternatively, we can use wandb's init() method:\n",
      "A note of caution before we start: when executing the code, you need to log in to \n",
      "different services, so make sure you pay attention when running the notebook!\n",
      "\n",
      "  As mentioned earlier, we will use W&B to get an \n",
      "idea of how well the training is working and if the model is improving over time.\n",
      "  pip install -U accelerate bitsandbytes datasets transformers peft trl \n",
      "sentencepiece wandb langchain huggingface_hub\n",
      "To download and train models from Hugging Face, we need to authenticate with the platform. \n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 258:\n",
      "Topics:\n",
      "  Again, we need to paste in our API token.\n",
      "\n",
      "  If the \n",
      "code comes from GitHub, we can filter by stars or by stars by repo owner.\n",
      "\n",
      "  This will ensure that we \n",
      "get new reports and a dashboard on W&B:\n",
      "import wandb\n",
      "if wandb.run is not None:\n",
      "    wandb.finish()\n",
      "\n",
      "  You can find your API key on the Authorize page: https://wandb.ai/authorize.\n",
      "\n",
      "  If the previous training run is still active – this could be from a previous execution of the notebook \n",
      "if you are running it a second time – let’s make sure we start a new one!\n",
      "  For example, we can use LangChain to set up training \n",
      "data.\n",
      "  In this recipe, we are fine-tuning for question-answering performance with the Squad V2 dataset. \n",
      "\n",
      "  Chapter 8\n",
      "235\n",
      "import os\n",
      "os.environ[\"WANDB_PROJECT\"] = \"finetuning\"\n",
      "To authenticate with W&B, you need to create a free account with them at https://www.wandb.\n",
      "\n",
      "  Next, we’ll need to choose a dataset against which we want to optimize.\n",
      "Subtopics:\n",
      "  Search engine placement could serve \n",
      "as a popularity filter, since it’s often based on user engagement with the content.\n",
      "  We can also customize our own dataset.\n",
      "  We can use lots of different \n",
      "datasets here that are appropriate for coding, storytelling, tool use, SQL generation, grade-school \n",
      "math questions (GSM8k), or many other tasks.\n",
      "  It would have been appealing to show data collection as a practical recipe in this \n",
      "chapter.\n",
      "  However, because of the complexity, it is out of the scope of the book.\n",
      "\n",
      "  There are quite a few methods available for filtering that could help reduce redundancy in \n",
      "the dataset.\n",
      "  Hugging Face provides a wealth of datasets, which \n",
      "can be viewed at this URL: https://huggingface.co/datasets.\n",
      "  For texts in natural language, quality filtering is not trivial.\n",
      "  ai.\n",
      "  Further, knowl-\n",
      "edge distillation techniques could be tweaked as a filter by fact density and accuracy.\n",
      "\n",
      "  You can see a detailed dataset description on Hugging Face: https://huggingface.co/spaces/\n",
      "evaluate-metric/squad_v2:\n",
      "from datasets import load_dataset\n",
      "dataset_name = \"squad_v2\"\n",
      "dataset = load_dataset(dataset_name, split=\"train\")\n",
      "eval_dataset\n",
      "  It might be harder to filter for quality from web data, but there are a lot of possibilities.\n",
      "  = load_dataset(dataset_name, split=\"validation\")\n",
      "\n",
      "  These cover a lot of different \n",
      "and even the most niche tasks.\n",
      "\n",
      "  For code \n",
      "models, we could apply code validation techniques to score segments as a quality filter.\n",
      "----------------------------------------\n",
      "Page 259:\n",
      "Topics:\n",
      "  The Squad V2 dataset has a part that’s sup-\n",
      "posed to be used in training and another one in validation, as we can see in the output of load_\n",
      "dataset(dataset_name):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features:\n",
      "  ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "236\n",
      "\n",
      "  The Squad V2 dataset is composed of various features, which we can see here:\n",
      "{'id': Value(dtype='string', id=None),\n",
      " 'title': Value(dtype='string', id=None),\n",
      " 'context': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None),\n",
      " 'answers': Sequence(feature={'text': Value(dtype='string', id=None),\n",
      " 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
      "The basic idea in training is prompting the model with a question and comparing the answer to \n",
      "the dataset.\n",
      "  LLaMa derivatives such as \n",
      "OpenLLaMa have performed quite well, as can be evidenced on the HF leaderboard: https://\n",
      "huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
      "OpenLLaMa version 1 cannot be used for coding tasks, because of the tokenizer.\n",
      "Subtopics:\n",
      "  We’ll use a 3B parameter model, which we’ll be able to use even on older hardware:\n",
      "\n",
      "  In the next section, we’ll use this setup to fine-tune an open-source LLM.\n",
      "Open-source models\n",
      "We want a small model that we can run locally at a decent token rate.\n",
      "  Therefore, let’s \n",
      "use v2!\n",
      "  LLaMa-2 models require \n",
      "signing a license agreement with your email address and getting confirmed (which, to be fair, \n",
      "can be very fast), as it comes with restrictions for commercial use.\n",
      "  We’ll use the validation splits for early stopping.\n",
      "  Early stopping will allow us to stop training \n",
      "when the validation error begins to degrade.\n",
      "\n",
      "  We are taking both training and validation splits.\n",
      "----------------------------------------\n",
      "Page 260:\n",
      "Topics:\n",
      "  Let’s load the model:\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
      "bnb_config = BitsAndBytesConfig(\n",
      "    load_in_4bit=True,\n",
      "    bnb_4bit_quant_type=\"nf4\",\n",
      "    bnb_4bit_compute_dtype=torch.float16,\n",
      ")\n",
      "device_map=\"auto\"\n",
      "base_model = AutoModelForCausalLM.from_pretrained(\n",
      "    model_id,\n",
      "    quantization_config=bnb_config,\n",
      "    device_map=\"auto\",\n",
      "    trust_remote_code=True,\n",
      ")\n",
      "\n",
      "  We’ll need to authenticate with Google for this to work.\n",
      "\n",
      "  base_model.config.use_cache = False\n",
      "The Bits and Bytes configuration makes it possible to quantize our model in 8, 4, 3, or even 2 bits \n",
      "with a much-accelerated inference and lower memory footprint, without incurring a big cost in \n",
      "terms of performance.\n",
      "\n",
      "  Chapter 8\n",
      "237\n",
      "model_id = \"openlm-research/open_llama_3b_v2\"\n",
      "new_model_name = f\"openllama-3b-peft-{dataset_name}\"\n",
      "We can use even smaller models such as EleutherAI/gpt-neo-125m, which can also give a par-\n",
      "ticularly good compromise between resource use and performance.\n",
      "\n",
      "  We are going to store model checkpoints on Google Drive; you need to confirm your login to your \n",
      "Google account:\n",
      "from google.colab import drive\n",
      "drive.mount('/content/gdrive')\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 261:\n",
      "Topics:\n",
      "  We can set our output directory for model checkpoints and logs to our Google Drive:\n",
      "output_dir = \"/content/gdrive/My Drive/results\"\n",
      "If you don’t want to use Google Drive, just set this to a directory on your computer.\n",
      "\n",
      "  For training, we need to set up a tokenizer:\n",
      "from transformers import AutoTokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_\n",
      "code=True)\n",
      "tokenizer.pad_token\n",
      "  We’ll set up LORA and other training arguments:\n",
      "from transformers import TrainingArguments, EarlyStoppingCallback\n",
      "from peft import LoraConfig\n",
      "# More info: https://github.com/huggingface/transformers/pull/24906\n",
      "base_model.config.pretraining_tp = 1\n",
      "peft_config = LoraConfig(\n",
      "    lora_alpha=16,\n",
      "    lora_dropout=0.1,\n",
      "    r=64,\n",
      "    bias=\"none\",\n",
      "    task_type=\"CAUSAL_LM\",\n",
      ")\n",
      "training_args = TrainingArguments(\n",
      "    output_dir=output_dir, \n",
      "    per_device_train_batch_size=4,\n",
      "    gradient_accumulation_steps=4,\n",
      "    learning_rate=2e-4,\n",
      "    logging_steps=10,\n",
      "    max_steps=2000,\n",
      "    num_train_epochs=100,\n",
      "    evaluation_strategy=\"steps\",\n",
      "    eval_steps=5,\n",
      "    save_total_limit=5,\n",
      "    push_to_hub=False,\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "238\n",
      "\n",
      "Subtopics:\n",
      "  = tokenizer.eos_token\n",
      "tokenizer.padding_side = \"right\"\n",
      "Now, we’ll define our training configuration.\n",
      "----------------------------------------\n",
      "Page 262:\n",
      "Topics:\n",
      "  Chapter 8\n",
      "239\n",
      "    load_best_model_at_end=True,\n",
      "    report_to=\"wandb\"\n",
      ")\n",
      "\n",
      "  The push_to_hub argument \n",
      "means that we can push the model checkpoints to the HuggingSpace Hub regularly during train-\n",
      "ing.\n",
      "  Finally, report_to=\"wandb\" means that we’ll send training \n",
      "stats, some model metadata, and hardware information to W&B, where we can look at graphs \n",
      "and dashboards for each run.\n",
      "\n",
      "  The training can then use our configuration:\n",
      "from trl import SFTTrainer\n",
      "trainer = SFTTrainer(\n",
      "    model=base_model,\n",
      "    train_dataset=dataset,\n",
      "    eval_dataset=eval_dataset,\n",
      "    peft_config=peft_config,\n",
      "    dataset_text_field=\"question\",  # this depends on the dataset!\n",
      "    \n",
      "  save_total_limit=5 means \n",
      "that only the last five models are saved.\n",
      "  For this to work, you need to set up the HuggingSpace authentication (with write permis-\n",
      "sions, as mentioned).\n",
      "  eval_steps is the number of update steps between two evaluations.\n",
      "  Alternatively, as I’ve done here, you can save your model locally or in the cloud, for example, in \n",
      "Google Drive in a directory.\n",
      "Subtopics:\n",
      "  max_seq_length=512,\n",
      "    tokenizer=tokenizer,\n",
      "    args=training_args,\n",
      "    callbacks=[EarlyStoppingCallback(early_stopping_patience=200)]\n",
      ")\n",
      "trainer.train()\n",
      "\n",
      "  Early stepping and a high number of \n",
      "maximum training steps should help to get the model to provide higher performance.\n",
      "  If we opt for this, as output_dir, we can use new_model_name.\n",
      "  This will be \n",
      "the repository name under which the model will be available here on Hugging Face: https://\n",
      "huggingface.co/models.\n",
      "\n",
      "  I’ve set max_steps and num_train_epochs very high, because I’ve \n",
      "noticed that training can still improve after many steps.\n",
      "  A few comments to explain some of these parameters are in order.\n",
      "  For early \n",
      "stopping, we need to set evaluation_strategy as \"steps\" and load_best_model_at_end=True.\n",
      "\n",
      "----------------------------------------\n",
      "Page 263:\n",
      "Topics:\n",
      "  Customizing LLMs and Their Output\n",
      "240\n",
      "\n",
      "  We should see some statistics as the training progresses, but it’s nicer to show the graph of per-\n",
      "formance, as we can see on W&B:\n",
      "Figure 8.2: Fine-tuning training loss over time (steps)\n",
      "\n",
      "  Let’s quickly show how to use this model in LangChain. \n",
      "\n",
      "  The training can take quite a while, even running on a TPU device.\n",
      "  Usually, the peft model is stored as an adapter, not as a full model; therefore, the loading is a \n",
      "bit different:\n",
      "from peft import PeftModel, PeftConfig\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "\n",
      "Subtopics:\n",
      "  trainer.model.push_to_hub(\n",
      "    repo_id=new_model_name\n",
      ")\n",
      "\n",
      "  We can now load the model back using a combination of our Hugging Face username and the \n",
      "repository name (the new model name).\n",
      "  After training is done, we can save the final checkpoint on disk for re-loading:\n",
      "trainer.model.save_pretrained(\n",
      "    os.path.join(output_dir, \"final_checkpoint\"),\n",
      ")\n",
      "\n",
      "  If you disable the early stopping, you can make this much faster.\n",
      "\n",
      "  Frequent evaluation slows the \n",
      "training down by a lot.\n",
      "  We can now share our final model with friends to brag about the performance we’ve achieved by \n",
      "manually pushing to Hugging Face:\n",
      "\n",
      "----------------------------------------\n",
      "Page 264:\n",
      "Topics:\n",
      "  Fine-tuning a PaLM model for text classification can be done like this:\n",
      "from skllm.models.palm import PaLMClassifier\n",
      "clf = PaLMClassifier(n_update_steps=100)\n",
      "clf.fit(X_train, y_train)\n",
      "  The Scikit-LLM library is not part of the setup that we dis-\n",
      "cussed in Chapter 3, Getting Started with LangChain, so you’d have to install it manually.\n",
      "  We’ve done everything so far on Google Colab, but we could equally execute this locally; just note \n",
      "that you need to have the huggingface peft library installed!\n",
      "\n",
      "  For example, both OpenAI’s GPT-3.5 and Google’s PaLM \n",
      "model offer this capability.\n",
      "  Chapter 8\n",
      "241\n",
      "from langchain.llms import HuggingFacePipeline\n",
      "model_id = 'openlm-research/open_llama_3b_v2'\n",
      "config = PeftConfig.from_pretrained(\"benji1a/openllama-3b-peft-squad_v2\")\n",
      "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
      "model = PeftModel.from_pretrained(model, \"benji1a/openllama-3b-peft-squad_\n",
      "v2\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_\n",
      "code=True)\n",
      "tokenizer.pad_token\n",
      "  We won’t go through a full recipe in \n",
      "this section, but please look at the Scikit-LLM library or the documentation of different cloud \n",
      "LLM providers to find all the details.\n",
      "  # y_train is a list of labels\n",
      "labels = clf.predict(X_test)\n",
      "\n",
      "  With the Scikit-LLM library, this is only a few lines of code.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "\n",
      "Subtopics:\n",
      "  Commercial models\n",
      "So far, we’ve shown how to fine-tune and deploy an open-source LLM.\n",
      "  You’d have to come up with a training dataset yourself.\n",
      "\n",
      "  I’ve also \n",
      "not included the training data, X_train.\n",
      "  This has been integrated with a few Python libraries.\n",
      "\n",
      "  Some commercial models \n",
      "can be fine-tuned on custom data as well.\n",
      "  = tokenizer.eos_token\n",
      "pipe = pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    max_length=256\n",
      ")\n",
      "\n",
      "----------------------------------------\n",
      "Page 265:\n",
      "Topics:\n",
      "  Prompts consist of three main components:\n",
      "•\t\n",
      "Instructions that describe the task requirements, goals, and format of input/output.\n",
      "  By prompting, we can accomplish few-shot learning or even zero-shot learning, as we’ll discuss \n",
      "in the next section.\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "242\n",
      "Similarly, you can fine-tune the GPT-3.5 model for text classification like this:\n",
      "from skllm.models.gpt import GPTClassifier\n",
      "clf = GPTClassifier(\n",
      "        base_model = \"gpt-3.5-turbo-0613\",\n",
      "        n_epochs = None, # int or None.\n",
      "  Prompt engineering\n",
      "Prompts are the instructions and examples we provide to language models to steer their behavior. \n",
      "\n",
      "  Interestingly, in the fine-tuning available on OpenAI, all inputs are passed through a moderation \n",
      "system to make sure that the inputs are compatible with safety standards.\n",
      "\n",
      "  •\t\n",
      "Input that the model must act on to generate the output.\n",
      "\n",
      "  •\t\n",
      "Examples that demonstrate the desired input-output pairs.\n",
      "  When None, will be determined \n",
      "automatically by OpenAI\n",
      "        default_label = \"Random\", # optional\n",
      ")\n",
      "clf.fit(X_train, y_train)\n",
      "  The following figure shows a few examples of prompting different language models (source: Pre-\n",
      "train, Prompt, and Predict - A Systematic Survey of Prompting Methods in Natural Language Processing \n",
      "by Liu and colleagues, 2021):\n",
      "\n",
      "  Prompts act as instructions that demonstrate to the LLM what the desired input-output map-\n",
      "ping is.\n",
      "\n",
      "  # y_train is a list of labels\n",
      "labels = clf.predict(X_test)\n",
      "\n",
      "Subtopics:\n",
      "  Carefully engineered prompts can \n",
      "make LLMs suitable for a wide variety of tasks beyond what they were originally trained for. \n",
      "\n",
      "  This concludes fine-tuning.\n",
      "  They provide diverse demon-\n",
      "strations of how different inputs should map to outputs.\n",
      "\n",
      "  They are important for steering the behavior of LLMs because they allow you to align the model \n",
      "outputs to human intentions without expensive retraining.\n",
      "  They \n",
      "explain the task to the model unambiguously.\n",
      "\n",
      "  LLMs can be deployed and queried without any task-specific tuning. \n",
      "\n",
      "----------------------------------------\n",
      "Page 266:\n",
      "Topics:\n",
      "  The most important first step is to start simple and work iteratively.\n",
      "  Chapter 8\n",
      "243\n",
      "Figure 8.3: Prompt examples, particularly knowledge probing in close form, and summari-\n",
      "zation\n",
      "Prompt engineering, also known as in-context learning, refers to techniques to steer LLM behavior \n",
      "through carefully designed prompts, without changing the model weights.\n",
      "Subtopics:\n",
      "  Providing relevant \n",
      "examples is highly effective in demonstrating the required reasoning chains or output styles.\n",
      "\n",
      "  This avoids overwhelming the model initially.\n",
      "  Prompt tuning, on the other hand, \n",
      "provides intuitive control over model behavior but is sensitive to the precise wording and design \n",
      "of prompts, suggesting the need for carefully crafted guidelines to achieve desired results.\n",
      "  For complex reasoning tasks, prompting the model to explain its step-by-step thought process \n",
      "leads to increased accuracy.\n",
      "  Problem decomposition prompts that break down complex problems into smaller, more \n",
      "manageable sub-tasks also improve reliability by enabling a more structured reasoning process. \n",
      "\n",
      "  But \n",
      "what do good prompts look like?\n",
      "\n",
      "  Techniques like chain-of-thought prompting guide the model to \n",
      "reason explicitly.\n",
      "  Sampling multiple candidate responses and picking the most consistent one helps reduce errors \n",
      "and inconsistencies, compared to relying on a single-model output.\n",
      "\n",
      "  The goal is to align \n",
      "the model outputs with human intentions for a given task.\n",
      "  Be as specific, descriptive, \n",
      "and detailed as possible about the exact task and desired format of the output.\n",
      "  Break complex tasks down \n",
      "into simpler sub-tasks.\n",
      "  Providing few-shot examples further demonstrates the desired reasoning for-\n",
      "mat.\n",
      "  Begin with concise, straight-\n",
      "forward instructions and build up complexity gradually as needed.\n",
      "----------------------------------------\n",
      "Page 267:\n",
      "Topics:\n",
      "  This table gives a brief overview of a few methods of prompting compared to fine-tuning:\n",
      "Technique\n",
      "Description\n",
      "Key Idea\n",
      "Performance \n",
      "Considerations\n",
      "Zero-Shot \n",
      "Prompting\n",
      "No examples provided; rely \n",
      "on the model’s training\n",
      "Leverages the model’s pre-\n",
      "training\n",
      "Works for simple \n",
      "tasks, but struggles \n",
      "with complex \n",
      "reasoning\n",
      "Few-Shot \n",
      "Prompting\n",
      "Provides a few demos of \n",
      "input and desired output\n",
      "Shows desired reasoning \n",
      "format\n",
      "Tripled accuracy \n",
      "on grade-school \n",
      "math\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "244\n",
      "Instead of focusing on what not to do, clearly specify the desired actions and outcomes.\n",
      "  Chain-of-Thought \n",
      "(CoT) prompting generates explicit reasoning steps, leading to the final output.\n",
      "  Prompt techniques\n",
      "Basic prompting methods include zero-shot prompting with just the input text, and few-shot \n",
      "prompting with a few demonstration examples showing the desired input-output pairs.\n",
      "Subtopics:\n",
      "  With \n",
      "iteration and experimentation, prompts can be optimized to improve reliability, even for complex \n",
      "tasks, and achieve a performance often comparable to fine-tuning.\n",
      "\n",
      "  Start simple, be specific, \n",
      "provide examples, prompt for explanations, decompose problems, and sample multiple responses \n",
      "– these are some best practices to steer LLMs effectively using careful prompt engineering.\n",
      "  This is especially \n",
      "beneficial for complex reasoning tasks.\n",
      "  Self-consistency sampling gener-\n",
      "ates multiple outputs and selects the one that aligns best with the examples.\n",
      "  Avoid imprecise or vague prompts.\n",
      "  CoT prompts can be manually written or generated au-\n",
      "tomatically via methods like augment-prune-select.\n",
      "\n",
      "  Research-\n",
      "ers have identified biases like majority label bias and recency bias that contribute to variability \n",
      "in few-shot performance.\n",
      "  Direct, \n",
      "unambiguous instructions work best.\n",
      "  More advanced prompting techniques include instruction prompting, where the task require-\n",
      "ments are described explicitly rather than just demonstrated.\n",
      "  Careful prompt design through example selection, ordering, and for-\n",
      "matting can help mitigate these issues.\n",
      "\n",
      "  After learning about best practices, let’s look at a few prompt techniques, from simple to increas-\n",
      "ingly more advanced!\n",
      "\n",
      "----------------------------------------\n",
      "Page 268:\n",
      "Topics:\n",
      "  Chapter 8\n",
      "245\n",
      "CoT\n",
      "Prefix responses with \n",
      "intermediate reasoning \n",
      "steps\n",
      "Gives the model space to \n",
      "reason before answering\n",
      "Quadrupled \n",
      "accuracy on a math \n",
      "dataset\n",
      "Least-\n",
      "to-Most \n",
      "Prompting\n",
      "Prompts the model for \n",
      "simpler subtasks first\n",
      "Decomposes a problem \n",
      "into smaller pieces\n",
      "Boosted accuracy \n",
      "from 16% to 99.7% \n",
      "on some tasks\n",
      "Self-\n",
      "Consistency\n",
      "Picks the most frequent \n",
      "answer from multiple \n",
      "samples\n",
      "Increases redundancy\n",
      "Gained 1–24 \n",
      "percentage points \n",
      "across benchmarks\n",
      "Chain-of-\n",
      "Density\n",
      "Iteratively creates dense \n",
      "summaries by adding \n",
      "entities\n",
      "Generates rich, concise \n",
      "summaries\n",
      "Improves \n",
      "information \n",
      "density in \n",
      "summaries\n",
      "Chain-of-\n",
      "Verification \n",
      "(CoV)\n",
      "Verifies an initial response \n",
      "by generating and \n",
      "answering questions\n",
      "Mimics human verification\n",
      "Enhances \n",
      "robustness and \n",
      "confidence\n",
      "Active \n",
      "Prompting\n",
      "Picks uncertain samples for \n",
      "human labeling as examples\n",
      "Finds effective few-shot \n",
      "examples\n",
      "Improves few-shot \n",
      "performance\n",
      "Tree-of-\n",
      "Thought\n",
      "Generates and automatically \n",
      "evaluates multiple \n",
      "responses\n",
      "Allows backtracking \n",
      "through reasoning paths\n",
      "Finds an optimal \n",
      "reasoning route\n",
      "Verifiers\n",
      "Trains a separate model to \n",
      "evaluate responses\n",
      "Filters out incorrect \n",
      "responses\n",
      "Lifted grade-school \n",
      "math accuracy \n",
      "by ~20 percentage \n",
      "points\n",
      "Fine-\n",
      "Tuning\n",
      "Fine-tunes on an \n",
      "explanation dataset \n",
      "generated via prompting\n",
      "Improves the model’s \n",
      "reasoning abilities\n",
      "73% accuracy on a \n",
      "commonsense QA \n",
      "dataset\n",
      "Table 8.2:\n",
      "  For closed-book QA, few-shot \n",
      "examples with an evidence-question-answer format work better than a QA format.\n",
      "\n",
      "  Prompting techniques for LLMs compared to fine-tuning\n",
      "Some prompting techniques incorporate external information retrieval to provide missing con-\n",
      "text to the LLM before generating the output.\n",
      "Subtopics:\n",
      "  For open-domain QA, relevant paragraphs can be \n",
      "retrieved via search engines and incorporated into the prompt.\n",
      "----------------------------------------\n",
      "Page 269:\n",
      "Topics:\n",
      "  Overall, prompts combine the strengths of instructions and examples to maximize \n",
      "steering of the LLM for the task at hand.\n",
      "\n",
      "  additional_kwargs={} \n",
      "example=False\n",
      "Few-shot learning\n",
      "Few-shot learning presents the LLM with just a few input-output examples relevant to the task, \n",
      "without explicit instructions.\n",
      "  You can find all the examples from this section in the prompting directory in the GitHub repository \n",
      "for the book.\n",
      "  Customizing LLMs and Their Output\n",
      "246\n",
      "In the next few subsections, we’ll go through a few of the aforementioned techniques.\n",
      "  LangC-\n",
      "hain provides tools to enable advanced prompt engineering strategies like zero-shot prompting, \n",
      "few-shot learning, chain-of-thought, self-consistency, and tree-of-thought.\n",
      "  This prompt tests the \n",
      "capabilities of the pre-trained model to understand and follow the instructions:\n",
      "from langchain import PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "model = ChatOpenAI()\n",
      "\n",
      "  Zero-shot prompting\n",
      "Zero-shot prompting, as opposed to few-shot prompting, involves feeding task instructions \n",
      "directly to an LLM without providing any demonstrations or examples.\n",
      "Subtopics:\n",
      "  Let’s start with the vanilla strategy: we just ask for a solution.\n",
      "\n",
      "  The FewShotPromptTemplate allows you to show the model just a few demonstration examples \n",
      "of the task to prime it, without explicit instructions.\n",
      "\n",
      "  All these techniques \n",
      "described here enhance the accuracy, consistency, and reliability of LLMs’ reasoning capabilities \n",
      "on complex tasks by providing clearer instructions, fine-tuning with targeted data, employing \n",
      "problem breakdown strategies, incorporating diverse sampling approaches, integrating verifi-\n",
      "cation mechanisms, and adopting probabilistic modeling frameworks.\n",
      "\n",
      "  Carefully selected, ordered, and formatted examples can improve the model’s \n",
      "inference abilities.\n",
      "  However, few-shot learning can be prone to biases and variability across trials. \n",
      "\n",
      "  This allows the model to infer the intentions and goals purely from \n",
      "demonstrations.\n",
      "  prompt = PromptTemplate(input_variables=[\"text\"], template=\"Classify the \n",
      "sentiment of this text: {text}\")\n",
      "chain = prompt | model\n",
      "print(chain.invoke({\"text\": \"I hated that movie, it was terrible!\"}))\n",
      "\n",
      "  This outputs the sentiment classification prompt with the input text, without any examples:\n",
      "content='The sentiment of this text is negative.'\n",
      "  Adding explicit instructions can make the intentions more transparent to the model and improve \n",
      "robustness.\n",
      "----------------------------------------\n",
      "Page 270:\n",
      "Topics:\n",
      "  In this \n",
      "example, we want an LLM to categorize customer feedback into Positive, Negative, or Neutral. \n",
      "\n",
      "  \",\n",
      "    \"output\": \"Negative\"\n",
      "}]\n",
      "We can use these examples in a prompt like this:\n",
      "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "example_prompt = PromptTemplate(\n",
      "    template=\"{input} -> {output}\",\n",
      "    input_variables=[\"input\", \"output\"],\n",
      ")\n",
      "prompt = FewShotPromptTemplate(\n",
      "    examples=examples,\n",
      "    example_prompt=example_prompt,\n",
      "    suffix=\"Question: {input}\",\n",
      "    input_variables=[\"input\"]\n",
      ")\n",
      "print((prompt | ChatOpenAI()).invoke({\"input\": \" This is an excellent book \n",
      "with high quality explanations.\"}))\n",
      "\n",
      "  Chapter 8\n",
      "247\n",
      "Let’s extend the previous example for sentiment classification with few-shot prompting.\n",
      "  We should get the following output:\n",
      "content='Positive' additional_kwargs={} example=False\n",
      "You can expect the LLM to use these examples to guide its classification of the new sentence.\n",
      "Subtopics:\n",
      "  The \n",
      "few-shot method primes the model without extensive training, relying instead on its pre-trained \n",
      "knowledge and the context provided by the examples.\n",
      "\n",
      "  [{\n",
      "    \"input\": \"I absolutely love the new update!\n",
      "  \",\n",
      "    \"output\": \"Positive\",\n",
      "    },{\n",
      "    \"input\": \"It's okay, but I think it could use more features.\n",
      "  We provide it with a few examples:\n",
      "examples =\n",
      "  \",\n",
      "    \"output\": \"Neutral\",\n",
      "    }, {\n",
      "    \"input\": \"I'm disappointed with the service, I expected much better \n",
      "performance.\n",
      "  Everything works \n",
      "seamlessly.\n",
      "----------------------------------------\n",
      "Page 271:\n",
      "Topics:\n",
      "  When asking an LLM to reason through a problem, it is often more effective to have it explain its \n",
      "reasoning before stating the final answer.\n",
      "  My friend \n",
      "gave me 3 apples.\n",
      "  I ate 2 apples.\n",
      "  \"\n",
      "prompt = PromptTemplate(\n",
      "  template=reasoning_prompt,\n",
      "  input_variables=[\"question\"]\n",
      ")\n",
      "model = ChatOpenAI()\n",
      "chain = prompt | model\n",
      "print(chain.invoke({\n",
      "   \"question\": \"There were 5 apples originally.\n",
      "  In zero-shot CoT, we just add the instruc-\n",
      "tion “Let’s think step by step!”\n",
      "  There are two variants of CoT, zero-shot and few-shot.\n",
      "  Asking \n",
      "an LLM to explain its thought process aligns well with its core capabilities.\n",
      "\n",
      "  Chain-of-thought prompting\n",
      "CoT prompting aims to encourage reasoning by getting the model to provide intermediate steps, \n",
      "leading to the definitive answer.\n",
      "  This encourages the LLM to logically think through \n",
      "the problem first, rather than just guessing the answer and trying to justify it afterward.\n",
      "  The Semant\n",
      "icSimilarityExampleSelector automatically finds the most relevant examples for each input.\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "248\n",
      "To choose examples tailored to each input, FewShotPromptTemplate can accept a SemanticSim\n",
      "ilarityExampleSelector, based on embeddings rather than hardcoded examples.\n",
      "  For example:\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "reasoning_prompt = \"{question}\\nLet's think step by step!\n",
      "Subtopics:\n",
      "  This is done by prefixing the prompt with instructions to show \n",
      "its thinking.\n",
      "\n",
      "  How many apples do I have now?\n",
      "  For many tasks, standard few-shot prompting works well, but there are many other techniques \n",
      "and extensions when dealing with more complex reasoning tasks.\n",
      "\n",
      "  to the prompt.\n",
      "\n",
      "  \",\n",
      "}))\n",
      "After running this, we get the reasoning process together with the result:\n",
      "\n",
      "----------------------------------------\n",
      "Page 272:\n",
      "Topics:\n",
      "  Few-shot chain-of-thought prompting is a few-shot prompt, where the reasoning is explained \n",
      "as part of the example solutions, with the idea to encourage an LLM to explain its reasoning \n",
      "before deciding.\n",
      "\n",
      "  It has been shown that CoT prompting can lead to more accurate results; however, this perfor-\n",
      "mance boost was found to be proportional to the size of the model, and the improvements were \n",
      "negligible or even negative in smaller models.\n",
      "\n",
      "  Chapter 8\n",
      "249\n",
      "content='Step 1: Originally, there were 5 apples.\\nStep 2: I ate 2 \n",
      "apples.\\nStep 3: So, I had 5 - 2 = 3 apples left.\\nStep 4: My friend gave \n",
      "me 3 apples.\\nStep 5: Adding the apples my friend gave me, I now have 3 + \n",
      "3 = 6 apples.'\n",
      "  \",\n",
      "    \"output\": \"Love and absolute works seamlessly are examples of positive \n",
      "sentiment.\n",
      "  This encourages the LLM to give a \n",
      "similar result explaining its reasoning.\n",
      "\n",
      "  additional_kwargs={} example=False\n",
      "The preceding approach is also called zero-shot chain-of-thought.\n",
      "\n",
      "Subtopics:\n",
      "  Therefore, the sentiment is neutral\",\n",
      "    }, {\n",
      "    \"input\": \"I'm disappointed with the service, I expected much better \n",
      "performance.\n",
      "  examples =\n",
      "  [{\n",
      "    \"input\": \"I absolutely love the new update!\n",
      "  \",\n",
      "    \"output\": \"It's okay is not an endorsement.\n",
      "  A good example of self-consistency prompting with LLMs is in the context of \n",
      "fact verification or information synthesis, where accuracy is paramount.\n",
      "\n",
      "  Therefore, the sentiment is positive\",\n",
      "    },{\n",
      "    \"input\": \"It's okay, but I think it could use more features.\n",
      "  \",\n",
      "    \"output\": \"The customer is disappointed and expected more.\n",
      "  This is \n",
      "negative\"\n",
      "}]\n",
      "In these examples, the reasons for the decision are explained.\n",
      "  These are then compared against each other, and the most consistent or frequent answer is selected \n",
      "as the final output.\n",
      "  If we go back to the few-shot examples from earlier, we can extend them as follows:\n",
      "\n",
      "  The customer further \n",
      "thinks it should be extended.\n",
      "  Self-consistency\n",
      "With self-consistency prompting, the model generates multiple candidate answers to a question. \n",
      "\n",
      "  Everything works \n",
      "seamlessly.\n",
      "----------------------------------------\n",
      "Page 273:\n",
      "Topics:\n",
      "  = LLMChain(\n",
      "   llm=ChatOpenAI(),\n",
      "   prompt=solutions_prompt,\n",
      "   output_key=\"solutions\"\n",
      ")\n",
      "\n",
      "  Most frequent solution: \n",
      "\"\"\"\n",
      "consistency_prompt = PromptTemplate(\n",
      "   template=consistency_template,\n",
      "   input_variables=[\"solutions\"]\n",
      ")\n",
      "consistency_chain = LLMChain(\n",
      "   llm=ChatOpenAI(),\n",
      "   prompt=consistency_prompt,\n",
      "   output_key=\"best_solution\"\n",
      ")\n",
      "\n",
      "  For the second step, we want to count the different answers.\n",
      "  Customizing LLMs and Their Output\n",
      "250\n",
      "In the first step, we’ll create multiple solutions to a question or a problem:\n",
      "from langchain import PromptTemplate, LLMChain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "solutions_template = \"\"\"\n",
      "Generate {num_solutions} distinct answers to this question:\n",
      "{question}\n",
      "Solutions:\n",
      "\"\"\"\n",
      "solutions_prompt = PromptTemplate(\n",
      "   template=solutions_template,\n",
      "   input_variables=[\"question\", \"num_solutions\"]\n",
      ")\n",
      "\n",
      "Subtopics:\n",
      "  solutions_chain\n",
      "  We can use an LLM again:\n",
      "consistency_template = \"\"\"\n",
      "For each answer in {solutions}, count the number of times it occurs. \n",
      "\n",
      "  Finally, choose the answer that occurs most.\n",
      "\n",
      "----------------------------------------\n",
      "Page 274:\n",
      "Topics:\n",
      "  There is actually an implementation now of ToT in the LangChain experimental package; however, \n",
      "let’s walk through an instructive step-by-step example of implementing ToT using LangChain.\n",
      "\n",
      "  This approach leverages the model’s ability to reason and utilize internal knowledge while reduc-\n",
      "ing the risk of outliers or incorrect information, by focusing on the most recurring answer, thus \n",
      "improving the overall reliability of the response given by the LLM.\n",
      "Tree-of-thought\n",
      "In Tree-of-Thought (ToT) prompting, we generate multiple problem-solving steps or approaches \n",
      "for a given prompt and then use the AI model to critique them.\n",
      "  We should get a response like this:\n",
      "1776 is the year in which the Declaration of Independence of the United \n",
      "States was signed.\n",
      "  First, we’ll define our four chain components with PromptTemplates.\n",
      "  Let’s ask a simple question and check the answer:\n",
      "print(answer_chain.run(\n",
      "   question=\"Which year was the Declaration of Independence of the United \n",
      "States signed?\",\n",
      "   num_solutions=\"5\"\n",
      "))\n",
      "\n",
      "  Chapter 8\n",
      "251\n",
      "Let’s put these two chains together with a SequentialChain:\n",
      "from langchain.chains import SequentialChain\n",
      "answer_chain = SequentialChain(\n",
      "   chains=[solutions_chain, consistency_chain],\n",
      "   input_variables=[\"question\", \"num_solutions\"],\n",
      "   output_variables=[\"best_solution\"]\n",
      ")\n",
      "\n",
      "  We should get the right response based on the vote; however, of the five responses we produced, \n",
      "three were wrong.\n",
      "\n",
      "  It occurs twice in the given answers (3 and 4).\n",
      "\n",
      "Subtopics:\n",
      "  The critique will be based on the \n",
      "model’s judgment of the solution’s suitability to the problem.\n",
      "\n",
      "  We need a solution template, \n",
      "an evaluation template, a reasoning template, and a ranking template.\n",
      "\n",
      "----------------------------------------\n",
      "Page 275:\n",
      "Topics:\n",
      "  Customizing LLMs and Their Output\n",
      "252\n",
      "Let’s first generate solutions:\n",
      "solutions_template = \"\"\"\n",
      "Generate {num_solutions} distinct solutions for {problem}.\n",
      "  Enhanced Reasoning:\n",
      "\"\"\"\n",
      "reasoning_prompt = PromptTemplate(\n",
      "  template=reasoning_template,\n",
      "  input_variables=[\"evaluations\"]\n",
      ")\n",
      "\n",
      "  Solutions:\n",
      "\"\"\"\n",
      "solutions_prompt = PromptTemplate(\n",
      "   template=solutions_template,\n",
      "   input_variables=[\"problem\", \"factors\", \"num_solutions\"]\n",
      ")\n",
      "Let’s ask the LLM to evaluate these solutions:\n",
      "evaluation_template = \"\"\"\n",
      "Evaluate each solution in {solutions} by analyzing pros, cons, \n",
      "feasibility, and probability of success.\n",
      "\n",
      "  Evaluations:\n",
      "\"\"\"\n",
      "evaluation_prompt = PromptTemplate(\n",
      "  template=evaluation_template,\n",
      "  input_variables=[\"solutions\"] \n",
      ")\n",
      "\n",
      "Subtopics:\n",
      "  Consider \n",
      "factors like {factors}.\n",
      "\n",
      "  After this step, we want to reason a bit more about them:\n",
      "reasoning_template = \"\"\"\n",
      "For the most promising solutions in {evaluations}, explain scenarios, \n",
      "implementation strategies, partnerships needed, and handling potential \n",
      "obstacles.\n",
      "\n",
      "----------------------------------------\n",
      "Page 276:\n",
      "Topics:\n",
      "  = LLMChain(\n",
      "   llm=ChatOpenAI(),\n",
      "   prompt=ranking_prompt,\n",
      "   output_key=\"ranked_solutions\"\n",
      ")\n",
      "\n",
      "  Chapter 8\n",
      "253\n",
      "Finally, we can rank these solutions given our reasoning so far:\n",
      "ranking_template = \"\"\"\n",
      "Based on the evaluations and reasoning, rank the solutions in {enhanced_\n",
      "reasoning} from most to least promising.\n",
      "\n",
      "  Next, we create chains from these templates before we put the chains all together:\n",
      "from langchain.chains.llm import LLMChain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "solutions_chain\n",
      "  Ranked Solutions:\n",
      "\"\"\"\n",
      "ranking_prompt = PromptTemplate(\n",
      "  template=ranking_template,\n",
      "  input_variables=[\"enhanced_reasoning\"]\n",
      ")\n",
      "\n",
      "  = LLMChain(\n",
      "   llm=ChatOpenAI(),\n",
      "   prompt=solutions_prompt,\n",
      "   output_key=\"solutions\"\n",
      ")\n",
      "evalutation_chain = LLMChain(\n",
      "   llm=ChatOpenAI(),\n",
      "   prompt=evaluation_prompt,\n",
      "   output_key=\"evaluations\"\n",
      ")\n",
      "reasoning_chain = LLMChain(\n",
      "   llm=ChatOpenAI(),\n",
      "   prompt=reasoning_prompt,\n",
      "   output_key=\"enhanced_reasoning\"\n",
      ")\n",
      "ranking_chain\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 277:\n",
      "Topics:\n",
      "  The ToT approach helps \n",
      "avoid dead ends by fostering exploration.\n",
      "  3. Evaluate existing language models and identify their strengths and \n",
      "weaknesses in reasoning.\n",
      "\n",
      "  Customizing LLMs and Their Output\n",
      "254\n",
      "Please note how each output_key corresponds to an input_key in the prompt of the following \n",
      "chain.\n",
      "  If you want to see more examples, in the LangChain \n",
      "cookbook, you can find a ToT for playing sudoku.\n",
      "\n",
      "  1. Train or fine-tune language models using datasets that are relevant to \n",
      "the reasoning task at hand.\n",
      "\n",
      "  5. Iteratively refine and optimize the reasoning capabilities of the \n",
      "language models based on evaluation results.\n",
      "\n",
      "  Finally, we connect these chains into a SequentialChain:\n",
      "from langchain.chains import SequentialChain\n",
      "tot_chain = SequentialChain(\n",
      "   chains=[solutions_chain, evalutation_chain, reasoning_chain, ranking_\n",
      "chain],\n",
      "   input_variables=[\"problem\", \"factors\", \"num_solutions\"],\n",
      "   output_variables=[\"ranked_solutions\"]\n",
      ")\n",
      "print(tot_chain.run(\n",
      "   problem=\"Prompt engineering\",\n",
      "   factors=\"Requirements for high task performance, low token use, and few \n",
      "calls to the LLM\",\n",
      "   num_solutions=3\n",
      "))\n",
      "Let’s run our tot_chain and see the printed output:\n",
      "\n",
      "  This allows us to leverage the LLM at each stage of the reasoning process.\n",
      "  2. Develop or adapt reasoning algorithms and techniques to improve the \n",
      "performance of language models in specific reasoning tasks.\n",
      "\n",
      "  4. Implement evaluation metrics to measure the reasoning performance of \n",
      "the language models.\n",
      "\n",
      "  They show the strengths of ToT. A lot of these topics \n",
      "are part of this chapter, while some will come up in Chapter 9, Generative AI in Production, where \n",
      "we’ll discuss evaluating LLMs and their performance.\n",
      "\n",
      "Subtopics:\n",
      "  It is important to note that the ranking of solutions may vary depending \n",
      "on the specific context and requirements of each scenario.\n",
      "\n",
      "  I wholeheartedly agree with the suggestions.\n",
      "----------------------------------------\n",
      "Page 278:\n",
      "Topics:\n",
      "  4.\t\n",
      "Name a few fine-tuning methods.\n",
      "\n",
      "  In the first recipe of this chapter, we implemented \n",
      "fine-tuning of a small open-source model for question answering.\n",
      "\n",
      "  LangChain \n",
      "provides building blocks to unlock advanced prompting strategies like few-shot learning, CoT, \n",
      "ToT, and others, as we’ve shown in the examples.\n",
      "\n",
      "  What is few-shot learning?\n",
      "7.\t\n",
      "\n",
      "  In Chapter 9, Generative AI in Production, we’ll talk about the productionization of generative AI \n",
      "and critical issues related to it, such as evaluating LLM apps, deploying them to a server, and \n",
      "monitoring them.\n",
      "\n",
      "  Questions\n",
      "I’d recommend that you go back to the corresponding sections of this chapter if you are unsure \n",
      "about any of the answers to these questions:\n",
      "1.\t\n",
      "\n",
      "  Summary\n",
      "Conditioning allows steering generative AI to improve performance, safety, and quality.\n",
      "  5.\t\n",
      "\n",
      "  Chapter 8\n",
      "255\n",
      "Prompt design is highly significant for unlocking LLM reasoning capabilities, and it offers the \n",
      "potential for future advancements in models and prompting techniques.\n",
      "  3.\t\n",
      "\n",
      "  How does ToT work?\n",
      "\n",
      "  What is CoT prompting?\n",
      "8.\t \n",
      "  What is quantization?\n",
      "6.\t\n",
      "\n",
      "  What is conditioning, and what is alignment?\n",
      "2.\t \n",
      "Subtopics:\n",
      "  These principles and \n",
      "techniques form a valuable toolkit for researchers and practitioners working with LLMs.\n",
      "\n",
      "  In fine-tuning, the lan-\n",
      "guage model is trained on many examples of tasks formulated as natural language instructions, \n",
      "along with appropriate responses.\n",
      "  In this \n",
      "chapter, the focus is on conditioning through fine-tuning and prompting.\n",
      "  These \n",
      "methods have been shown to enhance accuracy and consistency in reasoning tasks.\n",
      "  This is often done through reinforcement learning with human \n",
      "feedback; however, other techniques have been developed that have been shown to produce com-\n",
      "petitive results with lower resource footprints.\n",
      "  What is instruction tuning, and what is its importance?\n",
      "\n",
      "  There are many techniques for prompting that can improve the reliability of LLMs in complex \n",
      "reasoning tasks, including step-by-step prompting, alternate selection, inference prompts, prob-\n",
      "lem decomposition, sampling multiple responses, and employing separate verifier models.\n",
      "  What are the different methods of conditioning, and how can we distinguish them?\n",
      "\n",
      "----------------------------------------\n",
      "Page 279:\n",
      "Topics:\n",
      "  Customizing LLMs and Their Output\n",
      "256\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 280:\n",
      "Topics:\n",
      "  This chapter \n",
      "explores the practical considerations and best practices for productionizing generative AI, spe-\n",
      "cifically LLM apps.\n",
      "  We’ll discuss evaluation and observability, and cover a broad range of topics that encompass \n",
      "the governance and lifecycle management of operationalized AI and decision models, including \n",
      "generative AI models.\n",
      "\n",
      "  Throughout the chapter, we’ll work on practical examples with LLM apps, which you can find \n",
      "in the GitHub repository for the book at https://github.com/benman1/generative_ai_with_\n",
      "langchain.\n",
      "\n",
      "  So far, we’ve talked about models, agents, and LLM apps as well as different use cases, but there \n",
      "are many issues that become important when deploying these apps into production to engage \n",
      "with customers and to make decisions that can have a significant financial impact.\n",
      "  9\n",
      "Generative AI in Production\n",
      "As we’ve discussed in this book, LLMs have gained significant attention in recent years due to \n",
      "their ability to generate human-like text.\n",
      "  From creative writing to conversational chatbots, these \n",
      "generative AI models have diverse applications across industries.\n",
      "Subtopics:\n",
      "  While getting an LLM app ready for production, offline evaluation provides a preliminary un-\n",
      "derstanding of a model’s abilities in a controlled setting, and when in production, observability \n",
      "offers continuing insights into its performance in live environments.\n",
      "  However, taking these complex \n",
      "neural network systems from research to real-world deployment comes with significant challenges.\n",
      "\n",
      "  We’ll discuss a few tools \n",
      "for either case and I’ll give examples.\n",
      "  Main-\n",
      "taining rigorous testing, auditing, and ethical safeguards is essential for trustworthy deployment. \n",
      "\n",
      "  We’ll also discuss the deployment of LLM applications and \n",
      "give an overview of available tools and examples for deployment.\n",
      "\n",
      "  Before we deploy an application, performance and regulatory requirements \n",
      "need to be ensured, it needs to be robust at scale, and finally monitoring has to be in place.\n",
      "----------------------------------------\n",
      "Page 281:\n",
      "Topics:\n",
      "  •\t\n",
      "Resource management: The resource demands of LLMs call for an infrastructure that \n",
      "is both efficient and environmentally sustainable.\n",
      "  •\t\n",
      "Data security: Protecting sensitive information within LLM processes is essential for pri-\n",
      "vacy and compliance.\n",
      "  Practices to ensure deployment readiness involve:\n",
      "•\t\n",
      "Data management: Rigorous attention to data quality is critical to avoid biases that can \n",
      "emanate from imbalanced or inappropriate training data.\n",
      "  Generative AI in Production\n",
      "258\n",
      "The main sections of this chapter are:\n",
      "•\t\n",
      "How to get LLM apps ready for production\n",
      "•\t\n",
      "How to evaluate LLM apps\n",
      "•\t\n",
      "How to deploy LLM apps\n",
      "•\t\n",
      "How to observe LLM apps\n",
      "Let’s start with an overview of what it means and involves to get an LLM app ready for production!\n",
      "\n",
      "  •\t\n",
      "Ethical deployment and compliance: LLM applications are potentially capable of gen-\n",
      "erating harmful content, thus necessitating strict review processes, safety guidelines, \n",
      "and compliance with regulations such as HIPAA, especially in sensitive sectors such as \n",
      "healthcare.\n",
      "\n",
      "Subtopics:\n",
      "  Strong encryption measures and stringent access controls bolster \n",
      "security measures.\n",
      "\n",
      "  •\t\n",
      "Interpretability: To build trust and offer insight into the decision-making processes of \n",
      "LLMs, interpretability tools are increasingly important for users who need clarity on how \n",
      "model decisions are reached.\n",
      "\n",
      "  Detecting these deviations necessitates prompt retraining or model \n",
      "adjustments.\n",
      "\n",
      "  How to get LLM apps ready for production\n",
      "Deploying LLM applications to production is intricate, encompassing robust data management, \n",
      "ethical foresight, efficient resource allocation, diligent monitoring, and alignment with behavioral \n",
      "guidelines.\n",
      "  Substantial efforts in data cu-\n",
      "ration and ongoing scrutiny of model outputs are required to mitigate emerging biases.\n",
      "\n",
      "  Innovation in infrastructure helps to \n",
      "reduce costs and address environmental concerns tied to the energy demands of LLMs.\n",
      "\n",
      "  •\t\n",
      "Performance management: Models must be continually monitored for data drift—where \n",
      "changes in input data patterns can alter model performance—and performance degra-\n",
      "dation over time.\n",
      "----------------------------------------\n",
      "Page 282:\n",
      "Topics:\n",
      "  Human reviewers serve as an \n",
      "essential checkpoint in content validation and bring ethical discernment to AI outputs, ensuring \n",
      "adherence across all contexts.\n",
      "\n",
      "  We discussed hallucinations in Chapter 4, Building Capable Assistants.\n",
      "  •\t\n",
      "Hallucination mitigation: Hallucinations refer to instances where LLMs inadvertently \n",
      "generate or recall sensitive personal information from their training data corpus in the \n",
      "outputs, despite no prompting for such details in the input source—highlighting critical \n",
      "privacy concerns and the need for mitigation strategies.\n",
      "\n",
      "  First and \n",
      "foremost, it’s crucial to develop standardized datasets with relevant benchmarks to test and mea-\n",
      "sure model capabilities, including the detection of regressions and alignment with defined goals.\n",
      "\n",
      "  Chapter 9\n",
      "259\n",
      "•\t\n",
      "Model behavior standards: Models must align with ethical guidelines beyond basic func-\n",
      "tional performance—ensuring outputs are constructive (helpful), innocuous (harmless), \n",
      "and trustworthy (honest).\n",
      "  To explain outputs, we should invest in interpretability methods that explain how generative AI \n",
      "models arrive at their decisions.\n",
      "  A forward-thinking UX can foster a transparent relationship with users while reinforcing sensible \n",
      "use.\n",
      "  Mitigation techniques in-\n",
      "clude external retrieval and tool augmentation to provide pertinent context, as we discussed in \n",
      "Chapter 5, Building a Chatbot like ChatGPT, and Chapter 6, Developing Software with Generative AI, \n",
      "in particular.\n",
      "Subtopics:\n",
      "  For security, we can strengthen role-based access policies, employ stringent data encryption \n",
      "standards, adopt anonymization best practices where feasible, and ensure continuous verification \n",
      "through compliance audits.\n",
      "  Security is a huge topic in the context of LLMs, however, we’ll focus \n",
      "on evaluation, observability, and deployment in this chapter.\n",
      "\n",
      "  Visualizing attention mechanisms or analyzing feature impor-\n",
      "tance can peel back the layers of complexity, which is particularly crucial for high-stakes industries \n",
      "such as healthcare or finance.\n",
      "\n",
      "  There also needs to \n",
      "be a robust framework that includes ethical guidelines, safety protocols, and review processes \n",
      "to prevent the generation and dissemination of harmful content.\n",
      "  This can include anticipating inaccuracies, such as disclaimers on limitations, attributions, \n",
      "and collecting rich user feedback.\n",
      "\n",
      "  This results in stability and societal acceptance.\n",
      "\n",
      "  There is a danger of models recalling private information, and ongoing advances \n",
      "in methods spanning data filtering, architecture adjustments, and inference techniques show \n",
      "promise in mitigating these problems.\n",
      "\n",
      "  Essential recommendations for deploying LLM apps encompass an array of practices aimed at \n",
      "mitigating technical challenges, improving performance, and ensuring ethical integrity.\n",
      "  Metrics should be task-specific to gauge the model’s proficiency accurately.\n",
      "----------------------------------------\n",
      "Page 283:\n",
      "Topics:\n",
      "  While \n",
      "it may not be drastically different from the concept of MLOps, the distinction lies in the specific \n",
      "requirements connected to handling, refining, and deploying massive language models such as \n",
      "GPT-3, which houses 175 billion parameters.\n",
      "\n",
      "  It combines the practices of DevOps with machine learning to \n",
      "transition algorithms from experimental systems to production systems.\n",
      "  The term LMOps is more inclusive than LLMOps as it encompasses various types of language \n",
      "models, including both LLMs and smaller generative models.\n",
      "  Techniques such as model quantization, discussed in Chapter 8, Customizing LLMs \n",
      "and Their Output, or model distillation can also help reduce the resource footprint of the model. \n",
      "\n",
      "  Generative AI in Production\n",
      "260\n",
      "We need to optimize infrastructure and resource usage by employing distributed techniques \n",
      "such as data parallelism or model parallelism to facilitate workload distribution across multiple \n",
      "processing units.\n",
      "  LLMOps is a specialized sub-category of MLOps.\n",
      "  With insightful planning and preparation, generative AI promises to transform industries from \n",
      "creative writing to customer service.\n",
      "  MLOps aims to increase \n",
      "automation, improve the quality of production models, and address business and regulatory \n",
      "requirements.\n",
      "\n",
      "  Let’s start by intro-\n",
      "ducing MLOps and similar terms, and define what they mean and imply.\n",
      "\n",
      "  Terminology\n",
      "MLOps is a paradigm that focuses on deploying and maintaining machine learning models in \n",
      "production reliably and efficiently.\n",
      "Subtopics:\n",
      "  But thoughtfully navigating the complexities of these systems \n",
      "remains critical as they continue to permeate increasingly diverse domains.\n",
      "  This term acknowledges the ex-\n",
      "panding landscape of language models and their relevance in operational contexts.\n",
      "\n",
      "  Further, storing model outputs can reduce latency and costs for repeated queries.\n",
      "\n",
      "  This chapter aims to \n",
      "provide a practical guide to some of the pieces that we haven’t covered in this book so far, aiming \n",
      "to help you build impactful and responsible generative AI applications.\n",
      "  It refers to the operational capabilities and \n",
      "infrastructure necessary for fine-tuning and operationalizing LLMs as part of a product.\n",
      "  We can employ techniques such as model compression or other computer ar-\n",
      "chitectural optimizations for more efficient deployment regarding inference speed and latency \n",
      "management.\n",
      "  Before we continue our discussion, a few words on terminology are in order.\n",
      "  We cover strategies for \n",
      "data curation, model development, infrastructure, monitoring, and transparency.\n",
      "\n",
      "----------------------------------------\n",
      "Page 284:\n",
      "Topics:\n",
      "  MLOps is widely used and often encompasses the \n",
      "many more specialized terms we just covered.\n",
      "  Chapter 9\n",
      "261\n",
      "Foundational Model Orchestration (FOMO) specifically addresses the challenges faced when \n",
      "working with foundation models, that is, models trained on broad data that can be adapted to \n",
      "a wide range of downstream tasks.\n",
      "  Finally, AgentOps explicitly highlights the interactive nature of agents \n",
      "consisting of generative models operating with certain heuristics and includes tools.\n",
      "\n",
      "  The term ModelOps focuses on the governance and lifecycle management of AI and decision \n",
      "models as they are deployed.\n",
      "  While FOMO emphasizes the unique challenges of working specifically with foundational mod-\n",
      "els, LMOps provides a more inclusive and comprehensive coverage of a wider range of language \n",
      "models beyond just the foundational ones.\n",
      "  Even more broadly, AgentOps involves the operational management \n",
      "of LLMs and other AI agents, ensuring their appropriate behavior, managing their environment \n",
      "and resource access, and facilitating interactions between agents while addressing concerns \n",
      "related to unintended outcomes and incompatible objectives.\n",
      "\n",
      "  Therefore, we’ll stick to MLOps for the remainder \n",
      "of this chapter.\n",
      "\n",
      "  We will focus on the evaluation methods provided by LangChain.\n",
      "\n",
      "  Evaluating LLMs either as standalone entities or in conjunction with an \n",
      "agent chain is crucial to ensure they function correctly and produce reliable results, and this is \n",
      "an integral part of the ML lifecycle.\n",
      "  Before productionizing any LLM app, we should first evaluate its output, so we should start with \n",
      "this.\n",
      "  LMOps acknowledges the versatility and increasing \n",
      "importance of language models in various operational use cases, while still falling under the \n",
      "broader umbrella of MLOps.\n",
      "Subtopics:\n",
      "  How to evaluate LLM apps\n",
      "The crux of LLM deployment lies in the meticulous curation of training data to preempt biases, \n",
      "implementing human-led annotation for data enhancement, and establishing automated output \n",
      "monitoring systems.\n",
      "  The evaluation process determines the performance of the \n",
      "models in terms of effectiveness, reliability, and efficiency.\n",
      "\n",
      "  The emergence of all of these very specialized terms underscores the rapid evolution of the field; \n",
      "however, their long-term prevalence is unclear.\n",
      "  It highlights the need for managing multi-step processes, \n",
      "integrating with external resources, and coordinating the workflows involving these models.\n",
      "\n",
      "----------------------------------------\n",
      "Page 285:\n",
      "Topics:\n",
      "  Generative AI in Production\n",
      "262\n",
      "The goal of evaluating LLMs is to understand their strengths and weaknesses, enhancing accuracy \n",
      "and efficiency while reducing errors, thereby maximizing their usefulness in solving real-world \n",
      "problems.\n",
      "  This chart compares several open and closed source models \n",
      "on the FLASK benchmark (source: “FLASK: Fine-grained Language Model Evaluation based on Align-\n",
      "ment Skill Sets” by Ye and colleagues, 2023; https://arxiv.org/abs/2307.10928):\n",
      "\n",
      "  Evaluations can provide insights into how well an LLM generates outputs that are relevant, accu-\n",
      "rate, and helpful.\n",
      "  TruthfulQA brings a \n",
      "unique angle by benchmarking the truthfulness of LLM responses, foregrounding the significance \n",
      "of veracity.\n",
      "\n",
      "  Tests such as FLAN and FLASK stress behavioral dimensions, thus prioritizing \n",
      "responsible AI systems deployment.\n",
      "  ARC puts models’ question-answering abilities up against complex \n",
      "integrations of information, whereas HellaSwag assesses common-sense reasoning in physical \n",
      "situations using adversarial filtering.\n",
      "\n",
      "  SuperGLUE takes GLUE a step further with more challeng-\n",
      "ing tasks to monitor the fairness and comprehension of language models.\n",
      "  There are \n",
      "many standardized benchmarks such as MBPP to test basic programming skills, while GSM8K is \n",
      "utilized for multi-step mathematical reasoning.\n",
      "  Benchmarks such as MATH demand high-level reasoning capability evaluations.\n",
      "  HumanEval focuses on code generation’s functional correctness over syntactic similarity.\n",
      "  They offer a necessary first step toward refining a model before deployment.\n",
      "\n",
      "  API-Bank evaluates models’ aptitudes for making \n",
      "decisions about API calls.\n",
      "  GPT-4’s perfor-\n",
      "mance on this benchmark varies with prompting method sophistication, from few-shot prompts \n",
      "to reinforcement learning with reward modeling approaches.\n",
      "Subtopics:\n",
      "  Offline \n",
      "evaluations provide initial insights into model performance under controlled test conditions and \n",
      "include aspects such as hyperparameter tuning and benchmarking against peer models or es-\n",
      "tablished standards.\n",
      "  MMLU \n",
      "assesses language understanding across a wide range of subjects at varying depths, indicating \n",
      "proficiency in specialized domains.\n",
      "  While human assessments are sometimes seen as the gold standard, they are hard to scale and \n",
      "require careful design to avoid bias from subjective preferences or authoritative tones.\n",
      "  Notably, dialog-based fine-tuning \n",
      "can sometimes detract from capabilities assessed by measures such as MMLU.\n",
      "\n",
      "  This evaluation process typically occurs offline during the development phase.\n",
      "----------------------------------------\n",
      "Page 286:\n",
      "Topics:\n",
      "  Often, GPT-3.5 or GPT-4 are used as evaluators, which \n",
      "shows the OpenAI models emerging as winners.\n",
      "\n",
      "  In the result reported in the chart, Claude is the LLM evaluating all outputs.\n",
      "  This skews results in \n",
      "favor of Claude and models similar to it.\n",
      "  In LangChain, there are various ways to evaluate the outputs of LLMs, including comparing chain \n",
      "outputs, pairwise string comparisons, string distances, and embedding distances.\n",
      "  Chapter 9\n",
      "263\n",
      "Figure 9.1: Result of an evaluation with Claude as an evaluating language model\n",
      "\n",
      "  LangChain provides several tools for evaluating the outputs of LLMs.\n",
      "Subtopics:\n",
      "  This \n",
      "prompts an evaluator model to choose between two model outputs for the same input and ag-\n",
      "gregates the results to determine an overall preferred model.\n",
      "\n",
      "  The evaluation \n",
      "results can be used to determine the preferred model based on the comparison of outputs.\n",
      "  A common approach is \n",
      "to compare the outputs of different models or prompts using PairwiseStringEvaluator.\n",
      "  Confi-\n",
      "dence intervals and p-values can also be calculated to assess the reliability of the evaluation results.\n",
      "\n",
      "----------------------------------------\n",
      "Page 287:\n",
      "Topics:\n",
      "  Comparing two outputs\n",
      "This evaluation requires an evaluator, a dataset of inputs, and two or more LLMs, chains, or agents \n",
      "to compare.\n",
      "  1.\t\n",
      "Create the evaluator: Load the evaluator using the load_evaluator() function, specifying \n",
      "the type of evaluator (in this case, pairwise_string).\n",
      "\n",
      "  You can follow the code in this section online under the monitoring_and_evaluation fold-\n",
      "er in the book’s GitHub project.\n",
      "  4.\t\n",
      "Generate responses: Generate outputs for each of the models before evaluating them. \n",
      "\n",
      "  Here’s an example from the documentation for pairwise string comparisons:\n",
      "from langchain.evaluation import load_evaluator\n",
      "evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
      "evaluator.evaluate_string_pairs(\n",
      "    prediction=\"there are three dogs\",\n",
      "    prediction_b=\"4\",\n",
      "    input=\"how many dogs are in the park?\",\n",
      "    reference=\"four\",\n",
      ")\n",
      "\n",
      "  Generative AI in Production\n",
      "264\n",
      "Other evaluators allow assessing model outputs based on specific criteria such as correctness, \n",
      "relevance, and conciseness.\n",
      "  2.\t\n",
      "Select the dataset: Load a dataset of inputs using the load_dataset() function.\n",
      "\n",
      "  Let’s compare outputs of different prompts or LLMs with the \n",
      "PairwiseStringEvaluator, which prompts an LLM to select the preferred output given a specific \n",
      "input.\n",
      "\n",
      "  3.\t\n",
      "Define models to compare: Initialize the LLMs, chains, or agents to compare using the \n",
      "necessary configurations.\n",
      "  5.\t\n",
      "Evaluate pairs: Evaluate the results by comparing the outputs of different models for \n",
      "each input.\n",
      "Subtopics:\n",
      "  The evaluation process involves several steps:\n",
      "\n",
      "  This is often done using a random selection order to reduce positional bias.\n",
      "\n",
      "  This is typically done in batches to improve efficiency.\n",
      "\n",
      "  The CriteriaEvalChain can score outputs on custom or predefined \n",
      "principles without needing reference labels.\n",
      "  Configuring the evaluation model is also possible \n",
      "by specifying a different chat model such as ChatGPT as the evaluator.\n",
      "\n",
      "  This involves initializing the language model and any additional \n",
      "tools or agents required.\n",
      "\n",
      "  The evaluation aggregates the results to determine the preferred model.\n",
      "\n",
      "----------------------------------------\n",
      "Page 288:\n",
      "Topics:\n",
      "  The evaluators can be loaded with a custom LLM by \n",
      "passing the LLM object as a parameter to the load_evaluator() function.\n",
      "\n",
      "  Comparing against criteria\n",
      "LangChain provides several predefined evaluators for different evaluation criteria.\n",
      "  \\n\\nFinal Verdict: \n",
      "\n",
      "  [[B]]\",\n",
      "'value': 'B',\n",
      "'score': 0\n",
      "}\n",
      "The evaluation result includes a score between 0 and 1, indicating the effectiveness of each agent, \n",
      "sometimes along with reasoning that outlines the evaluation process and justifies the score.\n",
      "  Therefore, considering the criteria of correctness \n",
      "and accuracy, Assistant B's response is superior.\n",
      "  The evaluation LLM used in LangChain, by default, is GPT-4.\n",
      "  However, Assistant A's response is incorrect as it stated there \n",
      "are three dogs in the park, while the reference answer indicates there are \n",
      "four.\n",
      "  However, you can configure the \n",
      "evaluation LLM by specifying other chat models, such as ChatAnthropic or ChatOpenAI, with the \n",
      "desired settings (for example, temperature).\n",
      "  Without reference \n",
      "labels, the evaluator relies on the LLM’s predicted answer and scores it based on the specified \n",
      "criteria.\n",
      "  We could remove the reference and let an LLM judge the outputs instead.\n",
      "\n",
      "  It provides a way to verify whether an LLM or chain’s output complies with a defined set of cri-\n",
      "teria.\n",
      "  Chapter 9\n",
      "265\n",
      "The output from the evaluator should look as follows:\n",
      "     {'reasoning': \"Both assistants provided a direct answer to the user's \n",
      "question.\n",
      "Subtopics:\n",
      "  With reference labels, the evaluator compares the predicted answer to the reference label \n",
      "and determines its compliance with the criteria.\n",
      "\n",
      "  These eval-\n",
      "uators can be used to assess outputs based on specific rubrics or criteria sets.\n",
      "  On the other hand, Assistant B's response is accurate and matches \n",
      "the reference answer.\n",
      "  You can use this evaluator to assess correctness, relevance, conciseness, and other aspects \n",
      "of the generated outputs.\n",
      "\n",
      "  CriteriaEvalChain can be configured to work with or without reference labels.\n",
      "  Some common \n",
      "criteria include conciseness, relevance, correctness, coherence, helpfulness, and controversiality.\n",
      "\n",
      "  CriteriaEvalChain allows you to evaluate model outputs against custom or predefined criteria. \n",
      "\n",
      "  In \n",
      "this specific example against the reference, both results are factually incorrect based on the input. \n",
      "\n",
      "----------------------------------------\n",
      "Page 289:\n",
      "Topics:\n",
      "  custom_criteria = {\n",
      "    \"simplicity\": \"Is the language straightforward and unpretentious?\",\n",
      "    \"clarity\": \"Are the sentences clear and easy to understand?\",\n",
      "    \"precision\": \"Is the writing precise, with no unnecessary words or \n",
      "details?\",\n",
      "    \"truthfulness\": \"Does the writing feel honest and sincere?\",\n",
      "    \"subtext\": \"Does the writing suggest deeper meanings or themes?\",\n",
      "}\n",
      "evaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria)\n",
      "evaluator.evaluate_string_pairs(\n",
      "    prediction=\"Every cheerful household shares a similar rhythm of joy; \n",
      "but sorrow, in each household, plays a unique, haunting melody.\n",
      "  Generative AI in Production\n",
      "266\n",
      "LangChain supports both custom criteria and predefined principles for evaluation.\n",
      "  \",\n",
      ")\n",
      "We can get a very nuanced comparison of the two outputs, as this result shows:\n",
      "{'reasoning': 'Response A is simple, clear, and precise.\n",
      "  The message is similar to that of Response A, but it is less effectively \n",
      "conveyed due to the unnecessary complexity of the language.\\n\\nTherefore, \n",
      "based on the criteria of simplicity, clarity, precision, truthfulness, \n",
      "\n",
      "  The language is more \n",
      "complex and pretentious, with phrases like \"domicile of happiness\" and \n",
      "\"abode of despair\" instead of the simpler \"household\" used in Response A. \n",
      "\n",
      "Subtopics:\n",
      "  It uses \n",
      "straightforward language to convey a deep and sincere message about \n",
      "families.\n",
      "  Custom criteria \n",
      "can be defined using a dictionary of criterion_name: criterion_description pairs.\n",
      "  Here’s a simple example:\n",
      "\n",
      "  These \n",
      "criteria can be used to assess outputs based on specific requirements or rubrics.\n",
      "\n",
      "  \",\n",
      "    prediction_b=\"Where one finds a symphony of joy, every domicile of \n",
      "happiness resounds in harmonious,\"\n",
      "    \" identical notes; yet, every abode of despair conducts a dissonant \n",
      "orchestra, each\"\n",
      "    \" playing an elegy of grief that is peculiar and profound to its own \n",
      "existence.\n",
      "  The metaphor of music is used effectively to suggest deeper \n",
      "meanings about the shared joys and unique sorrows of families.\\n\\nResponse \n",
      "B, on the other hand, is less simple and clear.\n",
      "  \",\n",
      "    input=\"Write some prose about families.\n",
      "----------------------------------------\n",
      "Page 290:\n",
      "Topics:\n",
      "  String \n",
      "distance metrics such as Levenshtein and Jaro provide a quantitative measure of similarity between \n",
      "predicted and reference strings.\n",
      "  Chapter 9\n",
      "267\n",
      "and subtext, Response A is the better response.\\n\\n[[A]]', 'value': 'A', \n",
      "'score': 1}\n",
      "Alternatively, you can use the predefined principles available in LangChain, such as those from \n",
      "Constitutional AI.\n",
      "  String and semantic comparisons\n",
      "LangChain supports string comparison and distance metrics for evaluating LLM outputs.\n",
      "  String distance evaluators use distance metrics, such as the Levenshtein or Jaro distance, to \n",
      "measure the similarity or dissimilarity between predicted and reference strings.\n",
      "  This measures the semantic similarity between the two strings and can provide insights \n",
      "into the quality of the generated text.\n",
      "\n",
      "  Embedding distance evaluators can use embedding models, such as those based on GPT-4 or \n",
      "Hugging Face embeddings, to compute vector distances between the predicted and reference \n",
      "strings.\n",
      "  Embedding distances using models such as SentenceTransformers \n",
      "calculates semantic similarity between the generated and expected texts.\n",
      "\n",
      "  The evaluator returns the score 0.0966466944859925.\n",
      "Subtopics:\n",
      "  The use of principles in evaluation allows for a more focused assessment \n",
      "of the generated text.\n",
      "\n",
      "  These principles are designed to evaluate the ethical, harmful, and sensitive \n",
      "aspects of the outputs.\n",
      "  This provides a \n",
      "quantitative measure of how similar the predicted string is to the reference string.\n",
      "\n",
      "  Here’s a quick example from the documentation:\n",
      "from langchain.evaluation import load_evaluator\n",
      "evaluator = load_evaluator(\"embedding_distance\")\n",
      "evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan't \n",
      "go\")\n",
      "\n",
      "  You can change the embeddings used with \n",
      "the embeddings parameter in the load_evaluator() call.\n",
      "\n",
      "  String comparison evaluators compare \n",
      "predicted strings against reference strings or inputs.\n",
      "\n",
      "  Finally, there’s an agent trajectory evaluator, where the evaluate_agent_trajectory() method \n",
      "is used to evaluate the input, prediction, and agent trajectory.\n",
      "\n",
      "  This often gives better results than older string distance metrics, but these are also available and \n",
      "allow for simple unit testing and assessment of accuracy.\n",
      "----------------------------------------\n",
      "Page 291:\n",
      "Topics:\n",
      "  Generative AI in Production\n",
      "268\n",
      "We can also use LangSmith, a companion project for LangChain that aims to facilitate the passage \n",
      "of LLM apps from prototype to production, to compare our performance against a dataset.\n",
      "  Let’s log a run!\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "llm = ChatOpenAI()\n",
      "llm.predict(\"Hello, world!\")\n",
      "\n",
      "  LangSmith lists all runs so far on the LangSmith project \n",
      "page: https://smith.langchain.com/projects\n",
      "We can also find all runs via the LangSmith API:\n",
      "from langsmith import Client\n",
      "client = Client()\n",
      "runs = client.list_runs()\n",
      "print(runs)\n",
      "\n",
      "  We can run evaluations against benchmark datasets \n",
      "in LangSmith as we’ll see now.\n",
      "  First, please make sure you create an account on LangSmith here: \n",
      "https://smith.langchain.com/.\n",
      "You can obtain an API key and set it as LANGCHAIN_API_KEY in your environment.\n",
      "  If we don’t tell LangChain the project ID, it will log \n",
      "against the default project.\n",
      "  Each run comes with \n",
      "inputs and outputs, as runs[0].inputs and runs[0].outputs, respectively.\n",
      "\n",
      "  After this setup, when we run our LangChain agent or chain, we’ll \n",
      "be able to see the traces on LangSmith.\n",
      "\n",
      "  We can find all these runs on LangSmith.\n",
      "  We can also set \n",
      "environment variables for project ID and tracing:\n",
      "import os\n",
      "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
      "os.environ[\"LANGCHAIN_PROJECT\"] = \"My Project\"\n",
      "This configures LangChain to log traces.\n",
      "Subtopics:\n",
      "  Running evaluations against datasets\n",
      "As we’ve mentioned, comprehensive benchmarking and evaluation, including testing, are critical \n",
      "for safety, robustness, and intended behavior.\n",
      "  Let’s \n",
      "step through an example!\n",
      "\n",
      "  We can list runs from a specific project or by run_type, for example, chain.\n",
      "----------------------------------------\n",
      "Page 292:\n",
      "Topics:\n",
      "  # The Ship of Theseus \n",
      "Paradox\n",
      "    \"If someone lived their whole life chained in a cave seeing only \n",
      "shadows, how would they react if freed and shown the real world?\n",
      "  Why?\",  # False Dilemma\n",
      "    \"Do people tend to develop a preference for things simply because they \n",
      "are familiar with them?\n",
      "  What is identity based on?\",  # Theseus' Paradox\n",
      "    \"Does doing one thing really mean that a chain of increasingly \n",
      "negative events will follow?\n",
      "  \",  # Appeal to Ignorance\n",
      "]\n",
      "shared_dataset_name = \"Reasoning and Bias\"\n",
      "ds = client.create_dataset(\n",
      "    dataset_name=shared_dataset_name, description=\"A few reasoning and \n",
      "cognitive bias questions\",\n",
      "\n",
      "  \",  # \n",
      "Gambler's Fallacy\n",
      "    \"Present two choices as the only options when others exist.\n",
      "  Does this impact reasoning?\",  # Mere Exposure \n",
      "Effect\n",
      "    \"Is it surprising that the universe is suitable for intelligent life \n",
      "since if it weren't, no one would be around to observe it?\",  # Anthropic \n",
      "Principle\n",
      "    \"If Theseus' ship is restored by replacing each plank, is it still the \n",
      "same ship?\n",
      "  Is the \n",
      "statement \\\"You're either with us or against us\\\" an example of false \n",
      "dilemma?\n",
      "  \",  # \n",
      "Plato's Allegory of the Cave\n",
      "    \"Is something good because it is natural, or bad because it is \n",
      "unnatural?\n",
      "  Chapter 9\n",
      "269\n",
      "We can create a dataset from existing agent runs with the create_example_from_run() function \n",
      "– or from anything else.\n",
      "  \",  # Appeal to Nature \n",
      "Fallacy\n",
      "    \"If a coin is flipped 8 times and lands on heads each time, what \n",
      "are the odds it will be tails next flip?\n",
      "Subtopics:\n",
      "  Why or why not?\",  \n",
      "  Why could this \n",
      "impede reasoning?\n",
      "  Is it still the same ship?\n",
      "  Why is this a problematic argument?\",  # \n",
      "Slippery Slope Fallacy\n",
      "    \"Is a claim true because it hasn't been proven false?\n",
      "  Explain your reasoning.\n",
      "  [\n",
      "    \"A ship's parts are replaced over time until no original parts remain. \n",
      "\n",
      "  Why can this be a faulty argument?\n",
      "  Here’s how to create a dataset with a set of questions:\n",
      "questions =\n",
      "----------------------------------------\n",
      "Page 293:\n",
      "Topics:\n",
      "  Generative AI in Production\n",
      "270\n",
      ")\n",
      "for q in questions:\n",
      "    client.create_example(inputs={\"input\": q}, dataset_id=ds.id)\n",
      "\n",
      "  We can then define an LLM agent or chain on the dataset like this:\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.chains import LLMChain\n",
      "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n",
      "def construct_chain():\n",
      "    return LLMChain.from_string(\n",
      "        llm,\n",
      "        template=\"Help out as best you can.\\nQuestion: {input}\\nResponse: \n",
      "\",\n",
      "    )\n",
      "\n",
      "  To run an evaluation on a dataset, we can either specify an LLM or – for parallelism – use a \n",
      "constructor function to initialize the model or LLM app for each input.\n",
      "Subtopics:\n",
      "  Criteria({\"helpfulness\": \"Is the response \n",
      "helpful?\"}),\n",
      "        RunEvalConfig.Criteria({\"insightful\": \"Is the response carefully \n",
      "thought out?\"})\n",
      "    ]\n",
      ")\n",
      "\n",
      "  We’ll pass a dataset together with the evaluation configuration with evaluators to run_on_\n",
      "dataset() to generate metrics and feedback:\n",
      "from langchain.smith import run_on_dataset\n",
      "results = run_on_dataset(\n",
      "  client=client,\n",
      "  dataset_name=shared_dataset_name,\n",
      "  dataset=dataset,\n",
      "\n",
      "  Now, to evaluate the per-\n",
      "formance against our dataset, we need to define an evaluator as we saw in the previous section:\n",
      "from langchain.smith import RunEvalConfig\n",
      "evaluation_config = RunEvalConfig(\n",
      "    evaluators=[\n",
      "        RunEvalConfig.\n",
      "  As seen, the criteria are defined by a dictionary that includes a criterion as a key and a question \n",
      "to check for as the value.\n",
      "\n",
      "----------------------------------------\n",
      "Page 294:\n",
      "Topics:\n",
      "  Chapter 9\n",
      "271\n",
      "  llm_or_chain_factory=construct_chain,\n",
      "  evaluation=evaluation_config\n",
      ")\n",
      "\n",
      "  We can view the evaluator feedback in the LangSmith UI to identify areas for improvement:\n",
      "Figure 9.2: Evaluators in LangSmith\n",
      "We can click on any of these evaluations to see some detail, for example, for the careful thinking \n",
      "evaluator, we get this prompt that includes the original answer from the LLM:\n",
      "You are assessing a submitted answer on a given task or input based on a \n",
      "set of criteria.\n",
      "Subtopics:\n",
      "  [BEGIN DATA]\n",
      "***\n",
      "\n",
      "  [Input]: Is something good because it is natural, or bad because it is \n",
      "unnatural?\n",
      "  Here is the data:\n",
      "\n",
      "  Similarly, we could pass a dataset and evaluators to run_on_dataset() to generate metrics and \n",
      "feedback asynchronously.\n",
      "\n",
      "  This argument is faulty because it assumes that what is \n",
      "natural is automatically good or beneficial, and what is unnatural is \n",
      "automatically bad or harmful.\n",
      "  ***\n",
      "[Submission]: The argument that something is good because it is natural, \n",
      "or bad because it is unnatural, is often referred to as the \"appeal to \n",
      "nature\" fallacy.\n",
      "  However, this is not always the case.\n",
      "  Why can this be a faulty argument?\n",
      "\n",
      "  For \n",
      "\n",
      "----------------------------------------\n",
      "Page 295:\n",
      "Topics:\n",
      "  ***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria?\n",
      "  You can find more examples of this in the LangSmith documentation.\n",
      "\n",
      "  First, write out in a step by step \n",
      "manner your reasoning about each criterion to be sure that your conclusion \n",
      "is correct.\n",
      "  We haven’t discussed data annotation queues, a new feature in LangSmith that addresses a \n",
      "critical gap that emerges after prototyping.\n",
      "  Then \n",
      "print only the single character \"Y\" or \"N\" (without quotes or punctuation) \n",
      "on its own line corresponding to the correct answer of whether the \n",
      "submission meets all criteria.\n",
      "  LangSmith \n",
      "can help us with this as well.\n",
      "  Generative AI in Production\n",
      "272\n",
      "example, many natural substances can be harmful or deadly, such as certain \n",
      "plants or animals.\n",
      "Subtopics:\n",
      "  The response \n",
      "is not just a simple yes or no, but a detailed explanation that shows \n",
      "careful consideration of the question.\n",
      "\n",
      "  At the end, repeat just the letter again by \n",
      "itself on a new line.\n",
      "\n",
      "  A way to improve performance for a few types of problems is to use few-shot prompting.\n",
      "  Each log can be filtered by attributes such as errors \n",
      "to focus on problematic cases, or manually reviewed and annotated with labels or feedback and \n",
      "edited as needed.\n",
      "  Avoid simply stating the correct answers at the outset.\n",
      "  We get this evaluation:\n",
      "\n",
      "  Conversely, many unnatural things, such as modern \n",
      "medicine or technology, can greatly benefit our lives.\n",
      "  The submission provides a clear and concise explanation of the \"appeal to \n",
      "nature\" fallacy, demonstrating an understanding of the concept.\n",
      "  ***\n",
      "[Criteria]: insightful: Is the response carefully thought out?\n",
      "\n",
      "  Edited logs can be added to a dataset for uses including fine-tuning the model.\n",
      "\n",
      "  The criterion is whether the response is insightful and carefully thought \n",
      "out.\n",
      "\n",
      "  Therefore, the submission does meet the criterion of being insightful and \n",
      "carefully thought out.\n",
      "\n",
      "  It also \n",
      "provides examples to illustrate why this argument can be faulty, showing \n",
      "that the respondent has thought about the question in depth.\n",
      "  Therefore, whether \n",
      "something is natural or unnatural is not a reliable indicator of its value \n",
      "or harm.\n",
      "\n",
      "----------------------------------------\n",
      "Page 296:\n",
      "Topics:\n",
      "  •\t\n",
      "Reasoning heuristics: Retrieval Augmented Generation (RAG), Tree-of-Thought, and \n",
      "others.\n",
      "\n",
      "  LLMs are typically utilized using external LLM providers or self-hosted models.\n",
      "  We discussed models in Chapter 1, What Is Generative AI?\n",
      "  •\t\n",
      "Vector databases: Aid in retrieving contextually relevant information for prompts.\n",
      "\n",
      "  and Chapter 3, Getting Started with Lang-\n",
      "Chain, reasoning heuristics in Chapter 4, Building Capable Assistants - Chapter 7, LLMs for Data Sci-\n",
      "ence, vector databases in Chapter 5, Building a Chatbot like ChatGPT, and prompts and fine-tuning \n",
      "in Chapter 8, Customizing LLMs and Their Output.\n",
      "  With external \n",
      "providers, computational burdens are shouldered by companies such as OpenAI or Anthropic, \n",
      "while LangChain facilitates business logic implementation.\n",
      "  How to deploy LLM apps\n",
      "Given the increasing use of LLMs in various sectors, it’s imperative to understand how to effectively \n",
      "deploy models and apps into production.\n",
      "  •\t\n",
      "Custom LLM stack: A set of tools for shaping and deploying solutions built on LLMs.\n",
      "\n",
      "  There are lots of different ways to productionize LLM apps or applications \n",
      "with generative AI.\n",
      "Deployment for production requires research into, and knowledge of, the generative AI ecosystem, \n",
      "which encompasses different aspects including:\n",
      "•\t\n",
      "Models and LLM-as-a-Service: LLMs and other models either run on-premises or offered \n",
      "as an API on vendor-provided infrastructure.\n",
      "\n",
      "  Chapter 9\n",
      "273\n",
      "This concludes the topic of evaluation here.\n",
      "Subtopics:\n",
      "  •\t\n",
      "Pre-training and fine-tuning: For models specialized for specific tasks or domains.\n",
      "\n",
      "  Deployment services and frameworks can help to scale \n",
      "the technical hurdles.\n",
      "  •\t\n",
      "Prompt logging, testing, and analytics: An emerging sector inspired by the desire to \n",
      "understand and improve the performance of LLMs.\n",
      "\n",
      "  Now that we’ve evaluated our agent, let’s say we are \n",
      "happy with the performance and have decided to deploy it!\n",
      "  However, self-hosting open-source \n",
      "LLMs can significantly decrease costs, latency, and privacy concerns.\n",
      "\n",
      "  •\t\n",
      "Prompt engineering tools: These facilitate in-context learning without requiring expen-\n",
      "sive fine-tuning or sensitive data.\n",
      "\n",
      "  In the present chapter, we’ll focus on logging, \n",
      "monitoring, and custom tools for deployment.\n",
      "\n",
      "  What should we do next?\n",
      "\n",
      "----------------------------------------\n",
      "Page 297:\n",
      "Topics:\n",
      "  Deployment \n",
      "is further facilitated through integration with platforms including GCP’s Cloud Run and Replit, \n",
      "allowing quick cloning from an existing GitHub repository.\n",
      "  The following table summarizes the services and frameworks available for deploying LLM ap-\n",
      "plications:\n",
      "Name\n",
      "Description\n",
      "Type\n",
      "Streamlit\n",
      "Open-source Python framework for building and deploying \n",
      "web apps\n",
      "Framework\n",
      "Gradio\n",
      "Lets you wrap models in an interface and host on Hugging \n",
      "Face\n",
      "Framework\n",
      "Chainlit\n",
      "Build and deploy conversational ChatGPT-like apps\n",
      "Framework\n",
      "Apache Beam\n",
      "Tool for defining and orchestrating data processing \n",
      "workflows\n",
      "Framework\n",
      "Vercel\n",
      "Platform for deploying and scaling web apps\n",
      "Cloud service\n",
      "FastAPI\n",
      "Python web framework for building APIs\n",
      "Framework\n",
      "Fly.io\n",
      "App hosting platform with autoscaling and global CDN\n",
      "Cloud service\n",
      "DigitalOcean \n",
      "App Platform\n",
      "Platform to build, deploy, and scale apps\n",
      "Cloud service\n",
      "Google Cloud\n",
      "Services such as Cloud Run to host and scale containerized \n",
      "apps\n",
      "Cloud service\n",
      "\n",
      "  BentoML is a framework that enables the containerization of machine learning \n",
      "applications to use them as microservices running and scaling independently with automatic \n",
      "generation of OpenAPI and gRPC endpoints.\n",
      "\n",
      "  For example, you can deploy LangChain \n",
      "agents with Chainlit, creating ChatGPT-like UIs with Chainlit.\n",
      "  LangChain AI, the company maintaining LangChain, is developing a new library called LangServe. \n",
      "\n",
      "  You can also deploy LangChain to different cloud service endpoints, for example, an Azure Machine \n",
      "Learning online endpoint.\n",
      "  Generative AI in Production\n",
      "274\n",
      "Some tools with infrastructure offer the full package.\n",
      "  With Steamship, LangChain developers can rapidly deploy their apps, \n",
      "with features including production-ready endpoints, horizontal scaling across dependencies, \n",
      "persistent storage of app state, and multi-tenancy support.\n",
      "\n",
      "  Built on top of FastAPI and Pydantic, it streamlines documentation and deployment.\n",
      "Subtopics:\n",
      "  Key features include intermediary \n",
      "step visualization, element management and display (images, text, carousel, and others), and \n",
      "cloud deployment.\n",
      "  Additional deployment instructions \n",
      "for other platforms will follow shortly based on user input.\n",
      "\n",
      "----------------------------------------\n",
      "Page 298:\n",
      "Topics:\n",
      "  There are a few main requirements for running LLM applications:\n",
      "•\t\n",
      "Scalable infrastructure to handle computationally intensive models and potential spikes \n",
      "in traffic\n",
      "•\t\n",
      "Low latency for real-time serving of model outputs\n",
      "•\t\n",
      "Persistent storage for managing long conversations and app state\n",
      "•\t\n",
      "APIs for integration into end-user applications\n",
      "•\t\n",
      "Monitoring and logging to track metrics and model behavior\n",
      "Maintaining cost efficiency can be challenging with large volumes of user interactions and the \n",
      "high costs associated with LLM services.\n",
      "  Chapter 9\n",
      "275\n",
      "Steamship\n",
      "ML infrastructure platform for deploying and scaling \n",
      "models\n",
      "Cloud service\n",
      "Langchain-Serve\n",
      "Tool to serve LangChain agents as web APIs\n",
      "Framework\n",
      "BentoML\n",
      "Framework for model serving, packaging, and deployment\n",
      "Framework\n",
      "OpenLLM\n",
      "Provides open APIs to commercial LLMs\n",
      "Cloud service\n",
      "Databutton\n",
      "No-code platform to build and deploy model workflows\n",
      "Framework\n",
      "Azure ML\n",
      "Managed MLOps service on Azure for models\n",
      "Cloud service\n",
      "LangServe\n",
      "Built on top of FastAPI, but specialized for LLM app \n",
      "deployment\n",
      "Framework\n",
      "Table 9.1: Services and frameworks for deploying LLM applications\n",
      "All of these are well documented with different use cases, often directly referencing LLMs.\n",
      "  It’s crucial to avoid getting tied to \n",
      "one solution.\n",
      "  Strategies to manage efficiency include self-hosting \n",
      "models, auto-scaling resource allocations based on traffic, using spot instances, independent \n",
      "scaling, and batching requests to better utilize GPU resources.\n",
      "\n",
      "  We’ve \n",
      "already shown examples with Streamlit and Gradio, and we’ve discussed how to deploy them to \n",
      "the Hugging Face Hub as an example.\n",
      "\n",
      "  Flexibility and ease is very important, because we want to be able to iterate rapidly, which is \n",
      "vital due to the dynamic nature of ML and LLM landscapes.\n",
      "Subtopics:\n",
      "  A flexible, scalable serving layer that accommodates various models is key.\n",
      "  The choice of tools and the infrastructure determines trade-offs between these requirements. \n",
      "\n",
      "  Model \n",
      "composition and cloud providers’ selection forms part of this flexibility equation.\n",
      "\n",
      "----------------------------------------\n",
      "Page 299:\n",
      "Topics:\n",
      "  As mentioned, LangChain plays nicely with several open-source projects and frameworks such \n",
      "as Ray Serve, BentoML, OpenLLM, Modal, and Jina.\n",
      "  Leveraging \n",
      "one of these solutions for deployment allows developers to focus on developing impactful AI \n",
      "applications rather than infrastructure.\n",
      "\n",
      "  For the greatest degree of flexibility, Infrastructure as Code (IaC) tools such as Terraform, Cloud-\n",
      "Formation, or Kubernetes YAML files can recreate your infrastructure reliably and quickly.\n",
      "  Generative AI in Production\n",
      "276\n",
      "\n",
      "  It follows the principles of REST, \n",
      "which is an architectural style for designing networked applications.\n",
      "  More-\n",
      "over, continuous integration and continuous delivery (CI/CD) pipelines can automate testing \n",
      "and deployment processes to reduce errors and facilitate quicker feedback and iteration.\n",
      "\n",
      "  In the library documentation, there are several examples, including a Retrieval QA with Sources \n",
      "Chain, a Conversational Retrieval app, and a Zero Shot agent.\n",
      "  Following another example, we’ll \n",
      "implement a chatbot web server with Lanarky.\n",
      "\n",
      "  A REST API \n",
      "uses HTTP methods (such as GET, POST, PUT, or DELETE) to perform operations \n",
      "on resources, and it typically sends and receives data in a standardized format, such \n",
      "as JSON or XML.\n",
      "\n",
      "  Lanarky is a small, open-source library for deploying LLM applications that provides convenient \n",
      "wrappers around Flask API as well as Gradio for the deployment of LLM applications.\n",
      "  The full code for this recipe \n",
      "is available here: https://github.com/benman1/generative_ai_with_langchain/tree/main/\n",
      "webserver\n",
      "A Representational State Transfer Application Programming Interface (REST \n",
      "API) is a set of rules and protocols that allows different software applications to \n",
      "communicate with each other over the internet.\n",
      "  We’ll set up a web server using Lanarky that creates a ConversationChain instance with an LLM \n",
      "model and settings, and defines routes for handling HTTP requests.\n",
      "Subtopics:\n",
      "  Designed to be fast, easy to \n",
      "use, and efficient, it is a modern, high-performance web framework for building APIs with Python. \n",
      "\n",
      "  Designing a robust LLM application service can be a complex task requiring an understanding \n",
      "of the trade-offs and critical considerations when evaluating serving frameworks.\n",
      "  In the next sections, we’ll deploy apps using \n",
      "different tools.\n",
      "  This means \n",
      "you can get a REST API endpoint as well as the in-browser visualization at once and you only \n",
      "need a few lines of code.\n",
      "\n",
      "  We’ll start with a chat service web server based on FastAPI.\n",
      "FastAPI web server\n",
      "FastAPI is a very popular choice for the deployment of web servers.\n",
      "----------------------------------------\n",
      "Page 300:\n",
      "Topics:\n",
      "  Chapter 9\n",
      "277\n",
      "First, we’ll import the necessary dependencies, including FastAPI for creating the web server and \n",
      "ConversationChain and ChatOpenAI from LangChain for handling LLM conversations, along \n",
      "with some other required modules:\n",
      "from fastapi import FastAPI\n",
      "from langchain import ConversationChain\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from lanarky import LangchainRouter\n",
      "from starlette.requests import Request\n",
      "from starlette.templating import Jinja2Templates\n",
      "Please note that you need to set your environment variables as explained in Chapter 3, Getting \n",
      "Started with LangChain.\n",
      "  We can create an instance of ConversationChain, specifying the LLM model and its settings:\n",
      "chain = ConversationChain(\n",
      "        llm=ChatOpenAI(\n",
      "            temperature=0,\n",
      "            streaming=True,\n",
      "        ),\n",
      "        verbose=True,\n",
      "    ) \n",
      "\n",
      "  The templates variable gets set to a Jinja2Templates class, specifying the directory where tem-\n",
      "plates are located for rendering.\n",
      "  Now we create a FastAPI app, which will take care of most of the routing, except for LangChain \n",
      "specific requests that Lanarky will cover as we’ll see later:\n",
      "app = FastAPI()\n",
      "\n",
      "Subtopics:\n",
      "  This specifies how the webpage will be shown, allowing all kinds \n",
      "of customization:\n",
      "templates = Jinja2Templates(directory=\"webserver/templates\")\n",
      "\n",
      "  We can do this by importing the setup_environment() method from the \n",
      "config module as we’ve seen in many other examples before: \n",
      "from config import set_environment\n",
      "set_environment()\n",
      "\n",
      "----------------------------------------\n",
      "Page 301:\n",
      "Topics:\n",
      "  We can add additional \n",
      "routes to the router for handling JSON-based chat that even work with WebSocket requests:\n",
      "langchain_router = LangchainRouter(\n",
      "    langchain_url=\"/chat\", langchain_object=chain, streaming_mode=1\n",
      ")\n",
      "langchain_router.add_langchain_api_route(\n",
      "    \"/chat_json\", langchain_object=chain, streaming_mode=2\n",
      ")\n",
      "langchain_router.add_langchain_api_websocket_route(\"/ws\", langchain_\n",
      "object=chain)\n",
      "app.include_router(langchain_router)\n",
      "\n",
      "  Uvicorn excels in supporting high-performance, asyn-\n",
      "chronous frameworks such as FastAPI and Starlette.\n",
      "  The function associated with this endpoint returns a template response for \n",
      "rendering the index.html template:\n",
      "@app.get(\"/\")\n",
      "async def get(request: Request):\n",
      "    return templates.TemplateResponse(\"index.html\n",
      "  We will use Uvicorn to run our application.\n",
      "  This object is responsible for defining and \n",
      "managing the routes associated with the ConversationChain instance.\n",
      "  Generative AI in Production\n",
      "278\n",
      "An endpoint for handling HTTP GET requests at the root path (/) is defined using the FastAPI \n",
      "decorator @app.get.\n",
      "Subtopics:\n",
      "  Now our application knows how to handle requests made to the specified routes defined within \n",
      "the router, directing them to the appropriate functions or handlers for processing.\n",
      "\n",
      "  It is known for its ability to handle a large \n",
      "number of concurrent connections and performs well under heavy loads due to its asynchronous \n",
      "nature.\n",
      "\n",
      "  \", {\"request\": request})\n",
      "\n",
      "  A router object is created as a LangChainRouter class.\n",
      "  We can run the web server from the terminal like this:\n",
      "uvicorn webserver.chat:app –reload\n",
      "This command starts a web server, which you can view in your browser, at this local address: \n",
      "http://127.0.0.1:8000\n",
      "The reload switch (--reload) is particularly handy, because it means the server will be automat-\n",
      "ically restarted once you’ve made any changes.\n",
      "\n",
      "----------------------------------------\n",
      "Page 302:\n",
      "Topics:\n",
      "  Finally, Lanarky also plays nicely with Gradio, so with a \n",
      "few extra lines we have this webserver running as a Gradio app up and running.\n",
      "\n",
      "  Chapter 9\n",
      "279\n",
      "Here’s a snapshot of the chatbot application we’ve just deployed:\n",
      "Figure 9.3: Chatbot in Flask/Lanarky\n",
      "I think this looks quite nice for what little work we’ve put in.\n",
      "  The use of Uvicorn with \n",
      "load balancers enables horizontal scaling to handle large traffic volumes, improves response times \n",
      "for clients, and enhances fault tolerance.\n",
      "  In the next section, we’ll see how to build robust and cost-effective generative AI applications \n",
      "with Ray.\n",
      "  We’ll build a simple search engine using LangChain for text processing and then use \n",
      "Ray for scaling indexing and serving.\n",
      "\n",
      "  While Uvicorn itself does not provide \n",
      "built-in load balancing functionality, it can work together with other tools or technologies such \n",
      "as Nginx or HAProxy to achieve load balancing in a deployment setup, which distributes the \n",
      "incoming client requests across multiple worker processes or instances.\n",
      "  It also comes with a few nice features \n",
      "such as a REST API, a web UI, and a WebSocket interface.\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 303:\n",
      "Topics:\n",
      "  This can be found here: https://www.anyscale.com/blog/llm-open-\n",
      "source-search-engine-langchain-ray\n",
      "You can see this as an extension of the recipe in Chapter 5, Building a Chatbot like ChatGPT.\n",
      "  Ray helps with com-\n",
      "mon deployment needs such as low-latency serving, distributed training, and large-scale batch \n",
      "inference.\n",
      "  Generative AI in Production\n",
      "280\n",
      "Ray\n",
      "Ray provides a flexible framework to meet the infrastructure challenges of complex neural net-\n",
      "works in production by scaling out generative AI workloads across clusters.\n",
      "  Its capabilities include:\n",
      "•\t\n",
      "Scheduling distributed training jobs across GPU clusters using Ray Train\n",
      "•\t\n",
      "Deploying pre-trained models at scale for low-latency serving with Ray Serve\n",
      "•\t\n",
      "Running large batch inference in parallel across CPUs and GPUs with Ray Data\n",
      "•\t\n",
      "Orchestrating end-to-end generative AI workflows combining training, deployment, and \n",
      "batch processing\n",
      "We’ll use LangChain and Ray to build a simple search engine for the Ray documentation following \n",
      "an example implemented by Waleed Kadous for the anyscale Blog and on the langchain-ray \n",
      "repository on GitHub.\n",
      "  First, we’ll ingest and index the Ray docs so we can quickly find relevant passages for a search \n",
      "query:\n",
      "# Load the Ray docs using the LangChain loader\n",
      "loader = RecursiveUrlLoader(\"docs.ray.io/en/master/\"\n",
      "  # Split docs into sentences using LangChain splitter\n",
      "chunks = text_splitter.create_documents(\n",
      "    [doc.page_content for doc in docs],\n",
      "    metadatas=[doc.metadata for doc in docs])\n",
      "# Embed sentences into vectors using transformers\n",
      "embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1') \n",
      "\n",
      "  Ray also makes it easy to spin up on-demand fine-tuning or scale existing workloads \n",
      "from one machine to many.\n",
      "Subtopics:\n",
      "  )\n",
      "docs = loader.load()\n",
      "\n",
      "  The full code for this recipe under semantic search \n",
      "is available here: https://github.com/benman1/generative_ai_with_langchain/tree/main/\n",
      "search_engine.\n",
      "\n",
      "  You’ll \n",
      "also see how to run this as a FastAPI server.\n",
      "----------------------------------------\n",
      "Page 304:\n",
      "Topics:\n",
      "  [process_shard.remote(shard) for shard in shards]\n",
      "results = ray.get(futures)\n",
      "# Merge index shards\n",
      "db = results[0]\n",
      "for result in results[1:]:\n",
      "  db.merge_from(result)\n",
      "\n",
      "  # Split chunks into 8 shards\n",
      "shards = np.array_split(chunks, 8) \n",
      "# Process shards in parallel\n",
      "futures =\n",
      "  Chapter 9\n",
      "281\n",
      "# Index vectors using FAISS via LangChain\n",
      "db = FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "  Next, we’ll see how we can serve search queries with Ray Serve:\n",
      "# Load index and embedding\n",
      "db = FAISS.load_local(FAISS_INDEX_PATH)\n",
      "embedding = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n",
      "\n",
      "  Alternatively, we can accelerate the indexing by parallelizing \n",
      "the embedding step:\n",
      "# Define shard processing task\n",
      "@ray.remote(num_gpus=1) \n",
      "def process_shard(shard):\n",
      "  embeddings = LocalHuggingFaceEmbeddings('multi-qa-mpnet-base-dot-v1')\n",
      "  return FAISS.from_documents(shard, embeddings)\n",
      "\n",
      "Subtopics:\n",
      "  FAISS_INDEX_PATH is an arbitrary file name.\n",
      "  This builds our search index by ingesting the docs, splitting them into chunks, embedding the \n",
      "sentences, and indexing the vectors.\n",
      "  @serve.deployment\n",
      "\n",
      "  By running embedding on each shard in parallel, we can significantly reduce the indexing time.\n",
      "\n",
      "  We save the database index to disk:\n",
      "db.save_local(FAISS_INDEX_PATH)\n",
      "\n",
      "  I’ve set it to faiss_index.db.\n",
      "\n",
      "----------------------------------------\n",
      "Page 305:\n",
      "Topics:\n",
      "  Generative AI in Production\n",
      "282\n",
      "class SearchDeployment:\n",
      "  def __init__(self):\n",
      "    self.db = db\n",
      "    self.embedding = embedding\n",
      " \n",
      "  def __call__(self, request):  \n",
      "    query_embed = self.embedding(request.query_params[\"query\"])\n",
      "    results = self.db.max_marginal_relevance_search(query_embed)\n",
      "    return format_results(results)\n",
      "deployment = SearchDeployment.bind()\n",
      "# Start service\n",
      "serve.run(deployment)\n",
      "\n",
      "  We can query it from Python:\n",
      "import requests\n",
      "query = \"What are the different components of Ray\"\n",
      "         \" and how can they help with large language models (LLMs)?\n",
      "  If we save this to a file called serve_vector_store.py, we can get the server up and running using \n",
      "the following command from the search_engine directory:\n",
      "PYTHONPATH=../ python serve_vector_store.py\n",
      "Running this command in the terminal gives me this output:\n",
      "Started a local Ray instance.\n",
      "\n",
      "  View the dashboard at 127.0.0.1:8265\n",
      "\n",
      "  The search \n",
      "server, however, is running on localhost on port 8080.\n",
      "Subtopics:\n",
      "  \"\n",
      "response = requests.post(\"http://localhost:8000/\", params={\"query\": \n",
      "query})\n",
      "print(response.text)\n",
      "\n",
      "  This should load the index we generated and lets us serve search queries as a web endpoint!\n",
      "\n",
      "  The message shows us the URL of the dashboard, which we can access in the browser.\n",
      "----------------------------------------\n",
      "Page 306:\n",
      "Topics:\n",
      "  This practical guide has taken you through the key steps of deploying an LLM application locally \n",
      "using LangChain and Ray.\n",
      "  Chapter 9\n",
      "283\n",
      "For me, the server fetches the Ray use cases page at: https://docs.ray.io/en/latest/ray-\n",
      "overview/use-cases.html\n",
      "What I really liked was the monitoring with the Ray Dashboard, which looks like this:\n",
      "Figure 9.4: Ray Dashboard\n",
      "This dashboard is very powerful as it can give you a whole bunch of metrics and other information. \n",
      "\n",
      "  Collecting metrics is easy, since all you must do is set up and update variables of type Counter, \n",
      "Gauge, Histogram, and others within the deployment object or actor.\n",
      "  For time series charts, you \n",
      "should have either Prometheus or the Grafana server installed.\n",
      "\n",
      "  As you can see in the full implementation on GitHub, we can also spin this up as a FastAPI server. \n",
      "\n",
      "  This concludes our simple semantic search engine with LangChain and Ray.\n",
      "\n",
      "  We then served the search applica-\n",
      "tion via Ray Serve, which provides a flexible framework for low-latency querying.\n",
      "  By leveraging Ray’s distributed capabilities, we parallelized \n",
      "the intensive embedding task to accelerate the indexing time.\n",
      "  We first ingested and indexed documents to power a semantic search \n",
      "engine over the Ray documentation.\n",
      "Subtopics:\n",
      "  The Ray dash-\n",
      "board offered helpful monitoring insights into metrics such as request rates, latencies, and errors.\n",
      "\n",
      "----------------------------------------\n",
      "Page 307:\n",
      "Topics:\n",
      "  This might involve continuously collecting and analyzing metrics re-\n",
      "lated to system health such as memory usage, CPU utilization, network latency, \n",
      "and the overall application/service performance (such as response time).\n",
      "  Generative AI in Production\n",
      "284\n",
      "\n",
      "  Tracking, tracing, and monitoring are three important concepts in the field of soft-\n",
      "ware operation and management.\n",
      "  All three of these concepts fall within the \n",
      "category of observability.\n",
      "\n",
      "  As models and LLM apps grow more sophisticated and highly interwoven into the fabric of busi-\n",
      "ness applications, observability and monitoring during production become necessary to ensure \n",
      "their accuracy, efficiency, and reliability is ongoing.\n",
      "  How to observe LLM apps\n",
      "The dynamic nature of real-world operations means that the conditions assessed during offline \n",
      "evaluations hardly cover all potential scenarios that LLMs may encounter in production systems. \n",
      "\n",
      "Subtopics:\n",
      "  We need to implement monitoring tools to track vital metrics regularly.\n",
      "  Effective \n",
      "monitoring includes setting up alert systems for anomalies or unexpected behav-\n",
      "iors – sending notifications when certain thresholds are exceeded.\n",
      "  This includes user activ-\n",
      "ity, response times, traffic volumes, financial expenditures, model behavior patterns, and overall \n",
      "satisfaction with the app.\n",
      "  Ongoing surveillance allows for the early detection of anomalies such \n",
      "as data drift or unexpected lapses in capabilities.\n",
      "\n",
      "  The next section focuses on the significance \n",
      "of monitoring LLMs and highlights key metrics to track for a comprehensive monitoring strategy.\n",
      "\n",
      "  Monitoring is the ongoing process of overseeing the performance of a system or \n",
      "application.\n",
      "  Observability allows monitoring behaviors and outcomes as the model interacts with actual in-\n",
      "put data and users in production.\n",
      "  It includes logging, tracking, tracing, and alerting mechanisms \n",
      "to ensure healthy system functioning, performance optimization, and catching issues such as \n",
      "model drift early.\n",
      "\n",
      "  While tracking \n",
      "and tracing are about keeping detailed historical records for analysis and debugging, \n",
      "monitoring is aimed at real-time observation and immediate awareness of issues to \n",
      "ensure optimal system functionality at all times.\n",
      "\n",
      "  While all related to understanding and improving \n",
      "a system’s performance, they each have distinct roles.\n",
      "  While tracking and tracing \n",
      "are about keeping detailed historical records for analysis and debugging, monitor-\n",
      "ing is aimed at real-time observation and immediate awareness of issues to ensure \n",
      "optimal system functionality at all times.\n",
      "  Thus comes the need for observability in production – a more continuous, real-time observation \n",
      "to capture anomalies that offline tests could not anticipate.\n",
      "\n",
      "----------------------------------------\n",
      "Page 308:\n",
      "Topics:\n",
      "  By continuously evaluating the behavior and performance \n",
      "of LLM apps via validation, shadow launches, and interpretation along with dependable offline \n",
      "evaluation, organizations can identify and mitigate potential risks, maintain user trust, and \n",
      "provide an optimal experience.\n",
      "\n",
      "  •\t\n",
      "A/B testing: Helps compare how slight differences in models may result in different out-\n",
      "comes, which aids decision-making for model improvements.\n",
      "\n",
      "  Monitoring LLMs and LLM apps in production serves multiple purposes, including assessing mod-\n",
      "el performance, detecting abnormalities or issues, optimizing resource utilization, and ensuring \n",
      "consistent and high-quality outputs.\n",
      "  Chapter 9\n",
      "285\n",
      "The chief aim for monitoring and observability is to provide insights into LLM app performance \n",
      "and behavior through real-time data.\n",
      "  •\t\n",
      "Alerting mechanism: The system should raise alerts if it detects anomalous behavior or \n",
      "drastic performance drops.\n",
      "\n",
      "  •\t\n",
      "Ensuring appropriate behavior: Responses should be relevant, complete, helpful, harm-\n",
      "less, conform to the required format, and follow the user’s intent.\n",
      "\n",
      "  Some things you should consider when coming up with a strategy are:\n",
      "•\t\n",
      "Metrics to monitor: Define key metrics of interest such as prediction accuracy, latency, \n",
      "throughput, and others based on the desired model performance.\n",
      "\n",
      "  •\t\n",
      "Debugging issues: Monitoring helps identify unforeseen problems that can occur during \n",
      "runtime, enabling rapid resolution.\n",
      "\n",
      "  •\t\n",
      "Performance optimization: By tracking metrics such as inference times, resource usage, \n",
      "and throughput, you can make adjustments to improve the efficiency and effectiveness \n",
      "of LLM apps in production.\n",
      "\n",
      "  •\t\n",
      "Logging: Logs should provide comprehensive details regarding every relevant action \n",
      "performed by the LLM so analysts can track down any anomalies.\n",
      "\n",
      "Subtopics:\n",
      "  Regular monitoring can identify such sit-\n",
      "uations early and apply corrective measures.\n",
      "\n",
      "  •\t\n",
      "Monitoring frequency: Frequency should be determined based on how critical the model \n",
      "is to operations – a highly critical model may require near real-time monitoring.\n",
      "\n",
      "  •\t\n",
      "Avoiding hallucinations: We want to ensure the factual accuracy of the response, and – \n",
      "if we are using RAG – retrieved context quality, and sufficient effectiveness in using the \n",
      "context.\n",
      "\n",
      "  Since there are so many ways to monitor, it’s important to come up with a monitoring strategy. \n",
      "\n",
      "  This helps to do the following:\n",
      "•\t\n",
      "Preventing model drift: LLM performance can degrade over time due to changes in the \n",
      "characteristics of input data or user behavior.\n",
      "----------------------------------------\n",
      "Page 309:\n",
      "Topics:\n",
      "  •\t\n",
      "Token usage: The number of tokens correlates with the resource usage such as hardware \n",
      "utilization, latency, and costs.\n",
      "\n",
      "  •\t\n",
      "Query per Second (QPS): Calculates the number of queries or requests that the LLM can \n",
      "handle within a given time frame.\n",
      "  Beyond the crucial metrics of tonality, \n",
      "toxicity, and harmlessness, here is an expanded list that captures a wider range of evaluation areas:\n",
      "•\t\n",
      "Inference latency:\n",
      "  •\t\n",
      "Token per Second (TPS):\n",
      "  •\t\n",
      "Resource utilization: Measures the consumption of computational resources, such as \n",
      "the CPU, memory, and GPU, to reduce costs and avoid bottlenecks.\n",
      "\n",
      "  Monitoring QPS helps assess scalability and capacity \n",
      "planning.\n",
      "\n",
      "  •\t\n",
      "Model drift: Detects changes in LLM app behavior over time by comparing its outputs \n",
      "to a baseline or ground truth, ensuring the model remains accurate and aligned with \n",
      "expected outcomes.\n",
      "•\t\n",
      "Out-of-distribution inputs: Identifies inputs or queries falling outside the intended dis-\n",
      "tribution of the LLM’s training data, which can cause unexpected or unreliable responses.\n",
      "•\t\n",
      "User feedback metrics: Monitors user feedback channels to gather insights on user sat-\n",
      "isfaction, identify areas for improvement, and validate the effectiveness of the LLM app.\n",
      "\n",
      "  Generative AI in Production\n",
      "286\n",
      "\n",
      "  When monitoring LLMs and LLM applications, organizations can rely on a diverse set of metrics to \n",
      "gauge different aspects of performance and user experience.\n",
      "  •\t\n",
      "Error rate: Monitors the occurrence of errors or failures in LLM app responses, ensuring \n",
      "error rates are kept within acceptable limits to maintain the quality of outputs.\n",
      "\n",
      "  Measures the time it takes for the LLM app to process a request and \n",
      "generate a response.\n",
      "  Tracks the rate at which the LLM app generates tokens.\n",
      "  This list can easily be extended with many more metrics from Site \n",
      "Reliability Engineering (SRE) relating to task performance or the behavior of the LLM app.\n",
      "\n",
      "  Data scientists and machine learning engineers should check for staleness, incorrect learning, \n",
      "and bias using model interpretation tools such as LIME and SHAP.\n",
      "Subtopics:\n",
      "  TPS \n",
      "metrics are useful for estimating computational resource requirements and understanding \n",
      "model efficiency.\n",
      "\n",
      "  •\t\n",
      "User engagement: We can track how users engage with our app; for example, the frequency \n",
      "and duration of sessions or the usage of specific features.\n",
      "\n",
      "  Lower latency ensures a faster and more responsive user experience.\n",
      "\n",
      "  This is just a small selection.\n",
      "  The most predictive features \n",
      "changing suddenly could indicate a data leak.\n",
      "\n",
      "  •\t\n",
      "Tool/retrieval usage: Breakdown of the instances when retrieval and tools are used.\n",
      "\n",
      "----------------------------------------\n",
      "Page 310:\n",
      "Topics:\n",
      "  Chapter 9\n",
      "287\n",
      "Offline metrics such as AUC do not always correlate with online impacts on conversion rate, so \n",
      "it is important to find dependable offline metrics that translate to online gains relevant to the \n",
      "business, ideally direct metrics such as clicks and purchases that the system impacts directly.\n",
      "\n",
      "  The full code for the recipes in this section are available on GitHub in the monitoring_and_\n",
      "evaluation directory of the repository corresponding to this book.\n",
      "\n",
      "  Tracing is a more specialized form of tracking.\n",
      "  Tracking responses\n",
      "Tracking in this context refers to recording the full provenance of responses, including the tools, \n",
      "retrievals, the included data, and the LLM used in generating the output.\n",
      "Subtopics:\n",
      "  It should be cautioned, however, that you \n",
      "should study service providers’ privacy and data protection policies when relying on cloud service \n",
      "platforms.\n",
      "\n",
      "  We’ll use the terms tracking and tracing interchangeably in \n",
      "this section.\n",
      "\n",
      "  This is key for auditing \n",
      "and reproducibility of responses.\n",
      "  In the next section, we’ll start our journey into observability by monitoring the trajectory of an \n",
      "agent.\n",
      "\n",
      "  Particularly in distributed systems where a single \n",
      "transaction might span multiple services, tracing helps in maintaining an audit or \n",
      "breadcrumb trail, a detailed source of information about that request path through \n",
      "the system.\n",
      "  Tracking generally refers to the process of recording and managing information \n",
      "about a particular operation or series of operations within an application or system. \n",
      "\n",
      "  Effective monitoring enables the successful deployment and utilization of LLMs, boosting con-\n",
      "fidence in their capabilities and fostering user trust.\n",
      "  It involves recording the execution \n",
      "flow through software/systems.\n",
      "  This granular view enables developers to understand the interaction \n",
      "between various microservices and troubleshoot issues such as latency or failures \n",
      "by identifying exactly where they occurred in the transaction path.\n",
      "\n",
      "  For example, in machine learning applications or projects, tracking can involve keep-\n",
      "ing a record of parameters, hyperparameters, metrics, and outcomes across different \n",
      "experiments or runs.\n",
      "  It provides a way to document progress and changes over time.\n",
      "\n",
      "----------------------------------------\n",
      "Page 311:\n",
      "Topics:\n",
      "  LangChain comes with functionality for trajectory tracking and eval-\n",
      "uation, so seeing the traces of an agent via LangChain is really easy!\n",
      "  The tool first sends a ping to a website address and returns information about packages \n",
      "transmitted and latency or – in the case of an error – the error message:\n",
      "import subprocess\n",
      "from urllib.parse import urlparse\n",
      "from pydantic import HttpUrl\n",
      "from langchain.tools import StructuredTool\n",
      "def ping(url: HttpUrl, return_error: bool) -> str:\n",
      "    \"\"\"Ping the fully specified url.\n",
      "  Now we set up an agent that uses this tool with an LLM to make the calls given a prompt:\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.agents import initialize_agent, AgentType\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\n",
      "agent = initialize_agent(\n",
      "    llm=llm,\n",
      "    tools=[ping_tool],\n",
      "    agent=AgentType.OPENAI_MULTI_FUNCTIONS,\n",
      "    return_intermediate_steps=True, # IMPORTANT!\n",
      "\n",
      "  \"\"\"\n",
      "    hostname = urlparse(str(url)).netloc\n",
      "    completed_process = subprocess.run(\n",
      "    [\"ping\", \"-c\", \"1\", hostname], capture_output=True, text=True\n",
      "    )\n",
      "    output = completed_process.stdout\n",
      "    if return_error and completed_process.returncode !\n",
      "  Generative AI in Production\n",
      "288\n",
      "Tracking the trajectory of agents can be challenging due to their broad range of actions and \n",
      "generative capabilities.\n",
      "  You just have to set the \n",
      "return_intermediate_steps parameter to True when initializing an agent or an LLM.\n",
      "Let’s define a tool as a function.\n",
      "  = 0:\n",
      "        return completed_process.stderr\n",
      "    return output\n",
      "ping_tool = StructuredTool.from_function(ping)\n",
      "\n",
      "Subtopics:\n",
      "  Must include https:// in the url.\n",
      "  It’s convenient to re-use the function docstring as a description of \n",
      "the tool.\n",
      "----------------------------------------\n",
      "Page 312:\n",
      "Topics:\n",
      "  Let’s have a look beyond LangChain and see what else is out there for observability!\n",
      "\n",
      "  The agent reports this:\n",
      "The latency for https://langchain.com is 13.773 ms\n",
      "In results[\"intermediate_steps\"], we can see a lot of information about the agent’s actions:\n",
      "\n",
      "  \"return_error\": false\\n      }\\n    }\\n  ]\\n}'}}, \n",
      "example=False)]), 'PING langchain.com (35.71.142.77): 56 data bytes\\\n",
      "n64 bytes from 35.71.142.77: icmp_seq=0 ttl=249 time=13.773 ms\\\n",
      "n\\n--- langchain.com ping statistics ---\\n1 packets transmitted, 1 \n",
      "packets received, 0.0% packet loss\\nround-trip min/avg/max/stddev = \n",
      "13.773/13.773/13.773/0.000 ms\\n')]\n",
      "By providing visibility into the system and aiding in problem identification and optimization \n",
      "efforts, this kind of tracking and evaluation can be very helpful.\n",
      "\n",
      "  [(_FunctionsAgentAction(tool='ping', tool_input={'url': 'https://\n",
      "langchain.com', 'return_error': False}, log=\"\\nInvoking: `ping` with \n",
      "`{'url': 'https://langchain.com', 'return_error': False}`\\n\\n\\n\", message_\n",
      "log=[AIMessage(content='', additional_kwargs={'function_call': {'name': \n",
      "'tool_selection', 'arguments': '{\\n  \"actions\": [\\n    {\\n      \"action_\n",
      "name\": \"ping\",\\n      \"action\": {\\n        \"url\": \"https://langchain.\n",
      "com\",\\n        \n",
      "  The LangChain documentation demonstrates how to use a trajectory evaluator to examine the \n",
      "full sequence of actions and responses they generate and grade an OpenAI functions agent.\n",
      "  Argilla is an open-source data curation platform that can integrate user feed-\n",
      "back (human-in-the-loop workflows) with prompts and responses to curate datasets \n",
      "for fine-tuning.\n",
      "\n",
      "  Observability tools\n",
      "There are quite a few tools available as integrations in LangChain or through callbacks:\n",
      "•\t\n",
      "Argilla:\n",
      "  Portkey adds essential MLOps capabilities like monitoring detailed metrics, trac-\n",
      "ing chains, caching, and reliability through automatic retries to LangChain.\n",
      "•\t\n",
      "Comet.ml: Comet offers robust MLOps capabilities for tracking experiments, comparing \n",
      "models and optimizing AI projects.\n",
      "\n",
      "  Chapter 9\n",
      "289\n",
      ")\n",
      "result = agent(\"What's the latency like for https://langchain.com?\")\n",
      "\n",
      "  •\t\n",
      "Portkey:\n",
      "Subtopics:\n",
      "  That’s \n",
      "potentially very powerful stuff!\n",
      "\n",
      "----------------------------------------\n",
      "Page 313:\n",
      "Topics:\n",
      "  •\t\n",
      "Langfuse: With this open-source tool, we can conveniently monitor detailed information \n",
      "along traces regarding the latency, cost, and scores of our LangChain agents and tools.\n",
      "•\t\n",
      "LangKit: This extracts signals from prompts and responses to ensure safety and security. \n",
      "\n",
      "  •\t\n",
      "Weights and Biases (W&B) tracing: We’ve already shown an example of using W&B to \n",
      "monitor fine-training convergence, but it can also fulfill the roles of tracking other metrics \n",
      "and logging and comparing prompts.\n",
      "\n",
      "  •\t\n",
      "DataRobot MLOps: Monitors and manages models to detect issues before they impact \n",
      "performance.\n",
      "\n",
      "  •\t\n",
      "Argilla: An open-source platform for tracking training data, validation accuracy, param-\n",
      "eters, and more across machine learning experiments.\n",
      "\n",
      "  •\t\n",
      "Datadog APM integration: This integration allows you to capture LangChain requests, \n",
      "parameters, prompt completions, and visualize LangChain operations.\n",
      "  •\t\n",
      "Aim: An open-source visualization and debugging platform for ML models.\n",
      "  •\t\n",
      "ClearML: An open-source tool for automating training pipelines, seamlessly moving from \n",
      "research to production.\n",
      "\n",
      "  Generative AI in Production\n",
      "290\n",
      "•\t\n",
      "LLMonitor: Tracks lots of metrics including cost and usage analytics (user tracking), \n",
      "tracing, and evaluation tools (open-source).\n",
      "\n",
      "  •\t\n",
      "IBM Watson OpenScale: A platform providing insights into AI health with fast problem \n",
      "identification and resolution to help mitigate risks.\n",
      "\n",
      "  For example, the AgentOps SDK \n",
      "is aiming to provide an interface to a toolkit for evaluating and developing robust and reliable AI \n",
      "agents, but is still in closed alpha.\n",
      "\n",
      "  It logs inputs, \n",
      "outputs, and the serialized state of components, enabling visual inspection of individual \n",
      "LangChain executions and comparing multiple executions side by side.\n",
      "\n",
      "Subtopics:\n",
      "  •\t\n",
      "Splunk: Splunk’s Machine Learning Toolkit can provide observability into your machine \n",
      "learning models in production.\n",
      "\n",
      "  You can also cap-\n",
      "ture metrics such as request latency, errors, and token/cost usage.\n",
      "\n",
      "  Can also help with \n",
      "testing and monitoring model drift or degradation.\n",
      "\n",
      "  It currently focuses on text quality, relevance metrics, and sentiment analysis.\n",
      "\n",
      "  There are more tools out there at different stages of maturation.\n",
      "  •\t\n",
      "DeepEval: Logs default metrics including relevance, bias, and toxicity.\n",
      "----------------------------------------\n",
      "Page 314:\n",
      "Topics:\n",
      "  Let’s have a look at LangSmith now, which is another companion project of LangChain, developed \n",
      "for observability!\n",
      "\n",
      "  LangSmith\n",
      "LangSmith is a framework for debugging, testing, evaluating, and monitoring LLM applications \n",
      "developed and maintained by LangChain AI, the organization behind LangChain.\n",
      "  LangSmith allows you to:\n",
      "•\t\n",
      "Log traces of runs from your LangChain agents, chains, and other components\n",
      "•\t\n",
      "Create datasets to benchmark model performance\n",
      "•\t\n",
      "Configure AI-assisted evaluators to grade your models\n",
      "•\t\n",
      "View metrics, visualizations, and feedback to iterate and improve your LLMs\n",
      "\n",
      "  CallbackHandler() as an \n",
      "argument to the chain.run() call.\n",
      "\n",
      "  LangSmith \n",
      "serves as an effective tool for MLOps, specifically for LLMs, by providing features that cover \n",
      "multiple aspects of the MLOps process.\n",
      "  For \n",
      "example, Langfuse is open-source and provides an option of self-hosting.\n",
      "  With Langfuse, we can hand over langfuse.callback.\n",
      "  For example, for W&B, \n",
      "you can enable tracing by setting the LANGCHAIN_WANDB_TRACING environment variable to True. \n",
      "\n",
      "  Chapter 9\n",
      "291\n",
      "Most of these integrations are very easy to integrate into LLM pipelines.\n",
      "  Choose the option that \n",
      "best suits your needs and follow the instructions provided in the LangChain documentation to \n",
      "enable tracing for your agents.\n",
      "  It can help developers take their LLM applications from \n",
      "prototype to production by providing features for debugging, monitoring, and optimizing.\n",
      "\n",
      "Subtopics:\n",
      "  Some of these tools are open-source, and what’s great about these platforms is that they allow \n",
      "full customization and on-premises deployment for use cases where privacy is important.\n",
      "  Alternatively, you can use a context manager with wandb_tracing_enabled() to trace a specific \n",
      "block of code.\n",
      "  Having been released only recently, I am sure there’s much more to \n",
      "come for the platform, but it’s already great to see traces of how agents execute, detecting loops \n",
      "and latency issues.\n",
      "  It enables sharing traces and stats with collaborators to discuss improvements.\n",
      "\n",
      "----------------------------------------\n",
      "Page 315:\n",
      "Topics:\n",
      "  Generative AI in Production\n",
      "292\n",
      "On the LangSmith web interface, we can get a large set of graphs for a bunch of statistics that can \n",
      "be useful to optimize latency, hardware efficiency, and cost, as we can see here in the monitoring \n",
      "dashboard:\n",
      "Figure 9.5: Evaluator metrics in LangSmith\n",
      "The monitoring dashboard includes the following graphs that can be broken down into different \n",
      "time intervals: \n",
      "Statistics\n",
      "Category\n",
      "Trace Count, LLM Call Count, Trace Success Rates, LLM Call Success Rates\n",
      "Volume\n",
      "Trace Latency (s), LLM Latency (s), LLM Calls per Trace, Tokens / sec\n",
      "Latency\n",
      "Total Tokens, Tokens per Trace, Tokens per LLM Call\n",
      "Tokens\n",
      "% Traces w/ Streaming, % LLM Calls w/ Streaming, Trace Time-to-First-Token (ms), \n",
      "LLM Time-to-First-Token (ms)\n",
      "Streaming\n",
      "Table 9.2: Statistics in LangSmith\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 316:\n",
      "Topics:\n",
      "  Tracing in LangSmith\n",
      "The platform itself is not open-source, however, LangChain AI, the company behind LangSmith \n",
      "and LangChain, provides some support for self-hosting for organizations with privacy concerns. \n",
      "\n",
      "  There are, however, a few alternatives to LangSmith such as Langfuse, Weights and Biases, Dat-\n",
      "adog APM, Portkey, and PromptWatch, with some overlap in features.\n",
      "  We’ll focus on LangSmith \n",
      "here because it has a large set of features for evaluation and monitoring, and because it integrates \n",
      "with LangChain.\n",
      "\n",
      "  In the next section, we’ll demonstrate the utilization of PromptWatch for prompt tracking of \n",
      "LLMs in production environments.\n",
      "\n",
      "  Chapter 9\n",
      "293\n",
      "Here’s a tracing example in LangSmith for the benchmark dataset run that we saw in the How to \n",
      "evaluate LLM apps section:\n",
      "Figure 9.6:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 317:\n",
      "Topics:\n",
      "  Make sure you sign up with PromptWatch.io online and get your API key – you can find it under \n",
      "the account settings.\n",
      "\n",
      "  With PromptWatch.io, you can even track various aspects of LLM \n",
      "chains, actions, retrieved documents, inputs, outputs, execution time, tool details, and more for \n",
      "complete visibility in your system.\n",
      "\n",
      "  We \n",
      "can see this on PromptWatch.io.\n",
      "\n",
      "  Generative AI in Production\n",
      "294\n",
      "PromptWatch\n",
      "PromptWatch records information about response caching, chain execution, prompting and gen-\n",
      "erated output during interactions.\n",
      "  Next, we \n",
      "need to set up a prompt and a chain:\n",
      "prompt_template = PromptTemplate.from_template(\"Finish this sentence \n",
      "{input}\")\n",
      "my_chain = LLMChain(llm=OpenAI(), prompt=prompt_template)\n",
      "Using the PromptTemplate class, the prompt template is configured with one variable, input, \n",
      "indicating where the user input should be placed within the prompt.\n",
      "\n",
      "  We can create a PromptWatch block, where LLMChain is invoked with an input prompt:\n",
      "with PromptWatch() as pw:\n",
      "    my_chain(\"The quick brown fox jumped over\")\n",
      "\n",
      "  Let’s get the inputs out of the way:\n",
      "from langchain import LLMChain, OpenAI, PromptTemplate\n",
      "from promptwatch import PromptWatch\n",
      "As discussed in Chapter 3, Getting Started with LangChain, I’ve set all API keys in the environment \n",
      "in the set_environment() function.\n",
      "Subtopics:\n",
      "  If you’ve followed my recommendation, you can follow the \n",
      "imports up with this:\n",
      "from config import set_environment\n",
      "set_environment()\n",
      "\n",
      "  This is a simple example of the model generating a response based on the provided prompt.\n",
      "  The tracing and monitoring can be very useful for debugging \n",
      "and ensuring an audit trail.\n",
      "  Otherwise, please make sure you set your environment variables in the way you prefer.\n",
      "----------------------------------------\n",
      "Page 318:\n",
      "Topics:\n",
      "  Summary\n",
      "Taking a trained LLM from research into real-world production involves navigating many complex \n",
      "challenges around aspects such as scalability, monitoring, and unintended behaviors.\n",
      "  There are many more \n",
      "tools we could have explored, for example, the recently emerged LangServe, which is developed \n",
      "with LangChain applications in mind.\n",
      "  Particularly, we deployed \n",
      "applications with FastAPI and Ray.\n",
      "  Chapter 9\n",
      "295\n",
      "Figure 9.7: Prompt tracking at PromptWatch.io\n",
      "We can see the prompt together with the LLM’s response.\n",
      "  In earlier chapters, we used Streamlit.\n",
      "  With care and preparation, generative AI holds immense potential benefit to industries from \n",
      "medicine to education.\n",
      "\n",
      "Subtopics:\n",
      "  We could have explored \n",
      "more, for example around prompt templates and versioning, but there’s only so much we can \n",
      "cover here.\n",
      "  We’ve delved into deployment and the tools used for deployment.\n",
      "  The platform allows for in-depth analysis and troubleshooting in the web interface that enables \n",
      "users to identify the root causes of issues and optimize prompt templates.\n",
      "  Responsibly \n",
      "deploying capable, reliable models involves diligent planning around scalability, interpretability, \n",
      "testing, and monitoring.\n",
      "  Techniques such as fine-tuning, safety interventions, and defensive \n",
      "design enable us to develop applications that produce helpful, harmless, and readable outputs. \n",
      "\n",
      "  This seems quite useful \n",
      "to effectively monitor and analyze prompts, outputs, and costs in real-world scenarios.\n",
      "\n",
      "  promptwatch.io can also help with unit testing and versioning prompt templates.\n",
      "\n",
      "  While it’s still relatively fresh, it’s definitely worth watching \n",
      "out for more developments in the future.\n",
      "\n",
      "  We also get a dashboard with a time \n",
      "series of activity, where we can drill down into responses at certain times.\n",
      "----------------------------------------\n",
      "Page 319:\n",
      "Topics:\n",
      "  How can we evaluate LLM apps?\n",
      "4.\t\n",
      "Which tools can help to evaluate LLM apps?\n",
      "5.\t\n",
      "\n",
      "  LangChain sup-\n",
      "ports comparative evaluation between models, checking outputs against criteria, simple string \n",
      "matching, and semantic similarity metrics.\n",
      "  How can we monitor LLM applications?\n",
      "9.\t\n",
      "\n",
      "  If you are \n",
      "unsure about any of them, you might want to refer to the corresponding section in the chapter:\n",
      "1.\t\n",
      "\n",
      "  In your opinion, what is the best term for describing the operationalization of language \n",
      "models, LLM apps, or apps that rely on generative models in general?\n",
      "2.\t \n",
      "  6.\t\n",
      "Name a few tools used for deployment.\n",
      "\n",
      "  What’s LangSmith?\n",
      "\n",
      "  We’ve looked at different tools for observability such as PromptWatch and LangSmith.\n",
      "  Generative AI in Production\n",
      "296\n",
      "The evaluation of LLMs is important to assess their performance and quality.\n",
      "  What is a token and why should you know about token usage when querying LLMs?\n",
      "3.\t\n",
      "\n",
      "  What are the important metrics for monitoring LLMs in production?\n",
      "8.\t \n",
      "  In the next and final chapter, let’s discuss what the future of Generative AI will look like.\n",
      "\n",
      "  7.\t\n",
      "\n",
      "  LangSmith \n",
      "provides powerful capabilities to track, benchmark, and optimize LLMs built with LangChain. \n",
      "\n",
      "Subtopics:\n",
      "  Systematic evaluation is key to ensuring LLMs produce \n",
      "useful, relevant, and sensible outputs.\n",
      "\n",
      "  Its automated evaluators, metrics, and visualizations help accelerate LLM development and \n",
      "validation.\n",
      "\n",
      "  Monitoring LLMs is a vital aspect of deploying and maintaining these complex systems.\n",
      "  These provide different insights into model quality, \n",
      "accuracy, and appropriate generation.\n",
      "  Questions\n",
      "Please try and see if you can come up with the answers to these questions from memory.\n",
      "  With the \n",
      "increasing adoption of LLMs in various applications, ensuring their performance, effectiveness, \n",
      "and reliability is of utmost importance.\n",
      "  We’ve discussed the significance of monitoring LLMs, \n",
      "highlighted key metrics to track for a comprehensive monitoring strategy, and have given exam-\n",
      "ples of how to track metrics in practice.\n",
      "\n",
      "  What are the considerations for the production deployment of agents?\n",
      "\n",
      "----------------------------------------\n",
      "Page 320:\n",
      "Topics:\n",
      "  Chapter 9\n",
      "297\n",
      "Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 321:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 322:\n",
      "Topics:\n",
      "  10\n",
      "The Future of Generative Models\n",
      "\n",
      "  We will evaluate and address concerns such as misinformation, cybersecurity, privacy, and fair-\n",
      "ness, and think about how the changes and disruptions brought about by generative AI should \n",
      "influence regulations and practical implementation.\n",
      "\n",
      "  Given the massive potential for increased productivity in various industries, venture funding for \n",
      "generative AI start-ups skyrocketed in 2022 and 2023, and major players like Salesforce and Ac-\n",
      "centure among many others have made big commitments to generative AI with multibillion-dollar \n",
      "investments.\n",
      "  We’ll also \n",
      "evaluate and address concerns such as the extinction threat through AI.\n",
      "\n",
      "Subtopics:\n",
      "  We’ll discuss potential effects on jobs in multiple industries, and disruptive changes \n",
      "in creative industries, education, law, manufacturing, medicine, and the military.\n",
      "\n",
      "  In this book, so far, we have discussed generative models for building applications, and we have \n",
      "implemented a few simple ones – for example, for semantic search, applications for content \n",
      "creation, customer service agents, and assistants for developers and data scientists.\n",
      "  We’ll focus on value creation opportunities, where unique customization of \n",
      "foundation models for specific use cases stands out.\n",
      "  We have \n",
      "explored techniques such as tool use, agent strategies, semantic search with retrieval augmented \n",
      "generation, and the conditioning of models with prompts and fine-tuning.\n",
      "\n",
      "  In this chapter, we’ll deliberate on where this leaves us and where the future leads us.\n",
      "  It remains uncertain which entities – big \n",
      "tech firms, start-ups, or foundation model developers – will capture the most upsides.\n",
      "  We’ll consid-\n",
      "er weaknesses and socio-technical challenges of generative models, and strategies for mitigation \n",
      "and improvement.\n",
      "----------------------------------------\n",
      "Page 323:\n",
      "Topics:\n",
      "  The current state of generative AI\n",
      "As discussed in this book, in recent years, generative AI models have attained new milestones in \n",
      "producing human-like content across modalities including text, images, audio, and video.\n",
      "  The Future of Generative Models\n",
      "300\n",
      "The main sections of this chapter are:\n",
      "•\t\n",
      "The current state of generative AI\n",
      "•\t\n",
      "Economic consequences\n",
      "•\t\n",
      "Societal implications\n",
      "Let’s start with the current state of models and their capabilities.\n",
      "\n",
      "  These AI models leverage gargantuan datasets and \n",
      "computational scale, enabling them to capture intricate linguistic patterns, display a nuanced \n",
      "understanding of knowledge about the world, translate texts, summarize content, answer natural \n",
      "language questions, create appealing visual art, and acquire the capability to describe images. \n",
      "\n",
      "  Leading \n",
      "models like OpenAI’s GPT-4 and DALL-E 2, and Anthropic’s Claude display impressive fluency \n",
      "in content generation, be it textual or creative visual artistry.\n",
      "\n",
      "  Between 2022 and 2023, models have progressed in strides.\n",
      "  Seemingly by magic, the AI-generated outputs mimic human ingenuity – painting original art, \n",
      "writing poetry, producing human-level prose, and even engaging in sophisticated aggregation \n",
      "and synthesis of information from diverse sources.\n",
      "\n",
      "  AI can have serious bias issues because of the prejudiced data \n",
      "they are trained on.\n",
      "Subtopics:\n",
      "  They are easily \n",
      "confused by complex inferential questions, which could limit their applicability in certain fields \n",
      "of work.\n",
      "  Hallucinations show a lack of grounding in reality, given \n",
      "that they are based on patterns in data rather than an understanding of the real world.\n",
      "  This can lead to unfair results and make social inequalities worse.\n",
      "\n",
      "  But let’s be a bit more nuanced!\n",
      "  Further, \n",
      "models exhibit difficulties performing mathematical, logical, or causal reasoning.\n",
      "  Deficiencies persist compared to human cognition, including the frequent generation of plausible \n",
      "yet incorrect or nonsensical statements.\n",
      "  The black box problem of lack of explainability for outputs as well as for the models \n",
      "themselves hampers troubleshooting efforts, and controlling model behaviors within desired \n",
      "parameters remains challenging.\n",
      "  If generative models were previously \n",
      "capable of producing barely coherent text or grainy images, now we see high-quality 3D images, \n",
      "videos, and the generation of coherent and contextually relevant prose and dialogue, rivaling or \n",
      "even surpassing the fluency levels of humans.\n",
      "  Generative models come with weaknesses as well as strengths. \n",
      "\n",
      "----------------------------------------\n",
      "Page 324:\n",
      "Topics:\n",
      "  Chapter 10\n",
      "301\n",
      "Here is a table summarizing the key strengths and deficiencies of current generative AI compared \n",
      "to human cognition:\n",
      "Category\n",
      "Human Cognition\n",
      "Generative AI Models\n",
      "Language Fluency\n",
      "Contextually relevant, draws \n",
      "meaning from world knowledge\n",
      "Highly eloquent, reflects linguistic \n",
      "patterns\n",
      "Knowledge\n",
      "Conceptual understanding derived \n",
      "from learning and experience\n",
      "Statistical synthesis lacking \n",
      "grounding\n",
      "Creativity\n",
      "Originality reflecting personality \n",
      "and talent\n",
      "Imaginative but within training \n",
      "distribution\n",
      "Factual Accuracy\n",
      "Usually aligns with truth and \n",
      "physical reality\n",
      "Hallucinations reflecting training \n",
      "data biases\n",
      "Reasoning\n",
      "Intuitive yet can apply heuristics \n",
      "after training\n",
      "Logic is tightly limited to training \n",
      "distribution\n",
      "Bias\n",
      "Sometimes recognizes and can \n",
      "override inherent biases\n",
      "Propagates systemic biases in data\n",
      "Transparency\n",
      "Partial, subjective insights from \n",
      "think-aloud techniques\n",
      "Plausible reasoning from chain-of-\n",
      "thought prompts\n",
      "Table 10.1:\n",
      "  Strengths and deficiencies of LLMs\n",
      "While LLMs such as GPT-4 showcase language fluency on parity with humans, their lack of \n",
      "grounding, tendency for distortion, opaqueness, and potential for harm underscore deficiencies \n",
      "that temper the promise of generative AI.\n",
      "  As for transparency, while immense complexity poses an \n",
      "immense challenge, determined efforts seek to surface the lineage and mechanisms of reasoning \n",
      "for both humans (advances in the understanding of neurocognition) and AI (interpretability and \n",
      "explainability).\n",
      "  Throughout the book, we’ve discussed and implemented potential solutions that address \n",
      "the weaknesses of generative AI.\n",
      "\n",
      "  Let’s look more broadly at some of the socio-technical challenges involved in unlocking the \n",
      "capabilities of generative AI systems and discuss approaches to overcoming them!\n",
      "\n",
      "  We should keep in mind, however, that this gap analysis of human versus AI is for highlighting \n",
      "areas of improvement – as we have seen in domains such as Atari games, chess, and Go, AIs can \n",
      "reach superhuman levels if trained properly, and we haven’t touched the ceiling yet in many ar-\n",
      "eas.\n",
      "Subtopics:\n",
      "  Progress in domains like logical reasoning and bias \n",
      "mitigation remains at an early stage.\n",
      "  Addressing problematic areas is key to developing reliable and trustworthy sys-\n",
      "tems.\n",
      "----------------------------------------\n",
      "Page 325:\n",
      "Topics:\n",
      "  The Future of Generative Models\n",
      "302\n",
      "Challenges\n",
      "The profound potential of generative AI systems indicates an exciting future if development con-\n",
      "tinues at pace.\n",
      "  This table shows a summary of a few of the technical and organizational challenges \n",
      "together with approaches to tackle them:\n",
      "Challenge\n",
      "Potential Solutions\n",
      "Knowledge Freshness (+ Concept Drift)\n",
      "Continuous learning methods like elastic weight \n",
      "consolidation, stream ingestion pipelines, and \n",
      "efficient retraining procedures\n",
      "Specialized Knowledge\n",
      "Task-specific demonstrations and prompting, \n",
      "knowledge retrieval and grounding, and context \n",
      "expansion\n",
      "Downstream Adaptability\n",
      "Strategic fine-tuning methods, catastrophic \n",
      "forgetting mitigation, and optimized hardware access\n",
      "Biased Outputs\n",
      "Bias mitigation algorithms, balanced training data, \n",
      "audits, inclusivity training, and interdisciplinary \n",
      "research\n",
      "Harmful Content Generation\n",
      "Moderation systems, interruption and correction, and \n",
      "conditioning methods such as RLHF\n",
      "Logical Inconsistencies\n",
      "Hybrid architectures, knowledge bases, and retrieval \n",
      "augmentation\n",
      "Factual Inaccuracies\n",
      "Retrieval augmentation, knowledge bases, and \n",
      "consistent knowledge base updating\n",
      "Lack of Explainability\n",
      "Model introspection, concept attribution, and \n",
      "interpretable model designs\n",
      "Privacy Risks\n",
      "Differential privacy, federated learning, encryption, \n",
      "and anonymization\n",
      "High Latency and Compute Costs\n",
      "Model distillation, optimized hardware, and efficient \n",
      "model design\n",
      "Licensing Limitations\n",
      "Open/synthetic data, custom data, and fair licensing \n",
      "agreements\n",
      "Security/Vulnerabilities\n",
      "Adversarial robustness and cybersecurity best \n",
      "practices\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 326:\n",
      "Topics:\n",
      "  Technological innovation together with regulation and transparency of AI development will \n",
      "ensure that generative AI enhances human capability without compromising ethical standards. \n",
      "\n",
      "  With concerted effort in research \n",
      "and development, the aim is to steer generative AI toward alignment with societal values.\n",
      "  Strategies like employing \n",
      "simplified model architectures, using knowledge distillation, and developing specialized hardware \n",
      "are critical to reducing the carbon footprint of AI in the face of rapid progress.\n",
      "  This, in turn, will \n",
      "lead to a proliferation of start-ups applying core AI technologies.\n",
      "\n",
      "  Looking ahead, generative AI systems are poised to become more powerful and multifaceted.\n",
      "\n",
      "  Chapter 10\n",
      "303\n",
      "Governance\n",
      "Compliance frameworks and ethical development \n",
      "governance\n",
      "Table 10.2: Challenges of generative AI and potential solutions\n",
      "Challenges of generative AI go beyond just improving content generation—they encompass en-\n",
      "vironmental sustainability, algorithmic equity, and individual privacy.\n",
      "  To ensure fair AI, \n",
      "steps such as incorporating balanced datasets, applying bias mitigation algorithms, enforcing \n",
      "fairness through constrained optimization, and promoting inclusivity are essential, despite their \n",
      "complexity.\n",
      "\n",
      "  Addressing these challenges involves a broad spectrum of responses that must consider the entire \n",
      "life cycle of AI development.\n",
      "  To counteract potential harm from AI output, such as toxicity or false information (hallucination), \n",
      "techniques like reinforcement learning guided by human feedback and grounding responses in \n",
      "verified knowledge can be employed.\n",
      "  One of the most effective developments is flexible user control.\n",
      "Subtopics:\n",
      "  For \n",
      "reasons of computational efficiency and costs, this implies a shift from pretraining to specialized \n",
      "downstream conditioning (particularly, fine-tuning and prompt techniques).\n",
      "  Let’s have a look at some emerging trends in model development!\n",
      "\n",
      "  Finally, staying up to date with the evolving informational landscapes, comprehending specialized \n",
      "domains, and flexibly adapting to emerging needs represent newly visible obstacles as generative \n",
      "models permeate real-world contexts.\n",
      "\n",
      "  Additionally, securing sensitive data through privacy-pre-\n",
      "serving methods like differential privacy, federated learning, and real-time content correction is \n",
      "fundamental for upholding user dignity.\n",
      "\n",
      "  Such responses include innovative training objectives focused on \n",
      "consistency, structural knowledge integrations, and design of models for better controllability, \n",
      "as well as software and hardware optimization for infrastructure efficiency.\n",
      "\n",
      "----------------------------------------\n",
      "Page 327:\n",
      "Topics:\n",
      "  The Future of Generative Models\n",
      "304\n",
      "Trends in model development\n",
      "The current doubling time in training compute of very large models is about 8 months, outstrip-\n",
      "ping scaling laws such as Moore’s Law (transistor density at cost increases at a rate of currently \n",
      "about 18 months) and Rock’s Law (costs of hardware like GPUs and TPUs halve every 4 years). \n",
      "\n",
      "  This graph illustrates this trend in training compute of large models (source: Epoch, Parameter, \n",
      "Compute, and Data Trends in Machine Learning.\n",
      "  As discussed \n",
      "in Chapter 1, What Is Generative AI?, parameter sizes for large systems have been increasing at a \n",
      "similar rate as the training compute, which means we could see much larger and more expensive \n",
      "systems if this growth continues.\n",
      "\n",
      "  This could mean that highly powerful sys-\n",
      "tems will be concentrated in the hands of Big Tech.\n",
      "\n",
      "  Retrieved from https://epochai.org/mlinputs/\n",
      "visualization):\n",
      "Figure 10.1: Training FLOPs of notable AI systems\n",
      "The main point from this graph is the increase in compute, which is apparent since the 1960s, \n",
      "and the Cambrian explosion of models of the deep learning era at the top right.\n",
      "Subtopics:\n",
      "  Empirically derived scaling laws predict the performance of LLMs based on the given training \n",
      "budget, dataset size, and the number of parameters.\n",
      "----------------------------------------\n",
      "Page 328:\n",
      "Topics:\n",
      "  Further, there is a body of work on simplified model architectures, which have substantially fewer \n",
      "parameters and only modestly drop accuracy (for example, One Wide Feedforward is All You Need, \n",
      "Pessoa Pires and others, 2023).\n",
      "  The Chinchilla scaling law, developed by the Google DeepMind team, involved ex-\n",
      "periments with a wider range of model sizes and data sizes and suggests an optimal \n",
      "allocation of compute budget to model size and data size, which can be determined \n",
      "by optimizing a specific loss function under a constraint.\n",
      "\n",
      "  The KM scaling law, proposed by Kaplan and colleagues, derived through empirical \n",
      "analysis and fitting of model performance with varied data sizes, model sizes, and \n",
      "training compute, presents power-law relationships, indicating a strong codepen-\n",
      "dence between model performance and factors such as model size, dataset size, and \n",
      "training compute.\n",
      "\n",
      "  Chapter 10\n",
      "305\n",
      "However, future progress may depend more on data efficiency and model quality than sheer \n",
      "size.\n",
      "  As men-\n",
      "tioned in Chapter 6, Developing Software with Generative AI, we’ve recently seen models such as \n",
      "phi-1 (Textbooks Are All You Need, 2023, Gunasekar and colleagues), with about 1 billion parameters, \n",
      "that – despite its smaller scale – achieve high accuracy on evaluation benchmarks.\n",
      "Subtopics:\n",
      "  The future could see the co-existence of massive, general models with \n",
      "smaller and more accessible specialized niche models that provide faster and cheaper training, \n",
      "maintenance, and inference.\n",
      "\n",
      "  The authors \n",
      "suggest that improving data quality can dramatically change the shape of scaling laws.\n",
      "\n",
      "  It has already been shown that smaller specialized models can prove highly performant.\n",
      "  Though massive models grab headlines, computing power and energy constraints put a \n",
      "limit on unrestrained model growth.\n",
      "  It’s also unclear if performance will keep up further with \n",
      "the growth in parameters.\n",
      "  To compensate for model limitations, tools like search engines \n",
      "and calculators have been incorporated into agents, and multi-step reasoning strategies, plugins, \n",
      "and extensions may be increasingly used to expand capabilities.\n",
      "\n",
      "  Additionally, techniques such as fine-tuning, distillation, and \n",
      "prompting techniques can enable smaller models to leverage the capabilities of large foundations \n",
      "without replicating their costs.\n",
      "----------------------------------------\n",
      "Page 329:\n",
      "Topics:\n",
      "  •\t\n",
      "Model distillation: Transferring knowledge from a large model into a smaller, more ef-\n",
      "ficient one for easy deployment.\n",
      "\n",
      "  •\t\n",
      "Retrieval augmented generation: Enhancing text generation by retrieving relevant in-\n",
      "formation from sources.\n",
      "•\t\n",
      "Federated learning: Training models on decentralized data to improve privacy while \n",
      "benefiting from diverse sources.\n",
      "\n",
      "  The Future of Generative Models\n",
      "306\n",
      "The rapidly decreasing costs of AI model training represent a significant shift in the landscape, \n",
      "enabling broader participation in cutting-edge AI research and development.\n",
      "  Here is a brief summary of \n",
      "techniques and approaches for making generative AI more accessible and effective:\n",
      "•\t\n",
      "Simplified model architectures: Streamlining model design for easier management, better \n",
      "interpretability, and lower computational cost.\n",
      "\n",
      "  Open-source datasets and techniques such as synthetic data \n",
      "generation further democratize access to AI training by providing high-quality and data-efficient \n",
      "model development and removing some reliance on vast, proprietary datasets.\n",
      "  •\t\n",
      "Optimized inference engines: Software frameworks that increase the speed and efficiency \n",
      "of executing AI models on a given hardware.\n",
      "•\t\n",
      "Dedicated AI hardware accelerators: Specialized hardware like GPUs and TPUs that \n",
      "dramatically accelerate AI computations.\n",
      "\n",
      "Subtopics:\n",
      "  •\t\n",
      "Quantization: Converting models to lower precision by reducing bit sizes of weights and \n",
      "activations, decreasing model size and compute costs.\n",
      "\n",
      "  As noted, several \n",
      "factors are contributing to this trend, including optimization of training regimes, improvements \n",
      "in data quality, and the introduction of novel model architectures.\n",
      "  •\t\n",
      "Open-source and synthetic data: High-quality public datasets enable collaboration and \n",
      "synthetic data enhances privacy and can help reduce bias.\n",
      "\n",
      "  Open-source ini-\n",
      "tiatives contribute to the trend by providing cost-effective, collaborative platforms for innovation.\n",
      "\n",
      "  •\t\n",
      "Incorporating knowledge bases: Grounding model outputs in factual databases reduces \n",
      "hallucinations and improves accuracy.\n",
      "\n",
      "  •\t\n",
      "Synthetic data generation: Creating artificial training data to augment datasets while \n",
      "preserving privacy.\n",
      "\n",
      "  Among the technical advancements helping drive down these costs, quantization techniques have \n",
      "emerged as an essential contributor.\n",
      "----------------------------------------\n",
      "Page 330:\n",
      "Topics:\n",
      "  However, this route \n",
      "traditionally requires significant AI expertise, substantial computational resources, and rigorous \n",
      "data privacy safeguards, which can be prohibitively expensive and complex for smaller entities.\n",
      "\n",
      "  Conversely, in the self-service scenario, companies or individuals take on the task of training their \n",
      "own AI models.\n",
      "  Chapter 10\n",
      "307\n",
      "These innovations collectively lower barriers that have so far impeded real-world generative AI \n",
      "adoption across various segments:\n",
      "•\t\n",
      "Financial barriers are reduced by compressing large model performance into far smaller \n",
      "form factors through quantization and distillation.\n",
      "\n",
      "  •\t\n",
      "Democratizing access by tackling constraints like cost, security, and reliability unlocks \n",
      "benefits for vastly expanded audiences, steering generative creativity from a narrow con-\n",
      "centration toward empowering diverse human talents.\n",
      "\n",
      "  In the centralized scenario, generative \n",
      "AI and LLMs are primarily developed and controlled by large tech firms that invest heavily in the \n",
      "necessary computational hardware, data storage, and specialized AI/ML talent.\n",
      "  •\t\n",
      "The accuracy limitations hampering small models are relieved through grounding gen-\n",
      "eration with external information.\n",
      "\n",
      "  Big Tech vs. small enterprises\n",
      "As for the spread of technology, two primary scenarios exist.\n",
      "  •\t\n",
      "Specialized hardware exponentially accelerates throughput while optimized software \n",
      "maximizes the existing infrastructure.\n",
      "\n",
      "  With quantization \n",
      "and related techniques lowering barriers, we’re poised for a more diverse and dynamic era of AI \n",
      "development where resource wealth is not the only determinant of leadership in AI innovation.\n",
      "\n",
      "  They produce general models that are often made \n",
      "accessible to others through cloud services or APIs, but these one-size-fits-all solutions may not \n",
      "perfectly align with the requirements of every user or organization.\n",
      "\n",
      "Subtopics:\n",
      "  This could mean a democratization of the market, as we’ll see now.\n",
      "\n",
      "  The landscape is shifting from a focus on sheer model size and brute-force compute to clever, nu-\n",
      "anced approaches that maximize computational efficiency and model efficacy.\n",
      "  This approach allows for models that are customized to the specific needs and pro-\n",
      "prietary data of the user, providing more targeted and relevant functionality.\n",
      "  Entities like these \n",
      "benefit from economies of scale and resources that allow them to bear the high costs of training \n",
      "and maintaining these sophisticated systems.\n",
      "  •\t\n",
      "Privacy risks are mitigated via federated and synthetic techniques circumventing exposure.\n",
      "\n",
      "----------------------------------------\n",
      "Page 331:\n",
      "Topics:\n",
      "  If robust tools emerge to simplify and automate AI development, custom generative models may \n",
      "even be viable for local governments, community groups, and individuals to address hyper-local \n",
      "challenges.\n",
      "  In a timeframe of 3–5 years, constraints around computing and talent availability could ease \n",
      "considerably, eroding the centralized moat created by massive investments.\n",
      "  The Future of Generative Models\n",
      "308\n",
      "\n",
      "  Yet with the democratization of AI – driven by declining computational costs, more widespread \n",
      "AI training and tools, and innovations that simplify model training – the self-service scenario \n",
      "may become increasingly viable for smaller organizations, local governments, and community \n",
      "groups.\n",
      "  Specifically, if cloud \n",
      "computing costs decline as projected, and AI skills become more widespread through education \n",
      "and automated tools, self-training customized LLMs may become feasible for many companies.\n",
      "\n",
      "  As costs decline for computing, \n",
      "data storage, and AI talent, custom pre-training of specialized models could become feasible for \n",
      "small and mid-sized companies.\n",
      "\n",
      "  As these two business models continue to develop, a hybrid landscape may emerge where both \n",
      "approaches fulfill distinct roles based on use cases, resources, expertise, and privacy considerations. \n",
      "\n",
      "  These groups could potentially harness tailored AI solutions for highly specific tasks, \n",
      "gaining advantages in agility and privacy preservation.\n",
      "\n",
      "  Rather than relying on generic models from Big Tech, tailored generative AI fine-tuned on niche \n",
      "datasets could better serve unique needs.\n",
      "  While large tech firms currently dominate generative AI research and development, smaller entities \n",
      "may ultimately stand to gain the most from these technologies.\n",
      "  While centralized Big Tech firms benefit currently from economies of scale, distributed \n",
      "innovation from smaller entities could unlock generative AI’s full potential across all sectors of \n",
      "society.\n",
      "\n",
      "  The evolution of \n",
      "this landscape will largely depend on the pace of advancements that make AI more accessible, \n",
      "more cost-effective, and simpler to use without compromising robustness or privacy.\n",
      "\n",
      "  In the next section, we’ll discuss the potential of Artificial General Intelligence (AGI) and the \n",
      "threat of extinction by the malicious actions of a superintelligent artificial entity.\n",
      "\n",
      "Subtopics:\n",
      "  Presently, the centralized \n",
      "approach dominates due to the barriers in cost and expertise required for the self-service model. \n",
      "\n",
      "  Democratized access through cost \n",
      "reductions could enable such focused players to train performant models exceeding the capa-\n",
      "bilities of generalized systems.\n",
      "\n",
      "  The central question is how these scenarios will coexist and evolve.\n",
      "  Large firms might continue to excel in providing industry-specific models, while smaller entities \n",
      "could increasingly train or fine-tune their own models to meet niche demands.\n",
      "  Start-ups and non-profits often excel at rapidly iterat-\n",
      "ing to build cutting-edge solutions for specialized domains.\n",
      "----------------------------------------\n",
      "Page 332:\n",
      "Topics:\n",
      "  As we address pressing AI challenges, the discourse around AI’s threat and its potential for societal \n",
      "disruption should not overshadow immediate issues like fairness and privacy. \n",
      "\n",
      "  Chapter 10\n",
      "309\n",
      "Artificial General Intelligence\n",
      "Not all abilities in LLMs scale predictably with model size.\n",
      "  •\t\n",
      "Different architecture from biological brains: The relatively simple stacked transform-\n",
      "er architecture used in models like GPT-4 lacks the complex recurrent and hierarchical \n",
      "structures of the thalamocortical system thought to enable consciousness and general \n",
      "reasoning in humans.\n",
      "\n",
      "  •\t\n",
      "Limited real-world knowledge: Despite ingesting huge datasets, the factual knowledge \n",
      "and common sense of large models remain very restricted compared to humans.\n",
      "  Nevertheless, current neuroscientific perspectives and the limitations of existing AI structures \n",
      "provide compelling arguments against an imminent leap to AGI (inspired by the discussion in \n",
      "the article The feasibility of artificial consciousness through the lens of neuroscience by Jaan Aru and \n",
      "others; 2023):\n",
      "•\t\n",
      "Lack of embodied, embedded information: The current generation of LLMs lacks multi-\n",
      "modal and embodied experiences, being trained predominantly on textual data.\n",
      "  •\t\n",
      "Minimal social abilities or intent: Current AI systems have no innate motivations, social \n",
      "intelligence, or intent beyond their training objectives.\n",
      "  •\t\n",
      "Narrow capabilities: Existing models remain specialized for particular domains like text \n",
      "and fall short in flexibility, causal reasoning, planning, social skills, and general prob-\n",
      "lem-solving intelligence.\n",
      "Subtopics:\n",
      "  Capabilities such as in-context learn-\n",
      "ing may remain exclusive to particularly large models due to factors beyond raw computational \n",
      "growth.\n",
      "  •\t\n",
      "Data-driven limitations: Reliance on pattern recognition from training data rather than \n",
      "structured knowledge makes reliable generalization to novel situations difficult.\n",
      "\n",
      "  Fears of malicious goals or desire \n",
      "for domination seem unfounded.\n",
      "\n",
      "  This could change either with increasing tool use or with fun-\n",
      "damental changes to the models.\n",
      "\n",
      "  This \n",
      "impedes applicability in the physical world.\n",
      "\n",
      "  In contrast, \n",
      "human common sense and understanding of the physical world are developed through \n",
      "rich, diverse interactions involving multiple senses.\n",
      "\n",
      "  There’s speculation that sustained scaling – training vast models on even larger data-\n",
      "sets – might lead to broader skill sets and, some suggest, toward the development of AGI with \n",
      "reasoning abilities on par or beyond humans.\n",
      "\n",
      "----------------------------------------\n",
      "Page 333:\n",
      "Topics:\n",
      "  Software developers benefit from AI’s ability to produce \n",
      "code snippets, helping to expedite the development process.\n",
      "  For scholars and scientists, the ability \n",
      "of AI to distill complex research into comprehensive summaries can catalyze scholarly progress \n",
      "and innovation.\n",
      "\n",
      "  But LLM-enhanced software \n",
      "could transform 50% of tasks, affirming the force multiplication from complementary innovations.\n",
      "\n",
      "  Yet automation extends beyond employment loss – at present, under \n",
      "20% of US worker tasks seem automatable directly through LLMs.\n",
      "  Sectors like customer service, marketing, \n",
      "software engineering, and R&D may see over 75% of use case value.\n",
      "  Let’s discuss the broader economy, and – the elephant in the room – jobs!\n",
      "\n",
      "  Nonetheless, \n",
      "ongoing attention to safety research and ethical concerns is essential, especially as AI advances.\n",
      "\n",
      "  Assuming computing \n",
      "scales sustainably, projections estimate 30–50% of current work activities will be automatible \n",
      "by 2030, adding $6–8 trillion annually to global GDP.\n",
      "  In a professional context, generative AI is poised to amplify human creativity and transform \n",
      "traditional workflows across a range of industries.\n",
      "  The Future of Generative Models\n",
      "310\n",
      "Given current model limitations and the lack of agency, the notion of today’s AI rapidly evolving \n",
      "into a dangerous superintelligence appears highly unlikely.\n",
      "  For content creators, such as marketers and \n",
      "journalists, AI can rapidly generate initial drafts, fostering a baseline that human creativity can \n",
      "build upon for more customized outputs.\n",
      "  Economic consequences\n",
      "Integrating generative AI promises immense productivity gains through automating tasks across \n",
      "sectors – albeit risking workforce disruptions given the pace of change.\n",
      "  Still, the virtuous cycle between AI progress and emerging specializations signals \n",
      "hopes for an uplift over redundancy.\n",
      "Subtopics:\n",
      "  Thus automation’s labor impact remains complex – while augmenting productivity, transitional \n",
      "pains persist.\n",
      "  In formulating regulations, we must \n",
      "be vigilant against regulatory capture, where dominant industry players invoke far-fetched sce-\n",
      "narios of AI-driven destruction to distract from pressing concerns and to shape rules to fit their \n",
      "interests, potentially marginalizing the concerns of smaller entities and the public.\n",
      "  However, past innovations \n",
      "ultimately spawned new occupations, suggesting long-term realignment.\n",
      "\n",
      "  Developed regions are likely to witness faster uptake, displacing administrative, creative, and \n",
      "analytical roles initially.\n",
      "  And braiding priorities of sustainability, equity, and human \n",
      "dignity throughout this transformation promises optimizing empowerment over exploitation.\n",
      "\n",
      "----------------------------------------\n",
      "Page 334:\n",
      "Topics:\n",
      "  The long-term market impact and the winning generative \n",
      "AI business models have yet to unfold.\n",
      "\n",
      "  Technical experts like \n",
      "data scientists and programmers will remain key to developing AI tools and realizing their full \n",
      "business potential.\n",
      "  •\t\n",
      "Teachers will utilize AI for course preparation and personalized student support.\n",
      "•\t\n",
      "Journalists, paralegals, and graphic designers will employ generative AI to enhance content \n",
      "creation, raising concerns about job impacts.\n",
      "\n",
      "  •\t\n",
      "Demand will grow for experts in AI ethics, regulations, and security to oversee responsible \n",
      "development.\n",
      "•\t\n",
      "Musicians and artists will collaborate with AI, boosting creative expression and acces-\n",
      "sibility.\n",
      "\n",
      "  Concerns have emerged about saturation as generative AI tools are relatively easy to build using \n",
      "foundation models.\n",
      "  While current \n",
      "market hype is high, investors are tempering decisions given lower valuations and skepticism \n",
      "following the 2021 AI boom/bust cycle.\n",
      "  •\t\n",
      "Striking an optimal balance between AI capabilities and human judgment will be vital \n",
      "across sectors.\n",
      "\n",
      "  While certain jobs may be displaced by AI in the near term, especially routine cognitive tasks, it \n",
      "may automate certain activities rather than eliminate entire occupations.\n",
      "  Chapter 10\n",
      "311\n",
      "Here are some key predictions about how jobs may be impacted by advances in language models \n",
      "and generative AI:\n",
      "•\t\n",
      "Routine legal work like draft preparation will be increasingly automated, changing job \n",
      "roles for junior lawyers and paralegals.\n",
      "•\t\n",
      "Software engineering will see a rise in AI coding assistants handling mundane tasks, en-\n",
      "abling developers to focus on complex problem-solving.\n",
      "\n",
      "  •\t\n",
      "The common thread is that while routine tasks face increasing automation, human ex-\n",
      "pertise to steer AI directions and ensure responsible outcomes will remain indispensable.\n",
      "\n",
      "  •\t\n",
      "Data scientists will spend more time refining AI systems rather than building predictive \n",
      "models from scratch.\n",
      "•\t\n",
      "Demand for specialized roles like prompt engineering will continue to rise.\n",
      "\n",
      "Subtopics:\n",
      "  Customization of models and tools will allow value creation, but it’s unclear \n",
      "who will capture the most upsides and how powerful these applications can be.\n",
      "  By automating rote tasks, models may free up human time for higher-value \n",
      "work, boosting economic output.\n",
      "\n",
      "----------------------------------------\n",
      "Page 335:\n",
      "Topics:\n",
      "  In Chapter 1, What Is Generative AI?, we discussed the pertinent trend in the AI industry that en-\n",
      "compasses gains in efficiency stemming from the iterative refinement of code, the development \n",
      "of sophisticated tools, and the enhancement of techniques.\n",
      "  The improved efficiency because of \n",
      "new techniques and approaches, combined with the declining hardware costs, fosters a virtu-\n",
      "ous cycle: as costs diminish, AI adoption widens, in turn spurring further cost reductions and \n",
      "efficiency improvements.\n",
      "  •\t\n",
      "Key factors: Excessive hype, unrealistic growth projections, historically high \n",
      "valuations in 2021, and broader economic conditions all contributed to the \n",
      "boom-bust cycle.\n",
      "  The 2021 AI boom/bust cycle refers to a rapid acceleration in investment and growth \n",
      "in the AI start-up space followed by a market cooldown and stabilization in 2022 as \n",
      "projections failed to materialize and valuations declined.\n",
      "\n",
      "  Hundreds of AI start-ups were \n",
      "founded and funded during this period.\n",
      "\n",
      "  The Future of Generative Models\n",
      "312\n",
      "As AI models become more sophisticated and economical to operate, we can anticipate a sub-\n",
      "stantial proliferation of generative AI and LLM applications into novel domains.\n",
      "  Here’s a quick summary:\n",
      "•\t\n",
      "Boom phase (2020-2021): There was huge interest and skyrocketing invest-\n",
      "ment in AI start-ups offering innovative capabilities like computer vision, \n",
      "natural language processing, robotics, and machine learning platforms.\n",
      "  •\t\n",
      "Bust phase (2022): In 2022, the market underwent a correction, with val-\n",
      "uations of AI start-ups falling significantly from their 2021 highs.\n",
      "  To-\n",
      "tal funding for AI start-ups hit record levels in 2021, with over $73 billion \n",
      "invested globally according to Pitchbook.\n",
      "  Several \n",
      "high-profile AI start-ups like Anthropic and Cohere faced valuation mark-\n",
      "downs.\n",
      "  What emerges is a feedback loop where each iteration of efficiency \n",
      "catalyzes increased usage, which in itself leads to even greater efficiency – a dynamic poised to \n",
      "dramatically advance the frontier of AI capabilities.\n",
      "\n",
      "  Beyond just the \n",
      "plummeting hardware expenses that have historically followed Moore’s Law, there are additional \n",
      "economies of scale affecting AI systems.\n",
      "\n",
      "  Many investors became more cautious and selective with funding \n",
      "AI start-ups.\n",
      "Subtopics:\n",
      "  The cycle followed a classic pattern seen previously in \n",
      "sectors like dot-com and blockchain.\n",
      "\n",
      "  Market corrections in the broader tech sector also contributed \n",
      "to the bust.\n",
      "\n",
      "----------------------------------------\n",
      "Page 336:\n",
      "Topics:\n",
      "  Chapter 10\n",
      "313\n",
      "Let’s look at various sectors where generative models will have profound near-term impacts, \n",
      "starting with creative endeavors.\n",
      "\n",
      "  AI screenwriting \n",
      "tools analyze data to generate optimized scripts.\n",
      "  Other applications include Bloomberg News’ Bulletin service where chatbots create personalized \n",
      "one-sentence news summaries.\n",
      "  AIGC also enables AI news anchors that co-present broadcasts \n",
      "with real anchors by mimicking human appearance and speech from text input.\n",
      "  In post-production, AI color grading and editing tools like Colourlab \n",
      "AI and Descript simplify processes like color correction using algorithms.\n",
      "\n",
      "  For media, film, and advertising, AI unlocks new scales of personalized, dynamic content creation. \n",
      "\n",
      "  Robot reporters like the Los Angeles Times \n",
      "Quakebot can swiftly produce articles on breaking news.\n",
      "\n",
      "  Chinese news \n",
      "agency Xinhua’s virtual presenter Xin Xiaowei is an example, presenting broadcasts from different \n",
      "angles for an immersive effect.\n",
      "\n",
      "  Visual effects teams blend AI-enhanced digital \n",
      "environments and de-aging with live footage for immersive visuals.\n",
      "  AI-Generated Content (AIGC) is playing a growing role in \n",
      "transforming media production and delivery by enhancing efficiency and diversity.\n",
      "  AI also powers automated subtitle generation, even predicting dialogue in silent films by training \n",
      "models on extensive audio samples.\n",
      "  Creative industries and advertising\n",
      "The gaming and entertainment industries are leveraging generative AI to craft uniquely immersive \n",
      "user experiences.\n",
      "  Media outlets like the Associated Press \n",
      "generate thousands of stories per year using AIGC.\n",
      "Subtopics:\n",
      "  In journalism, \n",
      "text generation tools automate writing tasks traditionally done by human reporters, significant-\n",
      "ly boosting productivity while maintaining timeliness.\n",
      "  This expands accessibility via subtitles and recreates voice-\n",
      "overs synchronized to scenes.\n",
      "  In journalism, automated article generation using massive datasets can free up reporters to focus \n",
      "on more complex investigative stories.\n",
      "  This has implications for creative \n",
      "industries, as it can enhance the creative process and potentially create new revenue streams.\n",
      "  Major efficiency gains from automating creative tasks could increase leisure time \n",
      "spent online.\n",
      "  Generative AI can enable machines to generate new and original content, such as art, \n",
      "music, and literature, by learning from patterns and examples.\n",
      "  It \n",
      "also unlocks new scales of personalized, dynamic content creation for media, film, and advertising.\n",
      "\n",
      "  AIGC is transforming movie creation from screenwriting to post-production.\n",
      "  Deep fake technology rec-\n",
      "reates or revives characters convincingly.\n",
      "\n",
      "----------------------------------------\n",
      "Page 337:\n",
      "Topics:\n",
      "  AI also assists in advertising creativity and design – tools like Vinci produce customized attrac-\n",
      "tive posters from product images and slogans, while companies like Brandmark.io generate logo \n",
      "variations based on user preferences.\n",
      "  LANDR’s AI mastering uses machine learning to process and improve \n",
      "digital audio quality for musicians.\n",
      "\n",
      "  GAN technologies automate product listing generation \n",
      "with keywords for effective peer-to-peer marketing.\n",
      "  Animation tools like Adobe’s Character Animator and Anthropic’s Claude can help with the gen-\n",
      "eration of customized characters, scenes, and motion sequences, opening animation potential \n",
      "for non-professionals.\n",
      "  The Future of Generative Models\n",
      "314\n",
      "In advertising, AIGC unlocks new potential for efficient, customized advertising creativity and \n",
      "personalization.\n",
      "  For all these applications, advanced AI expands creative possibilities through both generative \n",
      "content and data-driven insights.\n",
      "  DeepDream’s \n",
      "algorithm imposes patterns on images, creating psychedelic art.\n",
      "  ControlNet adds constraints to steer diffusion models, increasing output \n",
      "variability.\n",
      "\n",
      "  In music, tools like Google’s Magenta, IBM’s Watson Beat, and Sony CSL’s Flow Machine can \n",
      "generate original melodies and compositions.\n",
      "  In visual arts, MidJourney uses neural networks to generate inspirational images that can kick-\n",
      "start painting projects.\n",
      "  AI painting conservation analyzes artwork to digitally \n",
      "repair damage and restore pieces.\n",
      "\n",
      "  Platforms like Creative Advertising System (CAS) and \n",
      "Smart Generation System Personalized Advertising Copy (SGS-PAC) leverage data to automat-\n",
      "ically generate ads with messaging targeted to specific user needs and interests.\n",
      "\n",
      "Subtopics:\n",
      "  AIVA similarly creates unique compositions from \n",
      "parameters tuned by users.\n",
      "  In all cases, quality control and properly attributing the contri-\n",
      "butions of human artists, developers, and training data remains an ongoing challenge as adoption \n",
      "spreads.\n",
      "\n",
      "  GANs can generate abstract \n",
      "paintings converging on a desired style.\n",
      "  AI-generated content allows advertisers to create personalized, engaging ads \n",
      "tailored to individual consumers at scale.\n",
      "  Synthetic ad production is also on the rise, \n",
      "enabling highly personalized, scalable campaigns that save time.\n",
      "\n",
      "  Artists have used its outputs to create prize-winning works.\n",
      "----------------------------------------\n",
      "Page 338:\n",
      "Topics:\n",
      "  Virtual simulations powered by generative AI can also \n",
      "create engaging, tailored learning experiences adapted to different learners’ needs and interests.\n",
      "\n",
      "  Chapter 10\n",
      "315\n",
      "Education\n",
      "One potential near-future scenario is that the rise of personalized AI tutors and mentors could \n",
      "democratize access to education for high-demand skills aligned with an AI-driven economy.\n",
      "  Governments should promote equal access to prevent \n",
      "generative AI from becoming a privilege of the affluent.\n",
      "  Overall, properly \n",
      "implemented AI tools promise to boost legal productivity and access to justice while requiring \n",
      "ongoing scrutiny regarding reliability and ethics.\n",
      "\n",
      "  Law\n",
      "Generative models like LLMs can automate routine legal tasks such as contract review, documen-\n",
      "tation generation, and brief preparation.\n",
      "  In \n",
      "the education sector, generative AI is already transforming how we teach and learn.\n",
      "  While AI tutors tailored to each student could enhance outcomes and engagement, poorer schools \n",
      "may be left behind, worsening inequality.\n",
      "Subtopics:\n",
      "  However, responsible and ethical use remains \n",
      "critical given considerations around transparency, fairness, and accountability.\n",
      "  Additional applications include explaining complex legal concepts in plain language \n",
      "and predicting litigation outcomes using case data.\n",
      "  Tools like \n",
      "ChatGPT can be used to automatically generate personalized lessons and customized content for \n",
      "individual students.\n",
      "  Interactive AI assistants that adapt courses to \n",
      "students’ strengths, needs, and interests could make learning efficient, engaging, and equitable. \n",
      "\n",
      "  They also enable faster, comprehensive legal research \n",
      "and analysis.\n",
      "  The accelerating pace of knowledge and the obsolescence \n",
      "of scientific findings mean that training children’s curiosity-driven learning should focus on \n",
      "developing the cognitive mechanisms involved in initiating and sustaining curiosity, such as \n",
      "awareness of knowledge gaps and the use of appropriate strategies to resolve them.\n",
      "\n",
      "  Democratizing opportunity for all stu-\n",
      "dents remains vital.\n",
      "\n",
      "  If implemented thoughtfully, personalized AI-powered education could make crucial skills ac-\n",
      "quisition accessible to anyone motivated to learn.\n",
      "  However, risks around perpetuating biases and spreading misinformation need to be studied \n",
      "further as these technologies evolve.\n",
      "  AI tutors provide real-time feedback on student writing assignments, freeing up \n",
      "teachers to focus on more complex skills.\n",
      "  However, challenges around access, biases, and socialization need addressing.\n",
      "\n",
      "  This reduces instructor workloads substantially by automating repetitive \n",
      "teaching tasks.\n",
      "----------------------------------------\n",
      "Page 339:\n",
      "Topics:\n",
      "  Military\n",
      "Militaries worldwide are investing in research to develop Lethal Autonomous Weapons Systems \n",
      "(LAWS).\n",
      "  Even with sophisticated \n",
      "AI, complex factors in war like proportionality and distinction between civilians and combatants \n",
      "require human judgment.\n",
      "\n",
      "  New techniques with neural networks are already employed to lower long-read DNA sequencing \n",
      "error rates (Baid and colleagues; DeepConsensus improves the accuracy of sequences with a gap-aware \n",
      "sequence transformer, September 2022), and, according to a report by ARK Investment Manage-\n",
      "ment (2023), in the short term, technology like this can make it already possible to deliver the \n",
      "first high-quality, whole long-read genome for less than $1,000.\n",
      "  The Future of Generative Models\n",
      "316\n",
      "Manufacturing\n",
      "In the automotive sector, generative models are employed to generate 3D environments for simu-\n",
      "lations and aid in the development of cars.\n",
      "  Additionally, generative AI is utilized for road-testing \n",
      "autonomous vehicles using synthetic data.\n",
      "Subtopics:\n",
      "  Medicine\n",
      "A model that can accurately predict physical properties from gene sequences would represent a \n",
      "major breakthrough in medicine and could have profound impacts on society.\n",
      "  Machines can process information and react faster than humans, removing emotion \n",
      "from lethal decisions.\n",
      "  It could further \n",
      "accelerate drug discovery and precision medicine, enable earlier disease prediction and prevention, \n",
      "provide a deeper understanding of complex diseases, and improve gene therapies.\n",
      "  If deployed, completely autonomous lethal weapons would represent an alarming step toward \n",
      "relinquishing control over life-and-death decisions.\n",
      "  They could violate international humanitarian \n",
      "law or be used by despotic regimes to terrorize populations.\n",
      "  However, this raises significant moral questions.\n",
      "  Robots and drones can identify targets and deploy lethal force without any human su-\n",
      "pervision.\n",
      "  This means that large-scale \n",
      "gene-to-expression models might not be far away either.\n",
      "\n",
      "  However, it also \n",
      "raises major ethical concerns around genetic engineering and could exacerbate social inequalities.\n",
      "\n",
      "  Once unleashed fully independently, \n",
      "the actions of autonomous killer robots would be impossible to predict or restrain.\n",
      "\n",
      "  These models can also process object information \n",
      "to comprehend the surrounding environment, understand human intent through dialogues, \n",
      "generate natural language responses to human input, and create manipulation plans to assist \n",
      "humans in various tasks.\n",
      "\n",
      "  Allowing machines to \n",
      "determine whether lives should be taken crosses a troubling threshold.\n",
      "----------------------------------------\n",
      "Page 340:\n",
      "Topics:\n",
      "  The rise of generative AI represents a significant milestone within a broader societal trend of how \n",
      "creative content is being generated and consumed.\n",
      "  Chapter 10\n",
      "317\n",
      "The advent of highly capable generative AI will likely transform many aspects of society in the \n",
      "coming years beyond the economics and the disruption of certain jobs.\n",
      "  Mar-\n",
      "keting efforts can be adapted to specific customer segments and local tastes while maintaining \n",
      "consistency and scale.\n",
      "\n",
      "  However, the capacity of generative AI to synthesize and remix copyrighted materials at scale \n",
      "presents intricate legal and ethical challenges.\n",
      "  Generative AI offers immense potential benefits across personal, societal, and industrial realms if \n",
      "deployed thoughtfully.\n",
      "  Existing tools struggle to identify content generated by AI, \n",
      "which complicates efforts to apply traditional copyright and authorship principles.\n",
      "  From a consumer standpoint, generative AI has the potential to deliver unprecedented person-\n",
      "alization.\n",
      "  This dilemma \n",
      "underscores the urgent need for legal frameworks that can keep pace with technological advances \n",
      "and navigate the complex interplay between rights-holders and AI-generated content.\n",
      "\n",
      "  Societal implications\n",
      "As generative models continue to develop and add value to businesses and creative projects, \n",
      "generative AI will shape the future of technology and human interaction across domains.\n",
      "  While \n",
      "their widespread adoption brings forth numerous benefits and opportunities for businesses and \n",
      "individuals, it is crucial to address the ethical and societal concerns that arise from increasing \n",
      "reliance on AI models in various fields.\n",
      "\n",
      "Subtopics:\n",
      "  Generative AI fits naturally \n",
      "within this paradigm by creating new content through the recombination of existing digital \n",
      "materials, promoting the ethos of shared, iterative creation.\n",
      "\n",
      "  Recommendation systems can fine-tune their outputs to individual preferences.\n",
      "  At a personal level, these models can enhance creativity and productivity, \n",
      "and increase accessibility to services like healthcare, education, and finance.\n",
      "  By democratizing \n",
      "access to knowledge resources, they can help students learn or aid professionals in making deci-\n",
      "sions by synthesizing expertise.\n",
      "  The internet has already nurtured a culture \n",
      "of remixing, where derivative works and co-creation are the norms.\n",
      "  The training of these models on extensive corpora \n",
      "that encompass literature, articles, images, and other copyrighted works creates a tangled web \n",
      "for attribution and compensation.\n",
      "  Let’s think a bit more \n",
      "broadly about the societal impact!\n",
      "\n",
      "  As virtual assistants, they provide instant, customized information \n",
      "to facilitate routine tasks.\n",
      "\n",
      "----------------------------------------\n",
      "Page 341:\n",
      "Topics:\n",
      "  AI could help or harm \n",
      "security depending on whether it is used responsibly.\n",
      "  For example, during the \n",
      "COVID-19 pandemic, the spread of misinformation and infodemics has been a major challenge. \n",
      "\n",
      "  Though no single fix exists, \n",
      "collective efforts promoting responsible AI development can help democratic societies address \n",
      "emerging threats.\n",
      "\n",
      "  The Future of Generative Models\n",
      "318\n",
      "One of the major problems that I can see is misinformation, either in the interest of political \n",
      "interest groups, foreign actors, or large corporations.\n",
      "  There are significant threats associated with AI techniques like micro-targeting and deepfakes. \n",
      "\n",
      "  Let’s discuss this threat!\n",
      "Misinformation and cybersecurity\n",
      "AI presents a dual-edged sword against disinformation.\n",
      "  AI can influence public opinion and sway elections.\n",
      "\n",
      "  Powerful AI can profile users psychologically to deliver personalized disinformation that facili-\n",
      "tates concealed manipulation, escaping broad examination.\n",
      "  Let’s talk more about regulations!\n",
      "\n",
      "  Big Data and AI could be leveraged to \n",
      "exploit psychological vulnerabilities and infiltrate online forums to attack and spread conspiracy \n",
      "theories.\n",
      "\n",
      "Subtopics:\n",
      "  AI can be used by political parties, governments, criminal groups, and even \n",
      "the legal system to launch lawsuits and/or extract money.\n",
      "\n",
      "  It can also generate fake audio/video content to damage reputations and sow confusion.\n",
      "  Disinformation has transformed into a multifaceted phenomenon, involving biased information, \n",
      "manipulation, propaganda, and intent to influence political behavior.\n",
      "  While it enables scalable detection, auto-\n",
      "mation makes it easier to spread sophisticated, personalized propaganda.\n",
      "  This likely will have far-reaching consequences in various domains.\n",
      "  A significant portion of in-\n",
      "ternet users may be obtaining the information they need without accessing external websites. \n",
      "\n",
      "  Careful governance and digital literacy are essential to build resilience.\n",
      "  It increases vulnerabilities to misinformation \n",
      "along with cyberattacks using generative hacking and social engineering.\n",
      "\n",
      "  There is a danger of large corporations being the gatekeepers of information and controlling \n",
      "public opinion, effectively being able to restrict certain actions or viewpoints.\n",
      "\n",
      "  State \n",
      "and non-state actors are weaponizing these capabilities for propaganda to damage reputations \n",
      "and sow confusion.\n",
      "----------------------------------------\n",
      "Page 342:\n",
      "Topics:\n",
      "  The consequence of AI bias includes potential harm to individuals or groups due to biased de-\n",
      "cisions made by AI systems.\n",
      "  Open-source models will continue to thrive, and \n",
      "local legislation in the EU and other countries will push for transparent use of AI.\n",
      "\n",
      "  Incorporating ethics training into computer science curricula can \n",
      "help reduce biases in AI code.\n",
      "  Local legislation in EU \n",
      "countries, for example, such as the European Commission’s proposal for harmonized rules on AI \n",
      "regulation, will drive more ethical use of language and imagery.\n",
      "\n",
      "  AI bias prevention is a long-term priority for many organizations; \n",
      "however, without legislation driving it, it can take time to be introduced.\n",
      "  •\t\n",
      "Oversight and regulations: Calls are mounting for oversight to ensure non-discrimina-\n",
      "tion, accuracy, and accountability from advanced AI systems.\n",
      "  Copyright laws remain ambiguous regarding AI-generated content.\n",
      "  To stay on \n",
      "the right path, organizations need to prioritize transparency, accountability, and guardrails to \n",
      "prevent bias in their AI systems.\n",
      "  •\t\n",
      "Ethics:\n",
      "  Chapter 10\n",
      "319\n",
      "Regulations and implementation challenges\n",
      "Realizing the potential of generative AI in a responsible manner involves addressing a number of \n",
      "practical legal, ethical, and regulatory issues:\n",
      "•\t\n",
      "Legal:\n",
      "  •\t\n",
      "Data protection:\n",
      "Subtopics:\n",
      "  Replicating copy-\n",
      "righted data in training also raises fair use debates that need clarification.\n",
      "\n",
      "  Collecting, processing, and storing the massive datasets required to train \n",
      "advanced models creates data privacy and security risks.\n",
      "  Governance models ensuring \n",
      "consent, anonymity, and safe access are vital.\n",
      "\n",
      "  Frameworks guiding development toward beneficial outcomes are indispensable. \n",
      "\n",
      "  Integrating ethics through design practices focused on transparency, explicability, and \n",
      "human oversight helps build trust.\n",
      "\n",
      "  There is a growing demand for algorithmic transparency.\n",
      "  However, there is \n",
      "resistance from these companies and developers, who argue that disclosing proprietary informa-\n",
      "tion would harm their competitive advantage.\n",
      "  Overall, proactive collaboration between policymakers, researchers, and civil society is essential \n",
      "to settle unresolved issues around rights, ethics, and governance.\n",
      "  Who owns the \n",
      "output – the model creator, training data contributors, or end users?\n",
      "  With pragmatic guardrails in \n",
      "place, generative models can fulfill their promise while mitigating harm.\n",
      "\n",
      "  This means that tech companies and \n",
      "developers should reveal the source code and inner workings of their systems.\n",
      "  However, flexible policies \n",
      "balancing innovation and risk are needed rather than burdensome bureaucracy.\n",
      "\n",
      "  By teaching developers how to build applications that are ethical \n",
      "by design, the probability of biases being embedded into the code can be minimized.\n",
      "----------------------------------------\n",
      "Page 343:\n",
      "Topics:\n",
      "  With responsible implementation, \n",
      "generative AI could propel growth, creativity, and accessibility in a more prosperous society. \n",
      "\n",
      "  Let’s conclude this chapter!\n",
      "\n",
      "  •\t\n",
      "Promoting access and inclusion: Equitable access to resources, relevant education, and \n",
      "myriad opportunities concerning AI is key to negating the amplification of disparities. \n",
      "\n",
      "  •\t\n",
      "Upholding democratic norms: Collaborative discussions, communal efforts, and reaching \n",
      "a compromise will inevitably prove more constructive in defining the future course of AI, \n",
      "as compared to unilateral decrees imposed by a solitary entity.\n",
      "  The Future of Generative Models\n",
      "320\n",
      "A current German law on fake news, which imposes a 24-hour timeframe for platforms to remove \n",
      "fake news and hate speech, is impractical for both large and small platforms.\n",
      "  Representativeness and diversity should be prioritized.\n",
      "\n",
      "  •\t\n",
      "The human-AI symbiosis: Rather than striving for outright automation, more advanta-\n",
      "geous systems would integrate and complement the creative prowess of humans with the \n",
      "productive efficiency of AI.\n",
      "Subtopics:\n",
      "  Policymakers may need to implement guardrails preventing misuse while pro-\n",
      "viding workers with support to transition as activities shift.\n",
      "  To maximize benefits, companies need to ensure human oversight, diversity, and transparency \n",
      "in development.\n",
      "  •\t\n",
      "Preventive measures and risk management: Constant evaluation of freshly emerging \n",
      "capabilities via interdisciplinary insights is necessary to evade future dangers.\n",
      "  Excessive \n",
      "apprehensions, however, should not impede potential progress.\n",
      "\n",
      "  Such a hybrid model will ensure optimal oversight.\n",
      "\n",
      "  Moreover, excessively slow developments could stifle innova-\n",
      "tion, suggesting that determining an ideal pace through encompassing public discourse \n",
      "is crucial.\n",
      "\n",
      "  Addressing potential risks early on and ensuring a just distribution of benefits designed to serve \n",
      "public welfare will cultivate a sense of trust among stakeholders, such as:\n",
      "•\t\n",
      "The dynamics of progress: Fine-tuning the pace of transformation is critical to avoid any \n",
      "undesired repercussions.\n",
      "  Additionally, the \n",
      "limited resources of smaller platforms make it unrealistic for them to police all content.\n",
      "  Public interest must take \n",
      "precedence.\n",
      "\n",
      "  More nuanced policies are needed that balance free speech, \n",
      "accountability, and feasibility for a diversity of technology platforms to comply.\n",
      "  Broader collaboration between government, civil society, academics, and industry can \n",
      "develop more effective frameworks to counter misinformation while protecting rights.\n",
      "\n",
      "  Relying solely on \n",
      "private companies to regulate online content raises concerns about a lack of oversight and due \n",
      "process.\n",
      "  Further, \n",
      "online platforms should not have the sole authority to determine what is considered truth, as this \n",
      "could lead to excessive censorship.\n",
      "----------------------------------------\n",
      "Page 344:\n",
      "Topics:\n",
      "  There are also philosophical debates around whether AI should be creating art, literature, or music \n",
      "that has historically reflected the human condition.\n",
      "\n",
      "  The forthcoming era of generative AI models offers a plethora of intriguing opportunities and \n",
      "unparalleled progression, yet it is interspersed with numerous uncertainties.\n",
      "  Chapter 10\n",
      "321\n",
      "The road ahead\n",
      "\n",
      "  As discussed in this \n",
      "book, many breakthroughs have been accomplished in recent months, but successive challenges \n",
      "continue to linger, mainly pertaining to precision, reasoning ability, controllability, and entrenched \n",
      "bias within these models.\n",
      "  Their work is being automated while low-skilled \n",
      "workers learn to leverage AI as a superpower.\n",
      "\n",
      "  While grandiose claims of superintelligent AI on the horizon may seem \n",
      "hyperbolic, consistent trends predict sophisticated capabilities sprouting within a few decades.\n",
      "\n",
      "  Most ominously, AI could be weaponized by militaries, \n",
      "terrorists, criminals, and governments for propaganda and influence.\n",
      "  On the positive side, AI can democratize skills, allowing amateurs to produce professional quality \n",
      "output in design, writing, and other areas.\n",
      "  Unlike physical automation of the past, \n",
      "generative AI threatens cognitive job categories previously considered safe from automation. \n",
      "\n",
      "Subtopics:\n",
      "  However, there are major concerns about job losses, especially for specialized middle-class \n",
      "roles like graphic designers, lawyers, and doctors.\n",
      "  On a technical level, generative models like ChatGPT often function as black boxes, with limited \n",
      "transparency into their decision-making processes.\n",
      "  On a practical level, generative \n",
      "models require extensive computational resources for training and deployment; however, we \n",
      "discussed developments and trends that change that.\n",
      "\n",
      "  The sheer pace of advancement creates unease surrounding human obsolescence and job dis-\n",
      "placement, which could further divide economic classes.\n",
      "  However, the proliferation of generative content raises valid concerns about misinformation, \n",
      "plagiarism in academia, and impersonation in online spaces.\n",
      "  A lack of model interpretability makes it \n",
      "difficult to fully understand model behavior or to control outputs.\n",
      "  Deepfakes produced in real-time \n",
      "will proliferate scams and erode trust.\n",
      "  There are also concerns about \n",
      "potential biases that could emerge from imperfect training data.\n",
      "  There are also fears about \n",
      "generative models exacerbating social media addiction due to their ability to produce endless \n",
      "customized content.\n",
      "\n",
      "  Businesses can benefit from faster, cheaper, on-demand \n",
      "work.\n",
      "  As these models become more adept \n",
      "at mimicking human expression, people may have difficulty discerning what is human-gener-\n",
      "ated versus AI-generated, enabling new forms of deception.\n",
      "  Managing this workforce transition ethically and equitably will require foresight and planning. \n",
      "\n",
      "----------------------------------------\n",
      "Page 345:\n",
      "Topics:\n",
      "  Join our community on Discord\n",
      "Join our community’s Discord space for discussions with the authors and other readers:\n",
      "https://packt.link/lang\n",
      "\n",
      "  As AI is entrusted with more \n",
      "consequential decisions, alignment with human values becomes critical.\n",
      "  Looking decades ahead, perhaps the deepest challenges are ethical.\n",
      "  The Future of Generative Models\n",
      "322\n",
      "For corporations, effective governance frameworks have yet to be established around acceptable \n",
      "use cases.\n",
      "Subtopics:\n",
      "  Generative models amplify risks of misuse, ranging from creating misinformation such \n",
      "as deepfakes to generating unsafe medical advice.\n",
      "  The goal should be \n",
      "to empower human potential, not mere technological advancement.\n",
      "\n",
      "  Legal questions around content licensing and \n",
      "intellectual property arise.\n",
      "  While generative models can enhance business productivity, quality \n",
      "control and bias mitigation incur costs.\n",
      "\n",
      "  While accuracy, rea-\n",
      "soning ability, controllability, and mitigating bias remain technical priorities, other priorities \n",
      "should include fortifying model robustness, promoting transparency, and ensuring alignment \n",
      "with human values.\n",
      "\n",
      "  While future capabilities remain uncertain, proactive governance and democratization of access \n",
      "are essential to direct these technologies toward equitable, benevolent outcomes.\n",
      "  Collaboration \n",
      "between researchers, policymakers, and civil society around issues of transparency, accountability, \n",
      "and ethics can help align emerging innovations with shared human values.\n",
      "----------------------------------------\n",
      "Page 346:\n",
      "Topics:\n",
      "  Why subscribe?\n",
      "•\t\n",
      "Spend less time learning and more time coding with practical eBooks and Videos from \n",
      "over 4,000 industry professionals\n",
      "•\t\n",
      "Improve your learning with Skill Plans built especially for you\n",
      "•\t\n",
      "Get a free eBook or video every month\n",
      "•\t\n",
      "Fully searchable for easy access to vital information\n",
      "•\t\n",
      "Copy and paste, print, and bookmark content\n",
      "At www.packt.com, you can also read a collection of free technical articles, sign up for a range of \n",
      "free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n",
      "\n",
      "  packt.com\n",
      "Subscribe to our online digital library for full access to over 7,000 books and videos, as well as \n",
      "industry leading tools to help you plan your personal development and advance your career.\n",
      "Subtopics:\n",
      "  For \n",
      "more information, please visit our website.\n",
      "\n",
      "----------------------------------------\n",
      "Page 347:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 348:\n",
      "Topics:\n",
      "  Other Books \n",
      " \n",
      "You May Enjoy\n",
      "If you enjoyed this book, you may be interested in these other books by Packt:\n",
      "\n",
      "  Transformers for Natural Language Processing and Computer Vision\n",
      "Denis Rothman\n",
      "ISBN: 9781805128724\n",
      "•\t\n",
      "Master the art of fine-tuning models and engineering effective prompts\n",
      "•\t\n",
      "Tackle examples of LLM risks by delving into strategies to mitigate them\n",
      "•\t\n",
      "Learn about the potential functional AGI capabilities of foundation models\n",
      "•\t\n",
      "Visualize transformer model activity for deeper insights using BertViz, LIME, and SHAP\n",
      "•\t\n",
      "Create and implement cross-platform chained models, such as HuggingGPT\n",
      "•\t\n",
      "Skyrocket your productivity with an automated generative ideation process\n",
      "•\t\n",
      "Go in-depth into vision transformers with CLIP, DALL-E 2, DALL-E 3, and GPT-4V\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 349:\n",
      "Topics:\n",
      "  Other Books You May Enjoy\n",
      "326\n",
      "Building LLM Apps\n",
      "Valentina Alto\n",
      "ISBN: 9781835462317\n",
      "•\t\n",
      "Core components of LLMs’ architecture, including encoder-decoders blocks, embedding \n",
      "and so on\n",
      "•\t\n",
      "Get well-versed with unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\n",
      "•\t\n",
      "Use AI orchestrators like LangChain, and Streamlit as frontend\n",
      "•\t\n",
      "Get familiar with LLMs components such as memory, prompts and tools\n",
      "•\t\n",
      "Learn non-parametric knowledge, embeddings and vector databases\n",
      "•\t\n",
      "Understand the implications of LFMs for AI research, and industry applications\n",
      "•\t\n",
      "Customize your LLMs with fine tuning\n",
      "•\t\n",
      "Learn the ethical implications of LLM-powered applications\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 350:\n",
      "Topics:\n",
      "  Other Books You May Enjoy\n",
      "327\n",
      "Generative AI Engineering\n",
      "Konrad Banachewicz\n",
      "ISBN: 9781805120513\n",
      "•\t\n",
      "Get to grips with the fundamentals of generative AI and its applications\n",
      "•\t\n",
      "Familiarize yourself with different types of generative models and when to use them\n",
      "•\t\n",
      "Train and Finetune generative models using PyTorch\n",
      "•\t\n",
      "Evaluate the performance of your models and fine-tune them for optimal results\n",
      "•\t\n",
      "Find best practices for deploying and scaling generative AI models in production envi-\n",
      "ronments\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 351:\n",
      "Topics:\n",
      "  Share your thoughts\n",
      "Now you’ve finished Generative AI with LangChain, we’d love to hear your thoughts!\n",
      "  Other Books You May Enjoy\n",
      "328\n",
      "Packt is searching for authors like you\n",
      "If you’re interested in becoming an author for Packt, please visit authors.packtpub.com and \n",
      "apply today.\n",
      "  If you pur-\n",
      "chased the book from Amazon, please click here to go straight to the Amazon review \n",
      "page for this book and share your feedback or leave a review on the site that you purchased it from.\n",
      "\n",
      "  We have worked with thousands of developers and tech professionals, just like you, \n",
      "to help them share their insight with the global tech community.\n",
      "Subtopics:\n",
      "  You can make a general appli-\n",
      "cation, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n",
      "\n",
      "  Your review is important to us and the tech community and will help us make sure we’re deliv-\n",
      "ering excellent quality content.\n",
      "\n",
      "----------------------------------------\n",
      "Page 352:\n",
      "Topics:\n",
      "  Index\n",
      "Symbols\n",
      "2021 AI boom/bust cycle  312\n",
      "A\n",
      "AgentOps  261\n",
      "agents  52, 53\n",
      "benefits  52\n",
      "AI, for software development\n",
      "code LLMs  175-179\n",
      "AI-Generated Content (AIGC)  313\n",
      "alignment  226\n",
      "Annoy (Approximate Nearest Neighbors\n",
      "  algorithm  141\n",
      "Anthropic  85\n",
      "API model integrations\n",
      "Anthropic  85\n",
      "Azure  84\n",
      "exploring  69-72\n",
      "fake LLM  72, 73\n",
      "Google Cloud Platform  77-79\n",
      "Hugging Face  75, 76\n",
      "Jina AI  80-82\n",
      "OpenAI  73, 75\n",
      "Replicate  82-84\n",
      "application, for customer service\n",
      "building  89-95\n",
      "Application Programming Interface (APIs)  69\n",
      "Approximate Nearest Neighbor \n",
      " (ANN)  59, 141\n",
      "Argilla  289, 290\n",
      "Artificial General Intelligence (AGI)  308-310\n",
      "Artificial Intelligence (AI)  1, 5, 144\n",
      "using, for software development  174, 175\n",
      "arXiv  117\n",
      "automated data science  207, 209\n",
      "AutoML  211-213\n",
      "data collection  209\n",
      "LLMs and generative AI benefits  206\n",
      "preprocessing and feature extraction  210\n",
      "visualization and EDA  210\n",
      "Automated Machine Learning  \n",
      "(AutoML)  211-213\n",
      "Automatic Speech Recognition (ASR)  33\n",
      "Azure  84\n",
      "B\n",
      "base model  15\n",
      "Big Bang of DL  10\n",
      "black-box scenario  212\n",
      "\n",
      "Subtopics:\n",
      "  Oh \n",
      "Yeah)\n",
      "----------------------------------------\n",
      "Page 353:\n",
      "Topics:\n",
      "  Index\n",
      "330\n",
      "Boom Phase  312\n",
      "Bust Phase   312\n",
      "Byte-Pair Encoding (BPE)  25\n",
      "C\n",
      "Chain of Density (CoD)  105, 106\n",
      "Chain-of-Thought (CoT)  128\n",
      "Chain-of-Thought (CoT) \n",
      " prompting  169, 244, 248, 249\n",
      "chains  50, 51\n",
      "chatbot  132\n",
      "ELIZA  132\n",
      "PARRY  132\n",
      "responses, moderating  167-169\n",
      "use cases  133\n",
      "chatbot implementation  153\n",
      "document loader  154, 155\n",
      "memory  160\n",
      "vector storage  155-160\n",
      "ChatGPT  132\n",
      "Chinchilla scaling law  305\n",
      "Chroma  147, 148\n",
      "Claude and Claude 2  18\n",
      "ClearML  290\n",
      "code LLMs  175-179\n",
      "code, with LLMs\n",
      "Llama 2  186\n",
      "small local model  187-189\n",
      "StarChat  184-186\n",
      "StarCoder  179-184\n",
      "writing  179\n",
      "Comet.ml  289\n",
      "commercial models  241, 242\n",
      "Common European Framework of Reference \n",
      "for Languages (CEFR)  17\n",
      "Conda\n",
      "cons  66\n",
      "pros  66\n",
      "reference link  68\n",
      "using  68\n",
      "conditioning  226\n",
      "conditioning LLMs  226, 227\n",
      "methods  227, 228\n",
      "conditioning LLMs, methods\n",
      "inference-time conditioning  230-232\n",
      "low-rank adaptation  229, 230\n",
      "reinforcement learning, with human \n",
      "feedback  228\n",
      "contextual compression  156\n",
      "continuous integration and continuous \n",
      "delivery (CI/CD)  276\n",
      "ConversationSummaryMemory\n",
      "using  164\n",
      "convolutional neural network (CNN)  31, 136\n",
      "Creative Advertising System (CAS)  314\n",
      "D\n",
      "Datadog APM integration  290\n",
      "data exploration\n",
      "with LLMs  217-222\n",
      "DataRobot MLOps  290\n",
      "data science\n",
      "generative models, impact  204-206\n",
      "generative models impact, principal  \n",
      "areas  204\n",
      "questions, answering by agents  213-216\n",
      "use cases, for generative AI  204\n",
      "DeepEval  290\n",
      "Deep Learning (DL)  5\n",
      "Denoising Diffusion Implicit Model  \n",
      "(DDIM)  29\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 354:\n",
      "Topics:\n",
      "  Index\n",
      "331\n",
      "dependencies\n",
      "setting up  65-67\n",
      "DocArray  156\n",
      "Docker\n",
      "cons  66\n",
      "pros  66\n",
      "reference link  68\n",
      "using  68\n",
      "document loaders, LangChain  149, 150\n",
      "documents\n",
      "information, extracting from  112-115\n",
      "DuckDuckGo  117\n",
      "E\n",
      "economic consequences  310-312\n",
      "creative industries and advertising  313, 314\n",
      "education  315\n",
      "law  315\n",
      "manufacturing  316\n",
      "medicine  316\n",
      "military  316\n",
      "Efficient Transfer Learning (PELT)  232\n",
      "Embedding class  73\n",
      "embeddings  70, 135-138\n",
      "bag-of-words approach  136\n",
      "word2vec  136\n",
      "Exploratory Data Analysis (EDA)  209\n",
      "extract, transform, and load (ETL)  209\n",
      "F\n",
      "Facebook AI Similarity Search (Faiss)  59, 142\n",
      "fact-checking\n",
      "hallucination, mitigating through  100-103\n",
      "fact-checking stages\n",
      "claim detection  100\n",
      "evidence retrieval  100\n",
      "verdict prediction  100\n",
      "fake LLM  72, 73\n",
      "FastAPI   276-279\n",
      "few-shot chain-of-thought prompting   249\n",
      "few-shot learning prompting  246-248\n",
      "Financial PhraseBank  91\n",
      "Finetuner  80\n",
      "fine-tuning  225, 232, 233\n",
      "advantages  232\n",
      "commercial models  241, 242\n",
      "open-source models  236-241\n",
      "setting up  233-236\n",
      "FizzBuzz  79\n",
      "Flowise library  49\n",
      "forward diffusion process  28\n",
      "Foundational Model Orchestration  \n",
      "(FOMO)  261\n",
      "foundation model  15\n",
      "G\n",
      "gcloud command-line interface (CLI)\n",
      "installation link  77\n",
      "Generative Adversarial Networks (GANs)  28\n",
      "generative AI models  2-8\n",
      "Artificial General Intelligence  309, 310\n",
      "Big Tech, versus small enterprises  307, 308\n",
      "challenges  302, 303\n",
      "current state  300, 301\n",
      "developing, terms  19\n",
      "forthcoming era  321, 322\n",
      "handling, on various domains  6\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 355:\n",
      "Topics:\n",
      "  Index\n",
      "332\n",
      "impact, on data science  204-206\n",
      "need for  8-11\n",
      "techniques and approaches, for making \n",
      "accessible  306\n",
      "trends, in model development  304-307\n",
      "usage, in other domains  33, 34\n",
      "Generative Pre-trained Transformer (GPT) \n",
      "models  3, 13-16\n",
      "conditioning  26\n",
      "pre-training  23, 24\n",
      "scaling  25, 26\n",
      "tokenization  24, 25\n",
      "usage, considerations  20-23\n",
      "Google Cloud Natural Language (NL)  77\n",
      "Google Cloud Platform (GCP)  77-80\n",
      "Google Colab  233\n",
      "GPT4All  88, 89\n",
      "grade-school math questions (GSM8k)  235\n",
      "Graph Convolutional Networks (GCNs)  141\n",
      "Graphics Processing Units (GPUs)  8, 233\n",
      "Graph Neural Networks (GNNs)  141\n",
      "Grouped-Query Attention (GQA)  22\n",
      "H\n",
      "hallucination\n",
      "mitigating, through fact-checking  100-103\n",
      "hierarchical navigable small world  \n",
      "(HNSW)  141, 144\n",
      "hnswlib  142\n",
      "Hugging Face  75, 76\n",
      "Hugging Face Transformers  86, 87\n",
      "HumanEval dataset  176\n",
      "HyperText Markup Language (HTML)  59\n",
      "I\n",
      "IBM Watson OpenScale  290\n",
      "inference-time conditioning  230-232\n",
      "techniques  231\n",
      "information, summarizing  103\n",
      "basic prompting  103, 104\n",
      "Chain of Density (CoD)  105, 106\n",
      "map reduce approach  107-109\n",
      "prompt templates, using  104\n",
      "token usage, monitoring  109-111\n",
      "infrastructure as a Service (IaaS)  84\n",
      "Infrastructure as Code (IaC)  276\n",
      "Integrated Development Environments \n",
      "(IDEs)  174\n",
      "J\n",
      "Jina AI  80-82\n",
      "reference link  80\n",
      "K\n",
      "k-dimensional trees (k-d trees)  140\n",
      "KM scaling law  305\n",
      "L\n",
      "Ladder Side-Tuning (LST)  232\n",
      "LangChain  46-50, 147\n",
      "agents  52, 53\n",
      "benefits  47\n",
      "chains  50, 51\n",
      "comparing, with frameworks  60, 61\n",
      "data loaders  148, 149\n",
      "document loaders  149, 150\n",
      "key components, exploring  50\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 356:\n",
      "Topics:\n",
      "  Index\n",
      "333\n",
      "memory  54, 55\n",
      "retrievers  148-151\n",
      "tools  55, 56\n",
      "working  57-59\n",
      "LangChain API\n",
      "reference link  59\n",
      "LangChain Expression Language (LCEL)  105\n",
      "LangChainHub library  49\n",
      "LangFlow library  49\n",
      "Langfuse  290\n",
      "LangKit  290\n",
      "LangServe  274\n",
      "LangSmith  291-293\n",
      "Language Models (LMs)  5\n",
      "Large Language Models  \n",
      "(LLMs)  1, 5, 11, 12, 37, 43-45\n",
      "areas  12\n",
      "deploying  273\n",
      "deployment readiness, ensuring  258, 259\n",
      "evaluating  261-264\n",
      "examples  45\n",
      "external services, integrating  44, 45\n",
      "Foundational Model Orchestration  \n",
      "(FOMO)  261\n",
      "limitations  38-42\n",
      "limitations, mitigating  42, 43\n",
      "LLMOps  260\n",
      "MLOps  260\n",
      "ModelOps  261\n",
      "need for  45\n",
      "observing  284-286\n",
      "parameters  9\n",
      "tasks, related to programming   174\n",
      "usage  27\n",
      "used, for data exploration  217-222\n",
      "large language models (LLMs), technical \n",
      "background\n",
      "GPT  13-16\n",
      "GPT models  20\n",
      "major players  18-20\n",
      "notable foundational GPT models  16-18\n",
      "latent diffusion model  31\n",
      "Lethal Autonomous Weapons Systems \n",
      "(LAWS)  316\n",
      "Llama 2  186\n",
      "LLaMa and LLaMa 2 series  17\n",
      "llama.cpp  87, 88\n",
      "LlamaHub library  48\n",
      "LLM apps, deployment  273-275\n",
      "aspects, for production  273\n",
      "FastAPI web server, using  276-279\n",
      "Ray, using  280\n",
      " requirements for running  275\n",
      "services and frameworks  274\n",
      "LLM apps, evaluation\n",
      "comparing, against criteria  265-267\n",
      "running, against datasets  268-272\n",
      "string and semantic comparisons  267\n",
      "two outputs, comparing  264, 265\n",
      "LLM apps, monitoring\n",
      "considerations  285\n",
      "evaluation areas  286\n",
      "LangSmith  291-293\n",
      "observability tools  289-291\n",
      "PromptWatch  294, 295\n",
      "responses, tracing  287-289\n",
      "LLMonitor  290\n",
      "LLMOps  260\n",
      "LMOps  260\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 357:\n",
      "Topics:\n",
      "  Index\n",
      "334\n",
      "locality sensitive hashing (LSH)  141\n",
      "local models\n",
      "exploring  85\n",
      "low-rank adaptation  229, 230\n",
      "Low-Rank Adaptation (LoRA)  228, 229\n",
      "M\n",
      "Machine Learning (ML)  5\n",
      "map reduce approach  107-109\n",
      "maps tokens  24\n",
      "Masked Language Modeling (MLM)  23\n",
      "Massive Multitask Language Understanding \n",
      "(MMLU)  2\n",
      "Maximum Marginal Relevance (MMR)   156\n",
      "md5 checksum tool  87\n",
      "Mean Squared Error (MSE)  31\n",
      "memory  54, 55\n",
      "options  54\n",
      "memory, chatbot implementation  160\n",
      "conversation buffers  161-163\n",
      "ConversationSummaryMemory  164\n",
      "knowledge graphs, storing  164\n",
      "long-term persistence  166\n",
      "memory mechanisms, combining  165, 166\n",
      "Mixture of Experts (MoE) model  15\n",
      "MLOps  260\n",
      "ModelOps  261\n",
      "monitoring process  284\n",
      "Multi-Head Attention (MHA)  21\n",
      "Multi-Query Attention (MQA)  22\n",
      "N\n",
      "Natural Language Processing (NLP)  2\n",
      "Negative Log-Likelihood (NLL)  23\n",
      "Neural Machine Translation (NMT)  20\n",
      "nmslib  142\n",
      "O\n",
      "observation-dependent reasoning  123, 124\n",
      "OpenAI  4, 73-75\n",
      "open-source models  236-241\n",
      "Optical Character Recognition (OCR)  7\n",
      "P\n",
      "PaLM 2  16\n",
      "Parameter-Efficient Fine-Tuning (PEFT)  229\n",
      "Perplexity (PPL)  23\n",
      "Pip\n",
      "cons  66\n",
      "pros  66\n",
      "reference link  67\n",
      "using  68\n",
      "plan-and-execute agent  123, 124\n",
      "platform as a service (PaaS)  84\n",
      "Poetry\n",
      "cons  66\n",
      "pros  66\n",
      "reference link  68\n",
      "using  68\n",
      "Portkey  289\n",
      "product quantization (PQ)  140\n",
      "prompt chaining  50\n",
      "prompt engineering  225, 242-244\n",
      "components  242\n",
      "techniques  244-246\n",
      "prompt engineering, techniques\n",
      "chain-of-thought  248, 249\n",
      "few-shot learning  246-248\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 358:\n",
      "Topics:\n",
      "  143\n",
      "similarity search  143\n",
      "Simple Workflow Service (SWF)  209\n",
      "Site Reliability Engineering (SRE)  286\n",
      "small local model  187-189\n",
      "Smart Generation System Personalized   314\n",
      "societal implications  317\n",
      "misinformation and cybersecurity  318\n",
      "regulations and implementation  \n",
      "challenges  319, 320\n",
      "Software as a Service (SaaS)  84\n",
      "software development\n",
      "AI, using for  174, 175\n",
      "automating  189-201\n",
      "Splunk  290\n",
      "SPTAG  143\n",
      "Stable Diffusion  \n",
      "  30\n",
      "StarChat  184-186\n",
      "StarCoder  179-184\n",
      "stochastic parrots  38\n",
      "Streamlit app  159\n",
      "advantages  120\n",
      "T\n",
      "Technology Innovation Institute (TII)  19\n",
      "Tensor Processing Units (TPUs)  85, 233\n",
      "\n",
      "  Index\n",
      "335\n",
      "self-consistency prompting  249-251\n",
      "Tree-of-Thought (ToT) prompting  251-255\n",
      "zero-shot prompting  246\n",
      "PromptWatch  294, 295\n",
      "ProsusAI/finbert  91\n",
      "Proximal Policy Optimization (PPO)  228\n",
      "Q\n",
      "quantization  230\n",
      "R\n",
      "Ray  280-284\n",
      "Read-Eval-Print Loop (REPL)  73\n",
      "reasoning strategies\n",
      "exploring  121-128\n",
      "reinforcement learning\n",
      "with human feedback  228, 229\n",
      "Reinforcement Learning with Human \n",
      "Feedback (RLHF)  26, 228\n",
      "Replicate  82-84\n",
      "reference link  83\n",
      "Representational State Transfer Application \n",
      "Programming Interface  \n",
      "(REST API)  276\n",
      "Retrieval-Augmented Generation  \n",
      "(RAG)  44, 131, 134\n",
      "Retrieval-Augmented Language Models \n",
      "(RALMs)  134\n",
      "retrievers, LangChain  150\n",
      "Arxiv retriever  151\n",
      "BM25 retriever  150\n",
      "custom retrievers  153\n",
      "dense retriever  151\n",
      "kNN retriever  151, 152\n",
      "PubMed retriever  152\n",
      "TF-IDF retriever  151\n",
      "Wikipedia retriever  151\n",
      "reverse diffusion process  29\n",
      "S\n",
      "self-consistency prompting  249-251\n",
      "semantic search  \n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 359:\n",
      "Topics:\n",
      "  Index\n",
      "336\n",
      "text-to-image models  27-32\n",
      "applications  27\n",
      "tokenization  24\n",
      "token usage\n",
      "monitoring  109-111\n",
      "tools  55, 56\n",
      "examples  55, 56\n",
      "information, retrieving with  116, 117\n",
      "questions, answering with  116\n",
      "visual interface, building with  118-121\n",
      "tracking  287\n",
      "transformer-based models  137\n",
      "transformers  13\n",
      "architectural features  21\n",
      "Tree-of-Thought (ToT) prompting  251-255\n",
      "Turing test  132\n",
      "U\n",
      "U-Net  31\n",
      "user interface (UI)  126\n",
      "utility chains  50\n",
      "V\n",
      "Variational Autoencoders (VAEs)  10, 30\n",
      "vector databases  143\n",
      "anomaly detection  143\n",
      "characteristics  144\n",
      "examples  145-147\n",
      "Natural Language Processing (NLP)  143\n",
      "personalization  143\n",
      "vector indexing  140\n",
      "vector libraries  141-143\n",
      "Annoy  142\n",
      "Faiss  142\n",
      "hnswlib  142\n",
      "nmslib  142\n",
      "SPTAG  143\n",
      "vector search  135, 139\n",
      "vector storage  135, 139\n",
      "venture capitalists (VCs)  144\n",
      "Visual Foundation Models (VFMs)  15\n",
      "W\n",
      "Weights and Biases (W&B)  234\n",
      "tracing  290\n",
      "Wikipedia  117\n",
      "Wolfram Alpha  117\n",
      "reference link  118\n",
      "Z\n",
      "Zero-Shot agent  124\n",
      "zero-shot chain-of-thought  249\n",
      "zero-shot prompting  231, 246\n",
      "\n",
      "Subtopics:\n",
      "----------------------------------------\n",
      "Page 360:\n",
      "Topics:\n",
      "  Scan the QR code or visit the link below\n",
      "https://packt.link/free-ebook/9781835083468\n",
      "2.\t\n",
      "Submit your proof of purchase\n",
      "3.\t\n",
      "\n",
      "  We’ll send your free PDF and other benefits to your email directly\n",
      "\n",
      "  Download a free PDF copy of this book\n",
      "Thanks for purchasing this book!\n",
      "\n",
      "  Is your eBook \n",
      "purchase not compatible with the device of your choice?\n",
      "Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n",
      "\n",
      "  The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free \n",
      "content in your inbox daily\n",
      "Follow these simple steps to get the benefits:\n",
      "1.\t\n",
      "\n",
      "Subtopics:\n",
      "  Do you like to read on the go but are unable to carry your print books everywhere?\n",
      "  Read anywhere, any place, on any device.\n",
      "  That’s it!\n",
      "  Search, copy, and paste code from your favorite technical \n",
      "books directly into your application. \n",
      "\n",
      "----------------------------------------\n",
      "Page 361:\n",
      "Topics:\n",
      "Subtopics:\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_page(pdf_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Extract text from each page\n",
    "    page_texts = {}\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        page_texts[page_num] = page.get_text()\n",
    "    \n",
    "    # Close the document\n",
    "    pdf_document.close()\n",
    "    \n",
    "    return page_texts\n",
    "\n",
    "def identify_topics(text):\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Basic topic identification (you may need more sophisticated methods)\n",
    "    topics = set()\n",
    "    subtopics = set()\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        if len(sent.ents) > 0:\n",
    "            topics.add(sent.text)\n",
    "        else:\n",
    "            subtopics.add(sent.text)\n",
    "    \n",
    "    return list(topics), list(subtopics)\n",
    "\n",
    "def main(pdf_path):\n",
    "    # Extract text from the PDF\n",
    "    page_texts = extract_text_from_page(pdf_path)\n",
    "    \n",
    "    for page_num, text in page_texts.items():\n",
    "        print(f\"Page {page_num + 1}:\")\n",
    "        topics, subtopics = identify_topics(text)\n",
    "        print(\"Topics:\")\n",
    "        for topic in topics:\n",
    "            print(f\"  {topic}\")\n",
    "        print(\"Subtopics:\")\n",
    "        for subtopic in subtopics:\n",
    "            print(f\"  {subtopic}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = 'example.pdf'\n",
    "\n",
    "# Run the main function\n",
    "main(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'title': '', 'author': '', 'subject': '', 'keywords': '', 'page_count': 361}}\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_metadata_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    metadata = {\n",
    "        \"title\": doc.metadata.get(\"title\", \"Unknown Title\"),\n",
    "        \"author\": doc.metadata.get(\"author\", \"Unknown Author\"),\n",
    "        \"subject\": doc.metadata.get(\"subject\", \"Unknown Subject\"),\n",
    "        \"keywords\": doc.metadata.get(\"keywords\", \"Unknown Keywords\"),\n",
    "        \"page_count\": len(doc)\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_sections_from_text(text):\n",
    "    sections = re.findall(r'(Chapter \\d+): (.+)', text)\n",
    "    return sections\n",
    "\n",
    "book_metadata = extract_metadata_from_pdf(\"example.pdf\")\n",
    "# sections = extract_sections_from_text(book_text)\n",
    "\n",
    "# Combine metadata and sections\n",
    "book_info = {\n",
    "    \"metadata\": book_metadata,\n",
    "    # \"sections\": sections\n",
    "}\n",
    "\n",
    "print(book_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '', 'author': '', 'subject': '', 'keywords': '', 'page_count': 361}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExtractorPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
